[
  {
    "objectID": "aboutme_index.html",
    "href": "aboutme_index.html",
    "title": "Nathan Bresette",
    "section": "",
    "text": "I am a senior at Truman State University, majoring in Statistics with a concentration in Data Science and a minor in Mathematics. Passionate about uncovering insights from data, I am dedicated to leveraging statistical techniques and data analysis tools to solve real-world problems. With a strong foundation in mathematics and a keen interest in data science, I am enthusiastic about contributing to projects that drive innovation and make a positive impact.\n \nIn my portfolio, you’ll find a diverse range of projects, including analyses, visualizations, and applications that showcase my skills and experiences. From developing R packages to conducting consulting projects and participating in DataFest competitions, I’ve gained practical knowledge and hands-on experience in various aspects of data science. I invite you to explore my portfolio to learn more about my work and how I can contribute to your projects and initiatives."
  },
  {
    "objectID": "datafest.html",
    "href": "datafest.html",
    "title": "DataFest",
    "section": "",
    "text": "DataFest is a competition of data in which teams of undergraduates work “around the clock” to discover and share meaning in a large, rich, and complex data set. It is a nationally coordinated weekend-long data analysis competition and challenges students to find their own story to tell with the data that is meaningful to the data donor.\nAt Truman, we had a group of four that worked on several smaller projects before the actual competition. The week before we did a practice DataFest where our teacher gave us an unclean dataset with millions of rows on car crashes across the United States. We worked from 9 am to 5 pm and our completed results are in Traffic Impact. The actual datafest competition project is CourseKata. Due to 4 people writing code, it is not in the cleanest format. I would recommend looking at the slideshow for our finished project\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraffic Impact\n\n\n\nData Cleaning\n\nCART\n\nR\n\nPresentation\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\n\n\n\n\n\n\n\nCourseKata\n\n\n\nData Cleaning\n\nXGBoost\n\nPresentation\n\nR\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datafest_file/Traffic Impact.html",
    "href": "datafest_file/Traffic Impact.html",
    "title": "Traffic Impact",
    "section": "",
    "text": "Analysis on Missouri crashes from 2021 to 2023 with a two-stage predictive model—comprising a decision tree and a CART regression tree was developed."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#overview-of-project",
    "href": "datafest_file/Traffic Impact.html#overview-of-project",
    "title": "Traffic Impact",
    "section": "Overview of Project",
    "text": "Overview of Project\nThe traffic dataset contains 7.7 million cases of crashes across the United States, from 2016 to 2024, as well as the locations of the crash, weather conditions, features of surrounding road, and the severity of the impact of the crash on traffic conditions. For purposes of our analysis, we focused on crashes in Missouri during the years 2021, 2022, and 2023. To enhance analysis, our group engineered new features within the dataset, highlighted by a new feature we called Traffic Impact. This variable was based on how long traffic was inhibited, as well as the total distance of road over which traffic was impacted. Our other main enhancement was refactoring and separating the weather categories into two separate features: one for the type of weather, and another for the severity of the weather.\nExploratory analysis of the new feature Traffic Impact to the weather conditions during which crashes occurred, we found that crashes that occurred during conditions involving snow and ice had a significantly higher Traffic Impact score than crashes in any other conditions. Additionally, when controlling for conditions with snow and ice, we found that temperature had no significant effect on the Traffic Impact Score.\nTo predict the severity of traffic impact, we utilized a two-stage model: one which categorizes if the crash has an impact, and another that predicts its Traffic Impact score. Models were trained on crashes from 2021 and tested on the crashes from 2022. The first stage model utilized is a categorical decision tree which resulted in an accuracy of 81.51%, a sensitivity of 91.41%, and specificity of 21.92%. The second stage model is a CART regression tree that resulted in a RMSEtrain of 1.32 and RMSEtest of 1.578. The second stage model predicted values had a correlation of 0.468 with the actual values."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#slideshow",
    "href": "datafest_file/Traffic Impact.html#slideshow",
    "title": "Traffic Impact",
    "section": "Slideshow",
    "text": "Slideshow"
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html",
    "href": "posts/DDPM/DDPM_Port.html",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "Magnetic Resonance Imaging (MRI) is vital for neuroscience and medical diagnostics. However, high-resolution MR images is time-consuming and expensive. It is also constrained by hardware limitations and patient comfort. Super-resolution techniques offer a powerful solution by reconstructing high-quality images from lower-resolution inputs which enhances clinical utility without increasing scan time. In this project, I explore the application of diffusion-based deep learning models to MRI super-resolution, leveraging the OASIS dataset.\n\n\n\nThe primary model used in this project is a UNet-based Denoising Diffusion Probabilistic Model (DDPM), implemented using HuggingFace’s diffusers library. The model learns to iteratively remove noise from high-resolution images while being conditioned on upsampled low-resolution inputs. Compared to GANs, diffusion models offer improved training stability and produce more diverse, less artifact-prone outputs—key benefits in medical imaging tasks where realism and detail are critical. See ‘Nathan’s Notes’ for my detailed notes on the differences.\n\n\n\nOne of the core challenges of this project was working on a Mac machine with limited computational power. This constraint required significant adaptations to standard approaches—such as reducing image dimensions, limiting batch sizes, and restricting the number of training epochs. I extracted and saved individual 2D grayscale slices from 3D MRI volumes, a practical compromise that preserved key anatomical information while reducing memory load. To further address data scarcity and prevent overfitting, I applied a variety of augmentation techniques, including rotation, flipping, affine transforms, and brightness/contrast jittering. This helped increase my tiny dataset of 8 images significantly\nI would explore larger batch sizes, deeper UNets, and longer diffusion chains for improved image fidelity. In particular, models like SR3 (Super-Resolution via Repeated Refinement) or latent diffusion models (LDMs) trained with multi-resolution or multi-slice 3D context could significantly enhance output quality. Additionally, integrating perceptual loss functions (e.g., VGG-based) and domain-specific priors (like anatomical landmarks) could offer further gains in realism and utility.\n\n\n\nDespite the poor performance of the model—evidenced by noisy, blurry outputs and low evaluation scores (PSNR ≈ 8.09 dB, SSIM ≈ 0.165), this project was an invaluable learning experience. I successfully implemented a complete diffusion-based super-resolution pipeline from scratch, including dataset preparation, augmentation, model training, and inference, all within the limitations of a Mac. If I had access to more computing power, I would experiment with longer training schedules, more complex U-Net backbones, and potentially newer models like DDIM or StableSR to achieve higher-quality reconstructions."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#purpose-of-project",
    "href": "posts/DDPM/DDPM_Port.html#purpose-of-project",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "Magnetic Resonance Imaging (MRI) is vital for neuroscience and medical diagnostics. However, high-resolution MR images is time-consuming and expensive. It is also constrained by hardware limitations and patient comfort. Super-resolution techniques offer a powerful solution by reconstructing high-quality images from lower-resolution inputs which enhances clinical utility without increasing scan time. In this project, I explore the application of diffusion-based deep learning models to MRI super-resolution, leveraging the OASIS dataset."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#methods",
    "href": "posts/DDPM/DDPM_Port.html#methods",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "The primary model used in this project is a UNet-based Denoising Diffusion Probabilistic Model (DDPM), implemented using HuggingFace’s diffusers library. The model learns to iteratively remove noise from high-resolution images while being conditioned on upsampled low-resolution inputs. Compared to GANs, diffusion models offer improved training stability and produce more diverse, less artifact-prone outputs—key benefits in medical imaging tasks where realism and detail are critical. See ‘Nathan’s Notes’ for my detailed notes on the differences."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#computing-limitations",
    "href": "posts/DDPM/DDPM_Port.html#computing-limitations",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "One of the core challenges of this project was working on a Mac machine with limited computational power. This constraint required significant adaptations to standard approaches—such as reducing image dimensions, limiting batch sizes, and restricting the number of training epochs. I extracted and saved individual 2D grayscale slices from 3D MRI volumes, a practical compromise that preserved key anatomical information while reducing memory load. To further address data scarcity and prevent overfitting, I applied a variety of augmentation techniques, including rotation, flipping, affine transforms, and brightness/contrast jittering. This helped increase my tiny dataset of 8 images significantly\nI would explore larger batch sizes, deeper UNets, and longer diffusion chains for improved image fidelity. In particular, models like SR3 (Super-Resolution via Repeated Refinement) or latent diffusion models (LDMs) trained with multi-resolution or multi-slice 3D context could significantly enhance output quality. Additionally, integrating perceptual loss functions (e.g., VGG-based) and domain-specific priors (like anatomical landmarks) could offer further gains in realism and utility."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#takeaways",
    "href": "posts/DDPM/DDPM_Port.html#takeaways",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "Despite the poor performance of the model—evidenced by noisy, blurry outputs and low evaluation scores (PSNR ≈ 8.09 dB, SSIM ≈ 0.165), this project was an invaluable learning experience. I successfully implemented a complete diffusion-based super-resolution pipeline from scratch, including dataset preparation, augmentation, model training, and inference, all within the limitations of a Mac. If I had access to more computing power, I would experiment with longer training schedules, more complex U-Net backbones, and potentially newer models like DDIM or StableSR to achieve higher-quality reconstructions."
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html",
    "href": "posts/ticketdata/ticketanalysis.html",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "",
    "text": "College Football Playoff Semifinals Ticket Price Analysis using AWS, Python, and R"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#data-cleaning",
    "href": "posts/ticketdata/ticketanalysis.html#data-cleaning",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nCodelibrary(tidyverse)\nTickets &lt;- read.csv(\"~/Downloads/tickets_grouped_by_url (22).csv\")\n\nTickets_clean &lt;- Tickets %&gt;% \n  filter(Section != \"N/A\") %&gt;% \n  mutate(Event.Name = case_when(\n    Event.Name == \"Fiesta Bowl: Boise State vs Penn State/SMU - CFP Quarterfinal\" ~ \"Fiesta Bowl: Boise State vs Penn State - CFP Quarterfinal\",\n    Event.Name == \"Sugar Bowl: Georgia vs Notre Dame/Indiana - CFP Quarterfinal\" ~ \"Sugar Bowl: Georgia vs Notre Dame - CFP Quarterfinal\",\n    Event.Name == \"Peach Bowl: Arizona State vs Texas/Clemson - CFP Quarterfinal\" ~ \"Peach Bowl: Arizona State vs Texas - CFP Quarterfinal\",\n    Event.Name == \"Rose Bowl: Oregon vs Ohio State/Tennessee - CFP Quarterfinal\" ~ \"Rose Bowl: Oregon vs Ohio State - CFP Quarterfinal\",\n    TRUE ~ Event.Name\n  )) %&gt;%\n  mutate(Event.Name = case_when(\n    Event.Name == \"Sugar Bowl: Georgia vs Notre Dame - CFP Quarterfinal\" ~ \"Sugar Bowl: Georgia vs Notre Dame\",\n    Event.Name == \"Fiesta Bowl: Boise State vs Penn State - CFP Quarterfinal\" ~ \"Fiesta Bowl: Boise State vs Penn State\",\n    Event.Name == \"Peach Bowl: Arizona State vs Texas - CFP Quarterfinal\" ~ \"Peach Bowl: Arizona State vs Texas\",\n    Event.Name == \"Rose Bowl: Oregon vs Ohio State - CFP Quarterfinal\" ~ \"Rose Bowl: Oregon vs Ohio State\",\n  )) %&gt;% \n  separate(Row_Details, into = c(\"section_area\", \"row_number\"), sep = \",\") %&gt;%\n  mutate(row_number = gsub(\"Row \", \"\", row_number)) %&gt;% \n  mutate(Price = as.numeric(Price)) %&gt;% \n    mutate(\n    Time.Scraped = ymd_hms(Time.Scraped),  # Convert to datetime\n    Scraped.Date = as.Date(Time.Scraped),  # Extract date\n    Scraped.Hour = hour(Time.Scraped),     # Extract hour\n    Scraped.Day = wday(Time.Scraped, label = TRUE)  # Extract day of the week\n  ) %&gt;% \n  filter(!is.na(Price))\n\n\n### Separate into different bowls for analysis\nSugar &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Sugar Bowl: Georgia vs Notre Dame\")\n\nFiesta &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Fiesta Bowl: Boise State vs Penn State\")\n\nPeach &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Peach Bowl: Arizona State vs Texas\")\n\nRose &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Rose Bowl: Oregon vs Ohio State\")"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#mean-ticket-price-by-bowl-game",
    "href": "posts/ticketdata/ticketanalysis.html#mean-ticket-price-by-bowl-game",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Mean Ticket Price by Bowl Game",
    "text": "Mean Ticket Price by Bowl Game\n\nCode# Summarize mean price over time for each bowl\nSugar_summary &lt;- Sugar %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Sugar Bowl\")\n\nFiesta_summary &lt;- Fiesta %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Fiesta Bowl\")\n\nPeach_summary &lt;- Peach %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Peach Bowl\")\n\nRose_summary &lt;- Rose %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Rose Bowl\")\n\n# Combine all summaries\nBowl_summary &lt;- bind_rows(Sugar_summary, Fiesta_summary, Peach_summary, Rose_summary)\n\n\nggplot(Bowl_summary, aes(x = Time.Scraped, y = mean_price, color = Bowl)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time for Bowl Games\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Bowl Game\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#section-mean-price-by-bowl-game",
    "href": "posts/ticketdata/ticketanalysis.html#section-mean-price-by-bowl-game",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Section Mean Price by Bowl Game",
    "text": "Section Mean Price by Bowl Game\n\nCodeSugar_summary &lt;- Sugar %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Sugar Bowl\")\n\nFiesta_summary &lt;- Fiesta %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Fiesta Bowl\")\n\nPeach_summary &lt;- Peach %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Peach Bowl\")\n\nRose_summary &lt;- Rose %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Rose Bowl\")\n\n\n# Plotting the data by Section\nggplot(Sugar_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Sugar Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Peach_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Peach Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Fiesta_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Fiesta Bowl Games\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Rose_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Rose Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#row-number-analysis",
    "href": "posts/ticketdata/ticketanalysis.html#row-number-analysis",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Row Number Analysis",
    "text": "Row Number Analysis\n\nCodeSugar_summary_row &lt;- Tickets_clean %&gt;%\n  group_by(row_number) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(row_number = as.numeric(trimws(as.character(row_number)))) %&gt;%\n  arrange(row_number) %&gt;%\n  mutate(row_number = factor(row_number, levels = sort(unique(row_number)))) %&gt;% \n  filter(!is.na(row_number))\n\n\nggplot(Sugar_summary_row, aes(x = as.numeric(row_number), y = mean_price)) +\n  geom_col(fill = \"red\") +\n  labs(title = \"Ticket Prices by Row for All Bowls\", x = \"Row Number\", y = \"Mean Price\") +\n  scale_x_continuous(breaks = seq(min(as.numeric(Sugar_summary_row$row_number)), \n                                  max(as.numeric(Sugar_summary_row$row_number)), \n                                  by = 5)) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html",
    "href": "posts/NCAA Basketball/basketball.html",
    "title": "NCAA Basketball Analysis",
    "section": "",
    "text": "Performed web scraping using Selenium and BeautifulSoup, followed by an in-depth analysis in that included Principal Component Analysis, XGBoost, and neural networks.\nI unfortunately lost my data and do not want to rescrape it since it is now a year later but I will leave the project here!"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#abstract",
    "href": "posts/NCAA Basketball/basketball.html#abstract",
    "title": "NCAA Basketball Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project presents an analysis of college basketball team performance based on data from men’s NCAA Basketball. After merging the datasets from haslametrics and teamrankings along with data cleaning and feature engineering in R, the dataset consisted of 16 columns with 361 rows.\nExploratory data analysis includes correlation analysis, visualization of distributions, and principal component analysis (PCA) to address collinearity among variables. Although PCA had good insights, it was not utilized due to its limited account for variance.\nThe feature engineered variable, ‘Rank_Category’, classifies teams into three categories based on their ‘Rank’ column: Rank (0-25), Top 50% (excluding Rank), and Bottom 50%. Modeling efforts focused on predicting ‘Rank_Category’ using XGBoost with racing ANOVA tuning which resulted in an accuracy of 79.12% and an AUC of 0.918. Variable importance analysis showed key predictors including defensive efficiency, win rate, defensive field goal percentage, offensive 2-point percentage, and offensive turnovers. Additionally, a neural network model achieved a higher accuracy of 97.80%."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "href": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "title": "NCAA Basketball Analysis",
    "section": "Webscraping Data in Python",
    "text": "Webscraping Data in Python\nScraping the first website Halsametrics.com with selenium\n\nCodefrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\n\n\n# Set up the WebDriver with ChromeOptions\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('/usr/local/bin/chromedriver')  # Add the path to chromedriver executable\n\n# Initialize the WebDriver\ndriver = webdriver.Chrome(options=chrome_options)\n\n# Navigate to the webpage\ndriver.get('https://haslametrics.com/')\n\n# Wait for the page to load and for the 'Defense' button to be clickable\nwait = WebDriverWait(driver, 20)\ndefense_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cboRatings\"]/option[@value=\"Defense\"]')))\n\n# Click the 'Defense' button to load the defensive ratings\ndefense_button.click()\n\n# Wait for the table to load\nwait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"myTable\"]')))\n\n# Scrape the table\ntable = driver.find_element(By.XPATH, '//*[@id=\"myTable\"]')\nhasla = pd.read_html(table.get_attribute('outerHTML'))[0]\n\n# Flatten the MultiIndex columns\nhasla.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in hasla.columns.values]\n\n# Rename 'Unnamed: 1_level_0 Team' to 'Team'\nhasla.rename(columns={'Unnamed: 1_level_0 Team': 'Team'}, inplace=True)\n\n# Extracting win/loss information and creating new columns\nhasla['Win'] = hasla['Team'].str.extract(r'\\((\\d+)-\\d+\\)')\nhasla['Loss'] = hasla['Team'].str.extract(r'\\(\\d+-(\\d+)\\)')\n\n# Remove parentheses and numbers from 'Team' column\nhasla['Team'] = hasla['Team'].replace(regex={'\\([^)]*\\)': '', '\\d+': ''})\n\nhasla['Team'] = hasla['Team'].str.strip()\n\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nhasla.to_excel(f'{desktop_path}/findhasla.xlsx', index=False)\n\n# Close the browser\ndriver.quit()\n\n\nScraping the second website teamrankings.com with BeautifulSoup\n\nCodedef scrape_and_merge(urls, new_column_names):\n    dfs = []\n\n    for url in urls:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        table = soup.find('table')\n        df = pd.read_html(StringIO(str(table)))[0]\n\n        if url in new_column_names:\n            df.columns = new_column_names[url]\n\n        for col in df.columns:\n            if col not in ['Rank', 'Team']:\n                df[col] = pd.to_numeric(df[col].replace('%', '', regex=True), errors='coerce')\n\n        dfs.append(df)\n\n    # Merge all DataFrames dynamically\n    combined_df = dfs[0]\n    for i, df in enumerate(dfs[1:], start=2):\n        combined_df = pd.merge(combined_df, df, on='Team', how='outer', suffixes=('', f'_{i}'))\n\n    # Drop duplicate 'Team' columns\n    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n\n    return combined_df\n\n# Define the URLs\nurls = [\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate',\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game',\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession'\n]\n\n# Create a dictionary with new column names for certain URLs\nnew_column_names = {\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate': ['Rank', 'Team', 'FTR_2023', 'FTR_L3', 'FTR_L1', 'FTR_Home', 'FTR_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct': ['Rank', 'Team', 'ORB_2023', 'ORB_L3', 'ORB_L1', 'ORB_Home', 'ORB_Away', 'ORB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct': ['Rank', 'Team', 'DRB_2023', 'DRP_L3', 'DRB_L1', 'DRB_Home', 'DRB_Away', 'DRB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct': ['Rank', 'Team', 'opp3_2023', 'opp3_L3', 'opp3_L1', 'opp3_Home', 'opp3_Away', 'opp3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct': ['Rank', 'Team', 'p3_2023', 'p3_L3', 'p3_L1', 'p3_Home', 'p3_Away', 'p3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct': ['Rank', 'Team', 'o2p_2023', 'o2p_L3', 'op2_L1', 'op2_Home', 'op2_Away', 'op2 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct': ['Rank', 'Team', '2p_2023', '2p_L3', '2p_L1', '2p_Home', '2p_Away', '2p 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game': ['Rank', 'Team', 'Pace_2023', 'Pace_L3', 'Pace_L1', 'Pace_Home', 'Pace_Away', 'Pace 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession': ['Rank', 'Team', 'TO_2023', 'TO_L3', 'TO_L1', 'TO_Home', 'TO_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession': ['Rank', 'Team', 'oppTO_2023', 'oppTO_L3', 'oppTO_L1', 'oppTO_Home', 'oppTO_Away', 'oppTO 2022']\n}\n\n\nCombining the data frames and saving to desktop\n\nCode# Call the function to scrape and merge data\ncombined_df = scrape_and_merge(urls, new_column_names)\n\ncombined_df['Team'] = combined_df['Team'].replace({\n'Miami (OH)' : 'Miami'\n\n})\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\ncombined_df.to_excel(f'{desktop_path}/findme.xlsx', index=False)\n\n\n\nneutral_input = input(\"Is it a neutral site game (Yes/No): \")\n\n\n# Drop duplicate team names in hasla\nhasla = hasla.drop_duplicates(subset=['Team'])\n\n# Drop duplicate team names in combined_df\ncombined_df = combined_df.drop_duplicates(subset=['Team'])\n\n# Merge the DataFrames based on 'Team'\nmerged_df = pd.merge(hasla, combined_df, on='Team', how='inner')\n\n# Save the merged DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nmerged_df.to_excel(f'{desktop_path}/merged_data.xlsx', index=False)"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "href": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "title": "NCAA Basketball Analysis",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nAll further code performed in R:\nOnce the data has been combined into one data frame, there are over 100 variables. Using dplyr, 16 columns are selected, renamed for easier readability, mutated to correct variable type (numeric, factor, etc), and a new variable is feauture engineered to split the ranks into three categories of Ranked, Top 50%, and Bottom 50%.\n\nCodelibrary(readxl)\nlibrary(tidyverse)\nmerged_data &lt;- read_excel(\"~/Desktop/merged_data.xlsx\")\n\nclean_data &lt;- merged_data %&gt;%\n  select(`Unnamed: 0_level_0 Rk`, `Win`, `Loss`, `DEFENSIVE SUMMARY Eff`, `DEFENSIVE SUMMARY 3P%`, `DEFENSIVE SUMMARY FG%`, `DEFENSIVE SUMMARY MR%`, `DEFENSIVE SUMMARY NP%`, FTR_2023, TO_2023, ORB_2023, DRB_2023, p3_2023, `2p_2023`, Pace_2023, TO_2023) %&gt;%\n  rename(\n    Rank = `Unnamed: 0_level_0 Rk`,\n    `Def_Eff` = `DEFENSIVE SUMMARY Eff`,\n    `Def_3P` = `DEFENSIVE SUMMARY 3P%`,\n    `Def_FG` = `DEFENSIVE SUMMARY FG%`,\n    `Def_MR` = `DEFENSIVE SUMMARY MR%`,\n    `Def_NP` = `DEFENSIVE SUMMARY NP%`,\n    Off_FTR = FTR_2023,\n    Off_TO = TO_2023,\n    Off_ORB = ORB_2023,\n    Def_DRB = DRB_2023,\n    Off_3P = p3_2023,\n    Off_2P = `2p_2023`,\n    Pace = Pace_2023\n  ) %&gt;% \n  mutate(Win = as.numeric(Win),\n         Loss = as.numeric(Loss)) \n\nclean_data$Rank_Category &lt;- ifelse(clean_data$Rank &gt;= 0 & clean_data$Rank &lt;= 25, \"Ranked\",\n                                   ifelse(clean_data$Rank &gt; 25 & clean_data$Rank &lt;= 181, \"Top 50%\", \"Bottom 50%\"))\nclean_data &lt;- clean_data %&gt;%\n  mutate(Rank_Category = as.factor(Rank_Category))\n\n\nThe final data cleaning step is checking total NA values for each variable which there are none\n\nCodecbind(lapply(lapply(clean_data, is.na), sum))"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "href": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "title": "NCAA Basketball Analysis",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCorrelation and Scatter Plots\nThe data exploration begins by looking at the correlation between variables. I created a function to make a correlation plot then if the correlation is above the absolute value of 0.6, it will plot the scatter plot of the two correlated variables. Due to the high correlation in this data, I have limited the output to only two of the scatterplots.\n\nCodelibrary(corrplot)\n\ncompute_and_plot_correlation &lt;- function(data, threshold = 0.6) {\n  # Select numeric columns\n  numeric_data &lt;- data[, sapply(data, is.numeric)]\n  \n  # Remove rows with missing values\n  numeric_data &lt;- numeric_data[complete.cases(numeric_data), ]\n  \n  # Compute correlation matrix\n  correlation_matrix &lt;- cor(numeric_data)\n  \n  # Find pairs of variables with correlation above or below the threshold\n  high_correlation_pairs &lt;- which(abs(correlation_matrix) &gt; threshold & upper.tri(correlation_matrix), arr.ind = TRUE)\n  \n  # Create scatter plots for high correlation pairs\n  plots &lt;- list()\n  for (i in 1:nrow(high_correlation_pairs)) {\n    var_x &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 1]]\n    var_y &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 2]]\n    \n    plot &lt;- ggplot(data = numeric_data, aes_string(x = var_x, y = var_y)) +\n      geom_point() +\n      labs(title = paste(\"Scatter Plot of\", var_y, \"vs\", var_x), x = var_x, y = var_y) + \n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    plots[[paste(var_x, var_y, sep = \"_\")]] &lt;- plot\n  }\n  \n  # Plot correlation matrix\n  corrplot(correlation_matrix, method = \"shade\", type = \"lower\", diag = FALSE, addCoef.col = \"black\", number.cex = 0.5)\n  \n  return(plots)\n}\n\n#Example call to function\nscatter_plots &lt;- compute_and_plot_correlation(clean_data)\n\nfor (i in seq_along(scatter_plots)) {\n  if (i &gt; 2) break\n  print(scatter_plots[[i]])\n}\n\n\nDistributions - Histograms\nI also made a function to make histograms for all numeric variables to view the distributions. Because all of our variables are numeric, no bar charts were made to view the distribution of categorical variables.\n\nCodecreate_histograms_ggplot &lt;- function(data) {\n  # Get numeric variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n  \n  # Initialize an empty list to store ggplot objects\n  plots &lt;- list()\n  \n  # Loop through each numeric variable and create a histogram using ggplot\n  for (var in numeric_vars) {\n    # Create ggplot object for histogram\n    plot &lt;- ggplot(data, aes_string(x = var)) +\n      geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n      labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    # Append ggplot object to the list\n    plots[[var]] &lt;- plot\n  }\n  \n  return(plots)\n}\n\n# Example call to function\nhist_plots &lt;- create_histograms_ggplot(clean_data)\n\n\n  print(hist_plots[[2]])\n  print(hist_plots[[3]])"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "href": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "title": "NCAA Basketball Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nIn our data exploration, the correlation plot showed the high correlation between our variables. Because of this, a principal component analysis was performed to reduce collinearity. A scree plot was used to determine the number of components. Figure 5 shows the scree plot.\n\nCodelibrary(htmlwidgets)\nlibrary(plotly)\n\nX &lt;- subset(clean_data, select = -c(Rank_Category, Win, Loss))\n\nprin_comp &lt;- prcomp(X, center = TRUE, scale. = TRUE)\n\n\nScree Plot\nThe scree plot shows there should be around 3 components to account for the most variance while also reducing the dimensions.\n\nCodeplot(prin_comp, type = \"l\", main = \"Scree Plot\")\n\n\n3D PCA\nA 3d plot with the three axes of the plot representing the first three principal components (PC1, PC2, and PC3). It also clusters the variable Ranked_Category very accurately. Although it clusters Ranked_Category well, it only accounts for 61.67% of the variance so we will not use it.\n\nCodesumm &lt;- summary(prin_comp)\nsumm$importance[2,]\n\ncomponents &lt;- prin_comp[[\"x\"]]\ncomponents &lt;- data.frame(components)\ncomponents$PC2 &lt;- -components$PC2\ncomponents$PC3 &lt;- -components$PC3\ncomponents = cbind(components, clean_data$Rank_Category)\n\n# Combine components with Ranked labels\ncomponents &lt;- cbind(components, Rank_Category = clean_data$Rank_Category)\n\n# Create Plotly figure\nfig &lt;- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Rank_Category,\n               colors = c('#636EFA','#EF553B','#00CC96'), type = \"scatter3d\", mode = \"markers\",\n               marker = list(size = 4))\n\n\n# Customize layout\nfig &lt;- fig %&gt;% layout(\n  title = \"61.67% Variance Explained\",\n  scene = list(bgcolor = \"#e5ecf6\")\n)\n\n# Show the plot\nfig\nsaveWidget(fig, \"interactive_plot.html\")"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "href": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "title": "NCAA Basketball Analysis",
    "section": "XGBoost Classification Model",
    "text": "XGBoost Classification Model\nDue to the high colinearity between our variables, our model must be able to take it into account. This model will be for exploration use rather than predictive so that we can see what variables are important to be ranked higher at the end of the season.\n\nCode#libs\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(pROC)\nlibrary(data.table)\nlibrary(kableExtra)\n\n\nSplitting into Training/Testing\n\nCodeDATA &lt;- clean_data %&gt;% \n  select(-Rank)\n\nset.seed(123)\nDATA_SPLIT &lt;- DATA %&gt;%\n  initial_split(strata = Rank_Category)\n\nDATA_TRAIN &lt;- training(DATA_SPLIT)\nDATA_TEST &lt;- testing(DATA_SPLIT)\n\nset.seed(234)\nDATA_folds &lt;- vfold_cv(DATA_TRAIN, strata = Rank_Category)\nDATA_folds\n\n\nRecipe\n\nCodeDATA_rec &lt;-\n  recipe(Rank_Category ~ ., data = DATA_TRAIN) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nprep(DATA_rec) # checking prep\n\n\nTuning Model\n\nCodexgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n#workflow\nxgb_workfl &lt;- workflow(DATA_rec, xgb_spec)\n\n\nRace Anova\nOur model will be an XGBoost that utilizes a racing ANOVA. Tidymodels will also be used so that we can tune trees, min_n, mtry, tree_depth, learn_rate, and loss_reduction.\n\nCodelibrary(finetune)\ndoParallel::registerDoParallel()\n\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_workfl,\n  resamples = DATA_folds,\n  grid = 20,\n  metrics = metric_set(accuracy),\n  control = control_race(verbose_elim = TRUE)\n)\n\n\nComparing Models\nThe plot below shows the racing ANOVA as it picks out the best model\n\nCodeanova &lt;- plot_race(xgb_rs)\n\nanova +\n  labs(title = \"Model Race ANOVA\",\n       y = \"Model Accuracy\") +\n  theme_minimal() +\n  theme(plot.title = (element_text(hjust = 0.5)))\n\n\nBest Model\nThe following code is used to extract the best model\n\nCodeshow_best(xgb_rs)\n\n\nMetrics\n\nCodexgb_last &lt;- xgb_workfl %&gt;%\n  finalize_workflow(select_best(xgb_rs, metric = \"accuracy\")) %&gt;%\n  last_fit(DATA_SPLIT)\n\nxgb_last$.metrics\n\n\nConfusion Matrix\nThe final model had an accuracy of 83.52 % for predicting ‘Ranked_Category’ and an AUC of 0.914. Although this is not the highest accuracy, the more important part is the importance of each variable for the model are Def_Eff, Win, Def_FG, Off_2p, and Off_TO as seen in the plot below.\n\nCodeDATA_pred &lt;- collect_predictions(xgb_last)$.pred_class\n\nDATA_act &lt;- DATA_TEST$Rank_Category\n\nconfusionMatrix(DATA_pred, DATA_act)\n\n\nVIP\n\nCodelibrary(vip)\nvip &lt;- extract_workflow(xgb_last) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\", num_features = 10, mapping = aes(fill = Variable))\nvip"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#neural-network",
    "href": "posts/NCAA Basketball/basketball.html#neural-network",
    "title": "NCAA Basketball Analysis",
    "section": "Neural Network",
    "text": "Neural Network\nNow that we know the important variables for ‘Ranked Category’, a neural network was performed to see how well we could predict it. The neural network was made in R using the command neuralnet(). The best neural network is seen in Figure 9 which had a 97.80% accuracy.\n\nCodelibrary(neuralnet)\nlibrary(caret)\nlibrary(tidymodels)\n\nnndata &lt;- clean_data \nset.seed(123)\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(nndata, prop = 3/4, strata = Rank_Category)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nNN &lt;- neuralnet(Rank_Category ~ ., train_data, hidden = c(5,3), linear.output = TRUE)\nplot(NN, rep = \"best\")\n\n\nConfusion Matrix\n\nCodepredicted_classes &lt;- predict(NN, test_data)\n# Extract predicted class labels\npredicted_classes &lt;- max.col(predicted_classes)\n\n# Convert the indices to class labels\npredicted_classes &lt;- levels(test_data$Rank_Category)[predicted_classes]\nactual_classes &lt;- test_data$Rank_Category\n\npredicted_classes &lt;- factor(predicted_classes, levels = levels(actual_classes))\n\n# length(predicted_classes)\n# print(predicted_classes)\n\n# Extract actual class labels from the test data\n# length(actual_classes)\n# print(actual_classes)\n\n# Create a confusion matrix\nconfusionMatrix(predicted_classes, test_data$Rank_Category)"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html",
    "href": "posts/InquisitR/InquisitR.html",
    "title": "InquisitR",
    "section": "",
    "text": "Designed to simplify the initial phases of data exploration while enhancing the visual appeal of graphs. Currently, it offers three essential functions listed below. I am also actively developing a fourth to streamline data type conversions."
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#boxplotr",
    "href": "posts/InquisitR/InquisitR.html#boxplotr",
    "title": "InquisitR",
    "section": "boxplotR",
    "text": "boxplotR\nThe function generates detailed boxplots for various combinations of factor and numeric variables within your dataset\n\nCodelibrary(ggplot2)\n\nboxplotR &lt;- function(data) {\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric and one factor variable.\")\n  }\n\n  factor_vars &lt;- names(data)[sapply(data, is.factor)]  # Get factor variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]  # Get numeric variable names\n\n  plots &lt;- list()\n\n  # Loop through each combination of factor and numeric variable\n  for (x in factor_vars) {\n    for (y in numeric_vars) {\n      plot_title &lt;- paste(\"Boxplot of\", y, \"by\", x)\n      plot &lt;- ggplot(data, aes_string(x = x, y = y)) +\n        geom_boxplot(fill = \"skyblue\", color = \"black\") +\n        labs(title = plot_title) +\n        theme_minimal()\n      plots[[paste(x, y, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\nboxplotR(iris)\n\n$Species_Sepal.Length\n\n\n\n\n\n\n\n\n\n$Species_Sepal.Width\n\n\n\n\n\n\n\n\n\n$Species_Petal.Length\n\n\n\n\n\n\n\n\n\n$Species_Petal.Width"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#correlationr",
    "href": "posts/InquisitR/InquisitR.html#correlationr",
    "title": "InquisitR",
    "section": "correlationR",
    "text": "correlationR\nThe function creates customizable pairwise plot matrices using GGally, allowing you to tailor plots for upper triangles, lower triangles, and diagonal views.\n\nCodelibrary(GGally)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nupperFn &lt;- function(data, mapping, ...) {\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  cor_value &lt;- round(cor(x, y), 2)\n  cor_label &lt;- format(cor_value, nsmall = 2)\n\n  data.frame(x = 1, y = 1, cor = cor_value) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_tile(aes(fill = cor), color = \"white\", width = 1, height = 1) +\n    geom_text(aes(label = cor_label), color = \"black\", size = 5, vjust = 0.5) +\n    scale_fill_gradient2(low = \"blue2\", high = \"red2\", mid = \"white\", midpoint = 0, limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n    theme_void() +\n    theme(legend.position = \"none\",\n          plot.margin = unit(c(0, 0, 0, 0), \"cm\")) +\n    theme_minimal()\n}\n\nlowerFn &lt;- function(data, mapping, method = \"lm\", ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(colour = \"skyblue3\") +\n    geom_smooth(method = method, color = \"black\", ...) +\n    theme_minimal()\n}\n\ndiagFn &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_density(aes(y = ..density..), colour = \"red\", fill = \"red\", alpha = 0.2) +\n    geom_histogram(aes(y = ..density..), colour = \"blue\", fill = \"skyblue3\", alpha = 0.5) +\n    theme_minimal()\n}\n\ncorrelationR &lt;- function(df) {\n  numeric_cols &lt;- df %&gt;%\n    dplyr::select(where(is.numeric)) %&gt;%\n    colnames()\n\n  if (length(numeric_cols) &lt; 2) {\n    stop(\"The dataframe must contain at least two numeric columns.\")\n  }\n\n  ggpairs(\n    df[, numeric_cols],\n    lower = list(continuous = wrap(lowerFn, method = \"lm\")),\n    diag = list(continuous = wrap(diagFn)),\n    upper = list(continuous = wrap(upperFn))\n  ) + theme_minimal()\n}\n\ndata(iris)\ncorrelationR(iris)"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#distributionr",
    "href": "posts/InquisitR/InquisitR.html#distributionr",
    "title": "InquisitR",
    "section": "distributionR",
    "text": "distributionR\nThis function easily visualize distributions of both numeric and categorical variables, with flexible options to suit your analysis needs.\n\nCodeutils::globalVariables(c(\"Category\", \"Frequency\"))\n\n[1] \"Category\"  \"Frequency\"\n\nCodelibrary(ggplot2)\n\ndistributionR &lt;- function(data, plot_bars = TRUE, plot_histograms = TRUE) {\n  # Check if input is a data frame\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.numeric(x) || is.character(x) || is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric or categorical (character/factor) variable.\")\n  }\n\n  plots &lt;- list()\n\n  # Create bar plots for categorical variables\n  if (plot_bars) {\n    char_factor_cols &lt;- names(data)[sapply(data, function(x) is.character(x) || is.factor(x))]\n    for (col in char_factor_cols) {\n      freq_table &lt;- table(data[[col]])\n      plot_data &lt;- as.data.frame(freq_table)\n      names(plot_data) &lt;- c(\"Category\", \"Frequency\")\n      plot &lt;- ggplot(plot_data, aes(x = Category, y = Frequency)) +\n        geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Bar Plot of\", col), x = col, y = \"Frequency\") +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n        theme_minimal()\n      plots[[paste(\"barplot\", col, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  # Create histograms for numeric variables\n  if (plot_histograms) {\n    numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n    for (var in numeric_vars) {\n      plot &lt;- ggplot(data, aes(x = .data[[var]])) +\n        geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n        theme_minimal()\n      plots[[paste(\"histogram\", var, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\ndistributionR(iris, plot_bars = TRUE, plot_histograms = TRUE)\n\n$barplot_Species\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Width\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Width\n\n\n\n\n\n\n\n\nGithub link to use my package https://lnkd.in/gR2h7gKB\nor just use: library(devtools) install_github(“NathanBresette/InquisitR”)"
  },
  {
    "objectID": "posts/Playfair/Playfair.html",
    "href": "posts/Playfair/Playfair.html",
    "title": "Playfair’s Recreation & Improvement",
    "section": "",
    "text": "Critiquing, recreating, and improving playfair’s famous graph\n\nCodeknitr::include_graphics(\"pf.png\")\n\n\n\n\n\n\n\nPart 1: Critique\nPlayfair’s chart works well in several ways. First, it uses bars as a simple and effective visual representation to compare taxation and revenue across nations. The use of height to represent quantitative values allows viewers to easily see differences between nations. Additionally, the connecting line between the two bars allow for an easier comparison when looking at one specific county. The categorical arrangement of the nations by population size also helps in maintaining clarity and comparison. The use of circle size to represent population size is simple yet effective. The chart combines several forms of data in one visualization. Playfair’s introduction an effective method of showing patterns and trends visually. Despite its innovation, the chart has limitations that detract from its effectiveness. One of the main issues is the overloading of visual elements—having multiple variables in a single visual without clear legends. Without reading more about the graph, it is impossible to tell what is taxation and revenue. The pie charts inside the countries circles are too small to see. The Venn diagram is also extremely small and I had to zoom in to see it. All things considered, Playfair’s chart effectively communicates its purpose—to compare the economic and geographical standings of European nations. While there are some things that are not ideal in its ability to precisely communicate all the data elements, the chart’s innovative use of bars for comparison was groundbreaking for its time. It set the foundation for modern data visualization, even though it lacks some refinements that we would expect today especially in DATA324! # Part 2 - Recreation\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\nlibrary(stringr) # str_wrap\neurope &lt;- read.csv(\"~/Downloads/playfair_european_nations.csv\")\n# Calculate radius\nRadius &lt;- (sqrt(europe$Area / 3.1459)) / 140  # Scale\n# Dynamic center calculation\nnum_countries &lt;- nrow(europe)\nspacing &lt;- 3  \ncenter &lt;- numeric(num_countries)  # Initialize\n# Position of the first circle\ncenter[1] &lt;- 10 \n# Calculate the center positions based on the radius of the circles\nfor (i in 2:num_countries) {\n  center[i] &lt;- center[i - 1] + Radius[i - 1] + Radius[i] + spacing\n}\n# Axisbreak based on the center\naxisbreak &lt;- unique(center)\n# Wrapping x-axis labels\nwrapped_labels &lt;- str_wrap(europe$Country, width = 15)\nPlayfair &lt;- ggplot(europe) +\n  geom_circle(aes(x0 = center, y0 = 0, r = Radius)) +\n  \n  # Yellow line for Taxation\n  geom_segment(aes(x = center + Radius, xend = center + Radius, y = 0, yend = Taxation),\n               size = 1, color = \"yellow\") +\n  \n  # Red line for Population\n  geom_segment(aes(x = center - Radius, xend = center - Radius, y = 0, yend = Population),\n               size = 1, color = \"red\") +\n  \n  # Dashed line to connect red and yellow\n  geom_segment(aes(x = center - Radius, xend = center + Radius, \n                   y = Population, yend = Taxation), linetype = \"longdash\") +\n  \n  # Solid black lines at y = 0, 10, and 20\n  geom_hline(yintercept = c(0, 10, 20), color = \"black\", size = 0.5) +\n  \n  # Apply wrapped labels and adjust x-axis text\n  scale_x_continuous(breaks = axisbreak, labels = wrapped_labels) +\n  \n  # Set y-axis to display 0 to 30 with every integer as a tick mark\n  scale_y_continuous(\n    limits = c(-10, 30), \n    breaks = seq(0, 30, by = 1),\n    sec.axis = sec_axis(trans = ~ ., \n                        breaks = seq(0, 30, by = 1))  # Add name for secondary axis\n  ) +\n  \n  # Custom old paper-like background\n  theme(\n    # Old paper color\n    panel.background = element_rect(fill = \"#d3b58f\"),  \n    plot.background = element_rect(fill = \"#d3b58f\"),   \n    panel.grid.major = element_line(color = \"darkgrey\", size = 0.5), \n    panel.grid.minor = element_blank(), \n    \n    # Axis stuff\n    axis.text.x = element_text(angle = 90, face = \"bold\", hjust = 1),\n    axis.text.y = element_text(size = 5),\n    axis.text.y.right = element_text(size = 5),\n    plot.title = element_text(hjust = 0.5, size = 7, face = \"bold\"),\n    panel.border = element_rect(color = \"black\", fill = NA) # black border\n  ) +\n  labs(title = \"Chart Representing the Extent, Population & Revenues, of the Principal Nations in Europe, after the division of Poland & Treaty of Luneville\", x = \"\", y =\"\")\nPlayfair\n\n\n\n\n\n\n\nPart 3: New Data Visualization\n\nCodeRecreation &lt;- ggplot(europe, aes(x = Population, y = Taxation)) + \n  geom_point(aes(col = Country, size = Area)) + \n  ylim(c(0, 30)) + \n  labs(title = \"New Playfair Data Visualization\",\n       y = \"Taxation (Millions)\", \n       x = \"Population (Millions)\",\n       size = \"Land Area\",    \n       color = \"Country\") +         \n  scale_size_continuous(breaks = c(62000, 200000, 500000, 1000000, 2000000, 4720000),  \n                        labels = c(\"62k\", \"200k\", \"500k\", \"1M\", \"2M\", \"4.7M\"),\n                        range = c(2, 12)) +                # Better visibility\n  geom_text(aes(label = Country),  # Data labels\n            vjust = -0.5,           # Adjust vertical position\n            size = 3,               # Size of the text\n            check_overlap = TRUE) +   \n  guides(color = \"none\") +           # Remove legend for Country\n  theme(legend.key = element_blank()) + # Remove legend background\n  theme_minimal()\nRecreation\n\n\n\n\n\n\n\nPart 4: Concluding Explanation\nIn creating this data visualization, I chose to map Population to the x-axis and Taxation to the y-axis. This allows viewers to easily observe the relationship between a country’s population size and its tax revenue, which is critical for understanding how population might influence taxation. Each point represents a different country that allows for a direct visual comparison across multiple nations. The choice to use size for the Area of each country is also significant. Larger countries are represented by larger points which enables viewers to quickly assess the relative land size of each nation with its population and taxation. The use of color to differentiate countries adds another layer of information, helping viewers quickly identify which point corresponds to which country. However, I opted to remove the color legend for Country to streamline the visualization and avoid clutter. Instead, I added data labels directly to the points, ensuring that viewers can easily discern which country each point represents. This decision supports clarity and allows for immediate identification without needing to look at a legend. In terms of design principles, I used a minimalist theme (using theme_minimal()), which eliminates unnecessary gridlines and distractions and helps the viewer’s attention on the data The vertical adjustment of text labels helps to ensure that country names do not overlap with the points as much. I also carefully selected the size breaks for the Area scale and used significant thresholds that correspond to the actual data values to avoid misleading interpretations. The breaks provide a clear representation of how land area varies among the countries and allow for a more intuitive understanding of size differences. The choice of colors could be more intentional by using a color palette that is colorblind friendly which could enhance accessibility. There are a few data labels that overlap and does not show in the graph. I would also add a footnote of where I got the data. My graph effectively illustrates that larger populations do not always correlate with higher taxation levels. This visualization aligns with Playfair’s assertion that visual data can reveal complex relationships that might not be immediately apparent through numerical data alone. Overall, the design choices made in this visualization work to highlight these relationships"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "During my time at Truman, I’ve engaged in three rewarding consulting experiences through STAT 310 - Data Collection & Statistical Communication and STAT 392 - Center for Applied Statistics and Evaluation. These consulting endeavors involved extensive client interaction, survey design, data analysis, and the delivery of comprehensive presentations."
  },
  {
    "objectID": "consulting.html#international-admissions-office",
    "href": "consulting.html#international-admissions-office",
    "title": "Consulting",
    "section": "International Admissions Office",
    "text": "International Admissions Office\nSTAT 310 afforded me the opportunity to collaborate with two peers on a semester-long consulting project for the International Admissions Office. Our primary objective was to assist our client in understanding the factors driving international students to choose Truman. This involved delving into various aspects such as the admissions process, initial awareness of the university, decision-making factors, and current perceptions. Our findings were compiled into a 22-page report supplemented with insightful data visualizations, which we presented to our client in a concise and impactful 15-minute presentation."
  },
  {
    "objectID": "consulting.html#compliance-office",
    "href": "consulting.html#compliance-office",
    "title": "Consulting",
    "section": "Compliance Office",
    "text": "Compliance Office\nSTAT 392 is a semester long class where we managed multiple client projects simultaneously, ensured clear communication and meet all deadlines. The first client was for Truman State University’s Institutional Compliance Office. The objective of the survey was to evaluate the office’s effectiveness in meeting the needs of the university community and to investigate areas that could benefit from further development. This was done through creating a survey, analyzing the results, and compiling a report for the office. I joined this project after the survey had been created so my main task was analyzing the survey and communicate results. After, a paper was compiled which focused on the respondents based on whether they have or have not interacted with the office."
  },
  {
    "objectID": "consulting.html#voice-diagnostics-survey",
    "href": "consulting.html#voice-diagnostics-survey",
    "title": "Consulting",
    "section": "Voice Diagnostics Survey",
    "text": "Voice Diagnostics Survey\nAnother project in STAT 392 was creating a nationwide survey about the protocols used for instrumental assessment of voice. I had the lead in this project and my clients were a professor and a graduate student. Unlike the past surveys, I had no background in protocols used for instrumental assessment of voice. This was a new challenge for me to become familiar enough in the field to help my clients get the best results. Although this project is still on going, with the help of my clients, we created a nationwide survey after several meetings of tweaking the survey, adding sections, and getting the best ordering of questions. This survey will be sent out soon and will be analzed after"
  },
  {
    "objectID": "posts/Polling/Polling.html",
    "href": "posts/Polling/Polling.html",
    "title": "Analysis of Nate Silver’s Polling Outcomes:",
    "section": "",
    "text": "Analysis of Nate Silver’s Polling Outcomes: Observed vs. Expected Results\nGoal:\nExamine, analyze, and enhance Nate Silver’s paper on presidential polling and why the polls are wrong\nOverview:\nNate Silver’s recent article (https://www.natesilver.net/p/theres-more-herding-in-swing-state) about polling inaccuracies caught my attention, especially his insights on how polling firms adjust weights in their models to create narratives of close races. He pointed out that there’s a staggering 1 in 9.5 trillion chance of so many polls showing such tight margins. Inspired by this, I decided to recreate his results using the same data and present some visually appealing graphs.\nI used a binomial cumulative distribution function to calculate the probability of observing 193 or more close polls, assuming a null hypothesis of 0.55. The result? A jaw-dropping 1 in 8 trillion chance of seeing such close outcomes by random chance alone. This suggests there’s almost zero probability that these polls reflect the true state of the race, indicating that some results may be manipulated to appear closer than they are. This discrepancy raises serious questions about the reliability of polling outcomes.\nTo illustrate how misleading these polls can be, I created three graphs: Graph 1: A histogram showing simulated outcomes, with a red dashed line marking the observed success of 193 close polls. This emphasizes just how rare these results are. Graph 2: A bar chart comparing expected vs. observed close polls, highlighting the significant deviation from historical norms. Graph 3: A cumulative distribution function (CDF) plot that gives a broader perspective on the likelihood of various polling outcomes, further reinforcing the unusual nature of what we’re seeing.\nThese findings underline the importance of critically examining polling methods and understanding how weight adjustments can skew results. As we head into this election season, let’s stay aware of how these factors shape our perceptions and take all predictions with a grain of salt. Most importantly, go vote!\nIntroduction\nIn examining the polling data, we start with the foundational parameters that govern our analysis from Nate Silver’s paper: https://www.natesilver.net/p/theres-more-herding-in-swing-state. We have a total of 249 polls in the database, with an expected probability of a poll showing a close result under the null hypothesis set at 0.55. This probability is based on Nate Silver’s analysis, which indicates that, in a tied race, we would expect 55% of the polls to show results within ±2.5 points. This expectation arises from the margin of error formula and varies significantly depending on sample size but for a more in depth reasoning, check out his paper. In the data, the actual number of polls that showed a close result is 193 so 193/249 polls or 78% showed a close race.\nBinomial Distribution\nUsing these parameters, we calculate the probability of observing 193 or more close polls under the null hypothesis. This is accomplished through the binomial cumulative distribution function (CDF):\n\nCode# Number of polls in the database\nn &lt;- 249\n\n# Expected probability of a poll showing a close result under the null hypothesis\np &lt;- 0.55\n\n# Number of polls that actually showed a close result\nobserved_successes &lt;- 193\n\n# Calculate the probability using the binomial cumulative distribution function\nprobability &lt;- pbinom(observed_successes - 1, n, p, lower.tail = FALSE)\n\n# Convert the probability into a \"1 in X\" number\none_in_x &lt;- 1 / probability\n\n# Print the result without scientific notation\nformatted_one_in_x &lt;- format(one_in_x, scientific = FALSE)\npaste(\"One in\", formatted_one_in_x, \"or about 8 trillion which is close to Nate Silver's 9 trillion probability\")\n\n[1] \"One in 8316941692884 or about 8 trillion which is close to Nate Silver's 9 trillion probability\"\n\n\nThe calculation indicates the rarity of observing such a result under the stated null hypothesis. This perspective aligns with Nate Silver’s approach to polling, where understanding the significance of results is crucial to interpreting their implications in real-world scenarios.\nVisualizing Simulated Outcomes\nTo further illustrate the significance of our findings, we conduct 10,000 simulations to model the distribution of close poll outcomes. By creating a histogram of these simulated outcomes, we can visually assess where the observed success fits within the broader distribution. In Graph 1, the red dashed line signifies the number of observed close polls. This visual representation allows us to contextualize our findings within the realm of simulated expectations and see how rare it would be if the polls are correct.\n\nCodelibrary(ggplot2)\nset.seed(123)\nsimulations &lt;- 10000\nsimulated_outcomes &lt;- rbinom(simulations, n, p)\n\n# Create a histogram of simulated outcomes\nggplot(data.frame(simulated_outcomes), aes(x = simulated_outcomes)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", x = observed_successes - 10, y = max(table(simulated_outcomes)) * 0.9,\n           label = bquote(bold(\"Observed: \") * bold(.(observed_successes))), color = \"red\") +\n  scale_x_continuous(limits = c(min(simulated_outcomes) - 5,200)) +  # Expand x-axis range\n  labs(\n    title = \"Graph 1: Simulated Distribution of Close Poll Outcomes\",\n    x = \"Number of Close Polls\",\n    y = \"Frequency\",\n    subtitle = \"The blue distrubition is the expected number of close polls from the simulation\nand the red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nComparing Expected vs. Observed Outcomes\nNext, we can create a bar chart, Graph 3, to compare the expected number of close polls against the observed number. The expected number is derived from the initial probability (0.55), while the observed number reflects real-world polling results. This comparison highlights the disparity between expected outcomes based on historical data and the actual observed results, providing insight into the polling landscape.\n\nCode# Calculate the expected number of close polls under the null hypothesis\nexpected_successes &lt;- n * p\n\n# Create a data frame for plotting\ncomparison_data &lt;- data.frame(\n  Category = c(\"Expected\", \"Observed\"),\n  Count = c(expected_successes, observed_successes)\n)\n\n# Plot\nggplot(comparison_data, aes(x = Category, y = Count, fill = Category)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  scale_fill_manual(values = c(\"skyblue\", \"red\")) +\n  labs(\n    title = \"Graph 2: Expected vs. Observed Number of Close Polls\",\n    y = \"Number of Close Polls\",\n    x = \"\",\n    caption = \"Expected is based on a probability of 0.55. Observed is the actual count.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nCumulative Distribution Function Analysis\nLastly, we can examine the cumulative distribution function (CDF) of the binomial distribution for further insights. The CDF plot provides a comprehensive view of the probability of observing various numbers of close polls, further emphasizing the significance of our observed results in relation to expectations.\n\nCode# Calculate the CDF for the binomial distribution\nx_vals &lt;- 0:n\ncdf_vals &lt;- pbinom(x_vals, n, p)\n\n# Create a data frame for plotting\ncdf_data &lt;- data.frame(x_vals, cdf_vals)\n\n# Plot the CDF\nggplot(cdf_data, aes(x = x_vals, y = cdf_vals)) +\n  geom_line(color = \"blue\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Graph 3: Cumulative Distribution Function (CDF) of Binomial Distribution\",\n    x = \"Number of Close Polls\",\n    y = \"Cumulative Probability\",\n    caption = \"The red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nConclusion\nThe analysis of polling data reveals a stark contrast between the expected outcomes based on Nate Silver’s framework and the actual results observed in recent polls. With an expected probability of p = 0.55, representing the likelihood that a poll should show a close result within ±2.5 points in a tied race, the finding of 193 close polls out of 249 conducted indicates a significant deviation from this norm. This discrepancy translates into an extraordinarily low probability, quantifying the actual observation as a 1 in 9 trillion scenario. Such results raise critical questions about the accuracy and reliability of the polling methodologies in use.\nA noteworthy aspect of this analysis is the way polling firms often adjust weights in their models to achieve closer race outcomes. By calibrating the weights based on historical voting patterns and demographic data, pollsters can create an artificial tightness in the race. This practice might stem from a desire to generate more competitive narratives or to align with expected electoral dynamics, but it can inadvertently lead to inflated expectations for how close the race actually is."
  },
  {
    "objectID": "posts/dash data jobs/datadash.html",
    "href": "posts/dash data jobs/datadash.html",
    "title": "Data Salary Analysis Dashboard",
    "section": "",
    "text": "RShiny dashboard that explores and visualizes data science salary trends from 2020-2024.\nLink to dashboard: https://nathanbresette.shinyapps.io/data324_project/\nOverview of Project\nThe project utilized a Kaggle dataset containing anonymized salary data from the data field, with variables such as job title, experience level, company size, and geographic location. Our goal was to provide users with an interactive tool to better understand the factors influencing salaries across different roles and regions. Through various visualizations like boxplots, bar charts, and maps, the app allows users to filter the data by other criteria, making it easy to identify patterns and draw meaningful insights. This project was a valuable opportunity to apply our skills in R, data manipulation, and visualization while working collaboratively to ensure the dashboard was functional and user-friendly.\nWe highly recommend visiting the “Documentation” tab within the app for a more detailed explanation of the dataset, the features available, and how to navigate the dashboard. The app not only visualizes salary trends but also aims to answer key questions such as how location, experience, and employment type impact earnings. Users can explore whether they are being compensated fairly by leveraging the linear model integrated into the app. This collaborative effort highlights the power of data visualization in making informed career decisions, and we hope it serves as a useful resource for those entering or advancing within the data field\nGroup Code\n\nCode# Libraries\nlibrary(shiny)\nlibrary(DT)\nlibrary(viridis)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(rsconnect)\n\nworld &lt;- st_read(\"data/world.gpkg\")\n\n# # Load country geometries from rnaturalearth\n#  world &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n# \n#  world &lt;- world %&gt;%\n#    dplyr::select(\"iso_a2\", \"name\", \"continent\")\n# \n#  st_write(world, \"~/Downloads/world.gpkg\")\n \n# Load Salaries dataset and clean it\nSalaries &lt;- read.csv(\"data/datafile.csv\")\n\nSalaries_Clean &lt;- Salaries %&gt;% \n  mutate(remote_ratio = factor(case_when(\n    remote_ratio == 0 ~ \"On Site\",\n    remote_ratio == 100 ~ \"Remote\",\n    remote_ratio == 50 ~ \"Hybrid\",\n    TRUE ~ \"Other\"\n  ), levels = c(\"On Site\", \"Hybrid\", \"Remote\"))) %&gt;% \n  mutate(experience_level = factor(case_when(\n    experience_level == \"EN\" ~ \"Entry-Level\",\n    experience_level == \"MI\" ~ \"Mid-Level\",\n    experience_level == \"SE\" ~ \"Senior-Level\",\n    experience_level == \"EX\" ~ \"Executive-Level\",\n    TRUE ~ \"Other\"\n  ), levels = c(\"Entry-Level\", \"Mid-Level\", \"Senior-Level\", \"Executive-Level\"))) %&gt;% \n  mutate(company_size = factor(case_when(\n    company_size == \"S\" ~ \"Small\",\n    company_size == \"M\" ~ \"Medium\",\n    company_size == \"L\" ~ \"Large\",\n    TRUE ~ as.character(company_size)\n  ), levels = c(\"Small\", \"Medium\", \"Large\"))) %&gt;% \n  dplyr :: select(-c(\"employment_type\", \"salary_currency\")) %&gt;%\n  rename(\n    WorkLocation = remote_ratio,\n    ExperienceLevel = experience_level,\n    CompanySize = company_size,\n    Salary = salary,\n    WorkYear = work_year,\n    JobTitle = job_title\n  )\n\n# Employee residence for mapping\nSalaries_Residence &lt;- Salaries %&gt;%\n  group_by(employee_residence) %&gt;%\n  summarize(count = n(), \n            mean_salary = mean(salary_in_usd, na.rm = TRUE), \n            median_salary = median(salary_in_usd, na.rm = TRUE))\n\n\n\n\nSalaries_Clean_Regression &lt;- Salaries_Clean %&gt;% \n  mutate(salary_in_usd = as.numeric(salary_in_usd),\n         ExperienceLevel = as.factor(ExperienceLevel),\n         JobTitle = as.factor(JobTitle),\n         employee_residence = as.factor(employee_residence),\n         company_location = as.factor(company_location))\n\ndataset &lt;- Salaries_Clean_Regression\n\n# Stepwise Regression model\nstepwise_model &lt;- stepAIC(lm(salary_in_usd ~ ExperienceLevel + \n                               JobTitle + employee_residence + WorkLocation + company_location + \n                               CompanySize, data = dataset), direction = \"both\")\n\n# World geometry join\nmap_data &lt;- world %&gt;%\n  left_join(Salaries_Residence, by = c(\"iso_a2\" = \"employee_residence\"))\n\n\n\n\n\n\nui &lt;- fluidPage(\n  titlePanel(\"Data Salary (USD) Analysis (2020-2024)\"),\n  tabsetPanel(\n    tabPanel(\"Data Frame\",\n             h4(\"Data Salary Data Frame\"),\n             DTOutput(\"salaries_table\")),\n    tabPanel(\"Location Map\",\n             h4(\"Employee Residence by Country or Continent\"),\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"map_var\", \"Select Variable to Fill Map:\",\n                             choices = c(\"Employee Count\" = \"count\", \n                                         \"Mean Salary\" = \"mean_salary\", \n                                         \"Median Salary\" = \"median_salary\")),\n                 selectInput(\"group_var\", \"Group By:\",\n                             choices = c(\"Country\", \"Continent\"))\n               ),\n               mainPanel(\n                 plotOutput(\"ggplot_map\"),\n                 h4(\"Grouped Table\"),\n                 DTOutput(\"continent_table\")\n               )\n             )\n    ),\n    tabPanel(\"Salary Box Plot\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"color_var\", \"Choose X Input:\", \n                             choices = c(\"ExperienceLevel\", \"WorkLocation\", \"CompanySize\")),\n                 selectizeInput(\"filter_experience\", \"Experience Level:\", \n                                choices = levels(Salaries_Clean$ExperienceLevel), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$ExperienceLevel)),\n                 selectizeInput(\"filter_location\", \"Work Location:\", \n                                choices = levels(Salaries_Clean$WorkLocation), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$WorkLocation)),\n                 selectizeInput(\"filter_company\", \"Company Size:\", \n                                choices = levels(Salaries_Clean$CompanySize), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$CompanySize))\n               ),\n               mainPanel(\n                 plotlyOutput(\"salary_box_plot\"),\n                 h4(\"Mean and Median Salary\"),\n                 DTOutput(\"summary_table\")\n               )\n             )),\n    tabPanel(\"Work Location Proportions\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"job_title\", \n                             \"Select Job Title:\", \n                             choices = sort(unique(Salaries_Clean$JobTitle)), \n                             selected = sort(unique(Salaries_Clean$JobTitle))[1],\n                             multiple = TRUE),\n                 selectizeInput(\"filter_experience_proportions\", \n                                \"Experience Level:\", \n                                choices = levels(Salaries_Clean$ExperienceLevel), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$ExperienceLevel)),\n                 selectizeInput(\"filter_company_proportions\", \n                                \"Company Size:\", \n                                choices = levels(Salaries_Clean$CompanySize), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$CompanySize))\n               ),\n               mainPanel(\n                 plotOutput(\"remote_ratio_plot\")\n               )\n             )),\n    tabPanel(\"Salary Comparison by Residence\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"company_size_input\", \"Company Size:\",\n                                        choices = c(\"Small\" = \"S\", \"Medium\" = \"M\", \"Large\" = \"L\"),\n                                        multiple = TRUE,\n                                        selected = c(\"S\", \"M\", \"L\")),\n                            \n                 selectInput(\"experience_level_input\", \"Experience Level:\",\n                                        choices = c(\"Entry-Level\" = \"EN\", \"Mid-Level\" = \"MI\", \n                                                    \"Senior-Level\" = \"SE\", \"Executive-Level\" = \"EX\"),\n                                        multiple = TRUE,\n                                        selected = c(\"EN\", \"MI\", \"SE\", \"EX\")),\n                 selectInput(\n                   \"work_location_input\",\n                   \"Work Location:\",\n                   choices = c(\"On Site\" = \"100\", \"Hybrid\" = \"50\", \"Remote\" = \"0\"), # Dynamically generate location choices\n                   multiple = TRUE,\n                   selected = c(\"100\", \"50\", \"0\"))\n               ),\n               \n               mainPanel(\n                 plotlyOutput(\"residence_plot\")\n               )\n             )\n    ),\n    \n    \n    tabPanel(\"Salary Prediction\",\n    sidebarLayout(\n      sidebarPanel(\n        selectInput(\"experience_level\", \"Experience Level:\", choices = levels(dataset$ExperienceLevel), selected = \"EX\"),\n        selectInput(\"worklocation\", \"Work Location:\", choices = levels(dataset$WorkLocation), selected = \"Hybrid\"),\n        selectInput(\"employee_residence\", \"Employee Country Residence:\", choices = levels(dataset$employee_residence), selected = \"US\"),\n        selectInput(\"companysize\", \"Company Size:\", choices = levels(dataset$CompanySize), selected = \"Medium\"),\n                selectInput(\"jobtitle\", \"Job Title:\", choices = levels(dataset$JobTitle), selected = \"Admin & Data Analyst\"),\n        \n        # Trigger prediction\n        actionButton(\"predict\", \"Predict Salary\")\n      ),\n      \n      mainPanel(\n        tags$style(HTML(\"\n    h3 {\n      font-size: 30px;\n    }\n    #predicted_salary {\n      font-size: 24px;\n    }\n  \")),\n        h3(\"Press the button to predict the salary\"),\n        textOutput(\"predicted_salary\")\n      )\n      \n    )\n    ),\n    \n    \n    \n    # Documentation Tab UI (if you're using `tabPanel` in Shiny)\n    # Documentation Tab UI (using tags$ul and tags$li for lists)\n    tabPanel(\n      \"Documentation\",\n      h3(\"Introduction to the Data Set\"),\n      p(\"This dataset is from Kaggle and provides anonymized information about salaries in the data field for 2024. It has a variety of factors that influence salaries, including experience level, job title, employment type (Remote/Onsite/Hybrid), and company size (Small/Medium/Large). The data also includes geographic location information.\"),\n      \n      p(\"Key variables included in this dataset are:\"),\n      tags$ul(\n        tags$li(strong(\"Experience Level:\") , \" The employees experience level in the data field, categorized as 'Entry-level', 'Mid-level', and 'Senior-level', and 'Executive-level'.\"),\n        tags$li(strong(\"Job Title:\") , \" The specific job title held by the individual, such as Data Scientist, Data Analyst, or Machine Learning Engineer.\"),\n        tags$li(strong(\"Company Size:\") , \" The size of the company where the individual is employed, classified into 'Small', 'Medium', and 'Large' companies\"),\n        tags$li(strong(\"Employment Type:\") , \" Whether the individual is working remotely, onsite, or hybrid.\"),\n        tags$li(strong(\"Location:\") , \" The geographical location of the individual,  representing a country or continent.\"),\n        tags$li(strong(\"Salary:\") , \" The compensation (in USD) received by the individual, representing their annual income.\")\n      ),\n      \n      h3(\"Motivation for Creating the App\"),\n      p(\"The primary motivation behind this app is to allow users to explore and analyze the factors that influence data salaries. With the increasing interest in data as a career, there is a growing need to understand what variables affect salary ranges in this field. By visualizing the salary data across various factors such as experience level, job title, company size, and geographic location, users can gain insights into the salary landscape for data professionals.\"),\n      \n      p(\"Specific questions that motivated the creation of the app include:\"),\n      tags$ul(\n        tags$li(\"How do salaries vary by geographic location, and what factors are most influential in determining location-based pay differences?\"),\n        tags$li(\"How does salary differ by experience level, job title, and company size?\"),\n        tags$li(\"What impact does working remotely or in person have on data salaries?\"),\n        tags$li(\"Based on a linear model, are you being payed fairly?\"),\n        \n      ),\n      \n      p(\"By providing these insights, the app aims to empower users with the ability to see their own salaries, evaluate job market trends, and make data-driven career decisions in the field of data.\"),\n      \n      h3(\"How the App Works\"),\n      p(\"This app allows users to interactively explore the relationships between salary and various factors. The key features of the app are:\"),\n      tags$ul(\n        tags$li(\"Dynamic Filtering: Users can filter the data by experience level, job title, company size, and work location. This helps to focus on specific subsets of the data.\"),\n        tags$li(\"Visualizing Salary Trends: The app generates various visualizations, such as maps, boxplots, and bar charts, to show how salaries are influenced by different variables. Users can easily see how compensation changes with different levels of experience, company size, and job titles.\"),\n        tags$li(\"Geographic Insights: Location-based salary data is displayed, allowing users to compare compensation trends across different geographic areas. Due to limited data, there is a small count for several countries/continents\"),\n        tags$li(\"Interactive Graphs: Users can hover over visualizations in real time, helping them answer specific questions or dive deeper into the data for more information.\")\n      ),\n      p(\"Users can start by selecting one or more variables and adjusting filters to view trends in the dataset. They can then explore the correlations between salaries and factors such as experience level, location, and job title through the visual representations provided.\"),\n      \n      h3(\"Conclusions\"),\n      p(\"From analyzing the dataset, several trends emerge:\"),\n      tags$ul(\n        tags$li(\"Geographical Location: Although the United States and North America have some of the best mean/median pay, conclusions can not be drawn since the counts for other countries and continents are so low.\"),\n        tags$li(\"Experience Level: Higher experience levels generally correlate with higher salaries. Senior and executive roles have a significant salary increase compared to entry-level positions.\"),\n        tags$li(\"Company Size: Larger and medium companies tend to offer higher salaries, likely due to greater financial resources and the scale of operations. Due to a low count for small countries, it may not be entirely accurate\"),\n        tags$li(\"Work Location: On site jobs have a higher pay than remote and hybrid job. Hybrid jobs have a signifcantly lower count so more data would need to be collected for more accurate conclusions.\"),\n        tags$li(\"Salary Comparison by Residence: Those who work in the country they reside in have a higher salary than those who do not overall. Due to the low count of those who do not reside in the country they work in, more data would need to be collected for more accuratae conclusions\")\n      ),\n      p(\"This app helps users see these patterns and more. By interacting with the visualizations, users can draw their own conclusions about the data job market, and make informed decisions about their career paths or salary expectations.\"),\n      \n      h3(\"Citations\"),\n      p(\"Data Source: The dataset used in this app is sourced from \", a(href = \"https://www.kaggle.com/datasets/yusufdelikkaya/datascience-salaries-2024/data\", \"Kaggle: Data Science Salaries 2024\"), \".\"))\n    \n\n  )\n)\n\n\nserver &lt;- function(input, output) {\n  \n  \n  filtered_data &lt;- reactive({\n    Salaries_Clean %&gt;%\n      filter(\n        ExperienceLevel %in% input$filter_experience,\n        WorkLocation %in% input$filter_location,\n        CompanySize %in% input$filter_company\n        \n      )\n  })\n  \n  filtered_data_2 &lt;- reactive({\n    Salaries_Clean %&gt;%\n      filter(\n        ExperienceLevel %in% input$filter_experience_proportions,\n        CompanySize %in% input$filter_company_proportions,\n        JobTitle %in% input$job_title\n      ) \n  })\n  \n  # Data Table\n  output$salaries_table &lt;- renderDT({\n    datatable(Salaries_Clean)\n  })\n  \n  # Salary Box Plot\n  output$salary_box_plot &lt;- renderPlotly({\n    p &lt;- ggplot(filtered_data(), aes_string(x = input$color_var, y = \"salary_in_usd\", fill = input$color_var)) +\n      geom_boxplot() +\n      scale_y_continuous(labels = scales::comma) +  \n      labs(title = paste(\"Salary Box Plot by\", input$color_var),\n           x = input$color_var,\n           y = \"Salary (USD)\") +\n      scale_fill_viridis(discrete = TRUE, begin = 0.3) +  # Begin later for lighter purple\n      theme_minimal()\n    \n    ggplotly(p)  \n  })\n  \n  # Mean and Median Table\n  output$summary_table &lt;- renderDT({\n    Salaries_Clean %&gt;%\n      group_by(!!sym(input$color_var)) %&gt;%\n      summarise(\n        `Count` = n(),\n        `Mean Salary`= round(mean(salary_in_usd, na.rm = TRUE), 2),\n        `Median Salary` = round(median(salary_in_usd, na.rm = TRUE), 2)\n      ) \n  })\n  \n  output$remote_ratio_plot &lt;- renderPlot({\n    filtered_data_2() %&gt;%\n      group_by(WorkYear, WorkLocation) %&gt;%\n      summarise(count = n(), .groups = 'drop') %&gt;%\n      mutate(percentage = count / sum(count) * 100) %&gt;%\n      ggplot(aes(x = as.factor(WorkYear), y = percentage, fill = WorkLocation)) +\n      geom_bar(stat = \"identity\", position = \"fill\") +\n      labs(title = paste(\"Proportion of Work Locations for Selected Filters by Year\"),\n           x = \"Year\",\n           y = \"Proportion (%)\",\n           fill = \"Work Location\") +\n      scale_fill_viridis(discrete = TRUE, begin = 0.3) +  # Begin later so lighter purple\n      theme_minimal()\n  })\n  \n\n  \n  \n\n  \n  output$ggplot_map &lt;- renderPlot({\n    # Dynamically group data with ifelse\n    grouped_map_data &lt;- map_data %&gt;%\n      st_set_geometry(NULL) %&gt;%  # Drop geometry for aggregation\n      group_by(Group = if (input$group_var == \"Country\") name else continent) %&gt;%\n      summarize(\n        count = sum(count, na.rm = TRUE),\n        mean_salary = round(mean(mean_salary, na.rm = TRUE), 2),\n        median_salary = round(median(median_salary, na.rm = TRUE), 2)\n      ) %&gt;%\n      left_join(world, by = c(\"Group\" = if (input$group_var == \"Country\") \"name\" else \"continent\")) %&gt;%\n      st_as_sf()  # Reattach geometry after grouping\n    \n    # Extract the range of the selected variable\n    selected_var &lt;- grouped_map_data[[input$map_var]]\n    min_val &lt;- min(selected_var, na.rm = TRUE)\n    max_val &lt;- max(selected_var, na.rm = TRUE)\n    \n    # Plot the map\n    ggplot(data = grouped_map_data) +\n      geom_sf(aes_string(fill = input$map_var), color = \"white\") +\n      scale_fill_viridis(\n        option = \"C\",\n        direction = 1,\n        na.value = \"lightgrey\",\n        limits = c(min_val, max_val),  # Ensure the scale fits the range\n        breaks = c(min_val, max_val),  # Only show min and max in the legend\n        labels = scales::comma\n      ) +\n      labs(\n        title = paste(\n          if (input$group_var == \"Country\") \"Employee Residence by Country\" else \"Employee Residence by Continent\",\n          \"-\", if (input$map_var == \"count\") \"Employee Count\" else if (input$map_var == \"mean_salary\") \"Mean Salary\" else \"Median Salary\"\n        ),\n        fill = if (input$map_var == \"count\") \"Employee Count\" else if (input$map_var == \"mean_salary\") \"Mean Salary\" else \"Median Salary\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\"),\n        legend.text = element_text(size = 10)\n      )\n  })\n  \n\n\n  \n  \n  # Grouped Table\n  output$continent_table &lt;- renderDT({\n    grouped_data &lt;- map_data %&gt;%\n      st_set_geometry(NULL) %&gt;%  # Drop geometry column\n      group_by(group = if (input$group_var == \"Country\") name else continent) %&gt;%\n      summarize(\n        `Employee Count` = sum(count, na.rm = TRUE),\n        `Mean Salary` = round(mean(mean_salary, na.rm = TRUE), 2),\n        `Median Salary` = round(median(median_salary, na.rm = TRUE), 2)\n      ) %&gt;%\n      arrange(desc(`Employee Count`))\n    \n    datatable(grouped_data, options = list(pageLength = 5, searching = FALSE))\n  })\n  \n  output$residence_plot &lt;- renderPlotly({\n    \n    # Filter based on inputs\n    filtered_data3 &lt;- reactive({\n      Salaries %&gt;%\n        filter(\n          company_size %in% input$company_size_input,\n          experience_level %in% input$experience_level_input,\n          remote_ratio %in% input$work_location_input \n          \n        ) %&gt;%\n        mutate(\n          CompanySize = case_when(\n            company_size == \"S\" ~ \"Small\",\n            company_size == \"M\" ~ \"Medium\",\n            company_size == \"L\" ~ \"Large\",\n            TRUE ~ as.character(company_size)\n          ),\n          ExperienceLevel = case_when(\n            experience_level == \"EN\" ~ \"Entry-Level\",\n            experience_level == \"MI\" ~ \"Mid-Level\",\n            experience_level == \"SE\" ~ \"Senior-Level\",\n            experience_level == \"EX\" ~ \"Executive-Level\",\n            TRUE ~ as.character(experience_level)\n          ),\n          ResidesInWorkCountry = ifelse(\n            employee_residence == company_location,\n            \"Resides in Work Country\",\n            \"Does Not Reside in Work Country\"\n          )\n        )\n    })\n    \n    data &lt;- filtered_data3()\n    \n    # Count observations \n    counts &lt;- data %&gt;%\n      group_by(ResidesInWorkCountry) %&gt;%\n      summarise(count = n(), .groups = \"drop\")\n    \n    # Make plot\n    p &lt;- plot_ly(\n      data = data,\n      x = ~ResidesInWorkCountry,\n      y = ~salary_in_usd,\n      type = \"box\",\n      color = ~ResidesInWorkCountry,\n      colors = viridis::viridis(2),\n      boxmean = TRUE\n    ) %&gt;%\n      layout(\n        title = \"Salary Distribution for Residents vs Non-Residents in Work Country\",\n        xaxis = list(title = \"Residency Status\"),\n        yaxis = list(title = \"Salary (USD)\"),\n        showlegend = FALSE\n        \n      )\n    \n    \n    # Add count above the boxplots\n    p &lt;- p %&gt;%\n      add_annotations(\n        x = counts$ResidesInWorkCountry,\n        y = max(data$salary_in_usd, na.rm = TRUE) + 5000, # Adjust position\n        text = paste(\"Count: \", counts$count),\n        showarrow = FALSE,\n        font = list(size = 12, color = \"black\"),\n        xanchor = \"center\",\n        yanchor = \"bottom\"\n      )\n    \n    # Return the plot\n    p\n  })\n  \n\n\n  \n  observeEvent(input$predict, {\n    # Collect inputs \n    new_data &lt;- data.frame(\n      ExperienceLevel = input$experience_level,\n      WorkLocation = input$worklocation,\n      employee_residence = input$employee_residence,\n      CompanySize = input$companysize,\n      JobTitle = input$jobtitle,  \n      stringsAsFactors = TRUE  # IDK why this worked\n    )\n    \n    # Predict salary using stepwise model\n    predicted_salary &lt;- predict(stepwise_model, newdata = new_data)\n    \n    # Output salary\n    output$predicted_salary &lt;- renderText({\n      paste(\"The predicted salary in USD is $\", round(predicted_salary, 2))\n    })\n  })\n  \n  \n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/dash/ShinyDashboard.html",
    "href": "posts/dash/ShinyDashboard.html",
    "title": "Real Estate R-Shiny Dashboard",
    "section": "",
    "text": "Create and host a dashboard using R-Shiny to explore housing prices in Taiwan!\nLink to the dashboard: https://nathanbresette.shinyapps.io/real_estate/\nI’ve built countless dashboards using Tableau, PowerBI, Quicksight, and Qualtrics, but I’ve always wanted to create and host one from scratch. After learning how in my data visualization class, I decided to take the plunge!\nThis dashboard tells the story of real estate prices in New Taipei City, Taiwan, and it’s fully interactive—not just a static display of data.\nKey Features:\nPairwise Plots: Using my custom R package, InquisitR, I generated detailed pairwise plots for easy exploration of variable relationships.\nInteractive Scatter Plots & Histograms: Users can choose the third variable’s color and select regression lines for scatterplots. The histogram allows manipulation of the number of bins.\nLeaflet Map: An interactive map visualizes the geographical distribution of properties, revealing key insights into property prices across the city.\nThis experience opened up new possibilities in data storytelling and visualization. While many companies rely on drag-and-drop tools, using R offers complete control over the graphs—and best of all, it’s free! The data was very limited with only 7 variables but this was the foundational learning step!\nDashboard Code:\n\nCodelibrary(shiny)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(GGally)\nlibrary(devtools)\n\n# My package!!!\ninstall_github(\"NathanBresette/InquisitR\")\nlibrary(InquisitR)\n\n\nlibrary(tidyr) \n\n# Read the dataset\ndata &lt;- read_excel(\"Real estate valuation data set 2.xlsx\")\n\n# Round numeric columns to three decimal places\nrounded_data &lt;- data %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3))) %&gt;% \n  select(-`No`)\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Real Estate Valuation Analysis\"),\n  \n  # Remove sidebar layout\n  mainPanel(\n    tabsetPanel(\n      id = \"tab\",\n      tabPanel(\"Overview\", \n               h4(\"Explore the dataset and its summary.\"),\n               DTOutput(\"data_table\")),\n      tabPanel(\"Summary Statistics\", \n               h4(\"View key summary statistics of the data.\"),\n               DTOutput(\"summary_stats\")),  # Changed to DTOutput for better presentation\n      tabPanel(\"Pairwise Plot\", \n               h4(\"Explore pairwise relationships between multiple variables.\"),\n               plotOutput(\"pairwisePlot\")),\n      tabPanel(\"Map of Properties\", \n               h4(\"View the geographical distribution of properties.\"),\n               leafletOutput(\"propertyMap\")),\n      tabPanel(\"Distribution of House Prices\", \n               h4(\"Explore the distribution of house prices per unit area.\"),\n               sliderInput(\"bins\", \"Number of bins:\", \n                           min = 5, max = 50, value = 30),  # Add slider to control bins\n               plotOutput(\"priceDistributionPlot\")),\n      tabPanel(\"House Price vs. House Age\", \n               h4(\"Analyze the relationship between house age and price.\"),\n               radioButtons(\"smoothing_age\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_age\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceAgePlot\")),\n      tabPanel(\"House Price vs. Distance to MRT Station\", \n               h4(\"Investigate the effect of distance to MRT stations on house prices.\"),\n               radioButtons(\"smoothing_distance\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_distance\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceDistancePlot\")),\n      tabPanel(\"House Price vs. Convenience Stores\", \n               h4(\"Examine how the number of convenience stores affects house prices.\"),\n               plotOutput(\"priceConveniencePlot\"))\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Overview Data Table\n  output$data_table &lt;- renderDT({\n    datatable(rounded_data, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # Summary Statistics\n  output$summary_stats &lt;- renderDT({\n    summary_stats &lt;- rounded_data %&gt;%\n      summarise(across(everything(), \n                       list(mean = ~round(mean(.x, na.rm = TRUE), 3),\n                            median = ~round(median(.x, na.rm = TRUE), 3),\n                            sd = ~round(sd(.x, na.rm = TRUE), 3),\n                            min = ~round(min(.x, na.rm = TRUE), 3),\n                            max = ~round(max(.x, na.rm = TRUE), 3)))) %&gt;%\n      pivot_longer(everything(), names_to = c(\".value\", \"variable\"), names_sep = \"_\") %&gt;%\n      rename(Variable = variable)\n    \n    datatable(summary_stats, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # House Price vs. House Age\n  output$priceAgePlot &lt;- renderPlot({\n    if (input$color_age == \"none\") {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`, color = .data[[input$color_age]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Distance to MRT Station\n  output$priceDistancePlot &lt;- renderPlot({\n    if (input$color_distance == \"none\") {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`, color = .data[[input$color_distance]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Number of Convenience Stores\n  output$priceConveniencePlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = factor(`X4 number of convenience stores`), y = `Y house price of unit area`)) +\n      geom_boxplot(fill = \"orange\", color = \"black\") +\n      labs(title = \"House Price vs. Number of Convenience Stores\",\n           x = \"Number of Convenience Stores\", y = \"House Price of Unit Area\") +\n      theme_minimal()\n  })\n  \n  # Distribution of House Prices\n  output$priceDistributionPlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = `Y house price of unit area`)) +\n      geom_histogram(bins = input$bins, fill = \"skyblue\", color = \"black\") +  # Use input$bins\n      labs(title = \"Distribution of House Prices\",\n           x = \"House Price of Unit Area\", y = \"Frequency\") +\n      theme_minimal()\n  })\n  \n  # Leaflet map\n  output$propertyMap &lt;- renderLeaflet({\n    leaflet(rounded_data) %&gt;%\n      addTiles() %&gt;%\n      addCircleMarkers(~`X6 longitude`, ~`X5 latitude`,\n                       radius = 5, color = \"blue\", fillOpacity = 0.5,\n                       popup = ~paste(\"Price per unit area:\", `Y house price of unit area`)) %&gt;%\n      setView(lng = 121.54, lat = 24.98, zoom = 13)\n  })\n  \n  # Pairwise Plot\n  output$pairwisePlot &lt;- renderPlot({\n    correlationR(rounded_data)  \n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\nImages of Dashboard\nMap of Taiwan:\n\nCodeknitr::include_graphics(\"Taiwan_Map.png\")\n\n\n\n\n\n\n\nPairwise Plot (My package!!!)\n\nCodeknitr::include_graphics(\"Pairwise_Shiny.png\")\n\n\n\n\n\n\n\nScatterplot\n\nCodeknitr::include_graphics(\"Scatterplot_Shiny.png\")"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html",
    "href": "posts/Brain Tumor/BrainTumor.html",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "",
    "text": "Medical imaging plays a critical role in diagnosing and managing brain tumors, which vary widely in type and severity. Automating tumor classification using neural networks can support radiologists by providing faster, consistent, and potentially more accurate assessments. This project aims to learn how to classify brain tumor MRI images into four categories—glioma, meningioma, pituitary tumors, and no tumor by using convolutional neural networks (CNNs). Specifically, I leveraged transfer learning with the lightweight MobileNetV2 architecture, pretrained on ImageNet, to adapt it to the medical imaging domain. Through this project, the goals include gaining practical experience in medical image classification, preprocessing real-world MRI datasets, and evaluating model performance using various metrics and visualization techniques such as Grad-CAM."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#introduction",
    "href": "posts/Brain Tumor/BrainTumor.html#introduction",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "",
    "text": "Medical imaging plays a critical role in diagnosing and managing brain tumors, which vary widely in type and severity. Automating tumor classification using neural networks can support radiologists by providing faster, consistent, and potentially more accurate assessments. This project aims to learn how to classify brain tumor MRI images into four categories—glioma, meningioma, pituitary tumors, and no tumor by using convolutional neural networks (CNNs). Specifically, I leveraged transfer learning with the lightweight MobileNetV2 architecture, pretrained on ImageNet, to adapt it to the medical imaging domain. Through this project, the goals include gaining practical experience in medical image classification, preprocessing real-world MRI datasets, and evaluating model performance using various metrics and visualization techniques such as Grad-CAM."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#background-on-tumor-types",
    "href": "posts/Brain Tumor/BrainTumor.html#background-on-tumor-types",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Background on Tumor Types",
    "text": "Background on Tumor Types\n\nMeningioma:\nTumors arising from the meninges (protective membranes covering brain/spinal cord). Usually benign and slow-growing but may cause pressure effects depending on size/location.\nPituitary Tumors:\nTumors in the pituitary gland (hormone control center at brain base). Usually benign but can alter hormone production, causing various symptoms.\nGlioma:\nTumors originating from glial cells (which support neurons). Tend to be more aggressive and malignant (e.g., astrocytomas, glioblastomas)."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#set-the-python-environment-explicitly-for-reticulate",
    "href": "posts/Brain Tumor/BrainTumor.html#set-the-python-environment-explicitly-for-reticulate",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Set the Python environment explicitly for reticulate",
    "text": "Set the Python environment explicitly for reticulate\n\nCodereticulate::use_python(\"/Users/nathanbresette/Documents/Portfolio/.venv/bin/python\", required = TRUE)\n\nreticulate::py_config()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#imports",
    "href": "posts/Brain Tumor/BrainTumor.html#imports",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Imports",
    "text": "Imports\n\nCodeimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport visualkeras"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#download-dataset-from-kaggle",
    "href": "posts/Brain Tumor/BrainTumor.html#download-dataset-from-kaggle",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Download Dataset from Kaggle",
    "text": "Download Dataset from Kaggle\n\nCodefrom kaggle.api.kaggle_api_extended import KaggleApi\n\nOUTPUT_DIR = \"brain_tumor_classification/data/raw/kaggle_brain_tumor\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef download():\n    api = KaggleApi()\n    api.authenticate()\n    dataset = \"masoudnickparvar/brain-tumor-mri-dataset\"\n    api.dataset_download_files(dataset, path=OUTPUT_DIR, unzip=True)\n    print(\"Dataset downloaded and extracted.\")\n\nif __name__ == \"__main__\":\n    download()\n\nDataset URL: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\nDataset downloaded and extracted."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#preprocess-images",
    "href": "posts/Brain Tumor/BrainTumor.html#preprocess-images",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Preprocess Images",
    "text": "Preprocess Images\n\nResize images to 224x224 (MobileNetV2 standard)\nNormalize pixel intensities later in data generator\nSplit into train (80%), validation (20%), test sets\nSave file paths and labels as CSVs for efficient loading\n\n\nCodeRAW_DIR = \"brain_tumor_classification/data/raw/kaggle_brain_tumor\"\nPROC_DIR = \"brain_tumor_classification/data/processed/kaggle_brain_tumor\"\nIMG_SIZE = (224, 224)\nSEED = 42\nrandom.seed(SEED)\n\ndef preprocess():\n    train_dir = os.path.join(RAW_DIR, \"Training\")\n    test_dir = os.path.join(RAW_DIR, \"Testing\")\n\n    classes = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n\n    # Create output dirs\n    for split in [\"train\", \"val\", \"test\"]:\n        for cls in classes:\n            os.makedirs(os.path.join(PROC_DIR, split, cls), exist_ok=True)\n\n    # Load train+val images\n    train_val_images = []\n    for cls in classes:\n        cls_path = os.path.join(train_dir, cls)\n        for img_name in os.listdir(cls_path):\n            train_val_images.append((os.path.join(cls_path, img_name), cls))\n\n    random.shuffle(train_val_images)\n    n = len(train_val_images)\n    train_cutoff = int(0.8 * n)  # 80% train, 20% val\n\n    train_images = train_val_images[:train_cutoff]\n    val_images = train_val_images[train_cutoff:]\n\n    # Load test images\n    test_images = []\n    for cls in classes:\n        cls_path = os.path.join(test_dir, cls)\n        for img_name in os.listdir(cls_path):\n            test_images.append((os.path.join(cls_path, img_name), cls))\n\n    def save_split(images, split_name):\n        records = []\n        for i, (src_path, label) in enumerate(images):\n            img = Image.open(src_path).convert(\"RGB\")\n            img = img.resize(IMG_SIZE)\n            filename = f\"{label}_{i:05d}.png\"\n            out_path = os.path.join(PROC_DIR, split_name, label, filename)\n            img.save(out_path)\n            records.append({\"filepath\": out_path, \"label\": label})\n        df = pd.DataFrame(records)\n        df.to_csv(os.path.join(PROC_DIR, f\"{split_name}_labels.csv\"), index=False)\n\n    save_split(train_images, \"train\")\n    save_split(val_images, \"val\")\n    save_split(test_images, \"test\")\n\n    print(\"Preprocessing complete!\")\n\nif __name__ == \"__main__\":\n    preprocess()\n\nPreprocessing complete!"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#build-model",
    "href": "posts/Brain Tumor/BrainTumor.html#build-model",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Build Model",
    "text": "Build Model\n\nLoad CSV label files\nMap labels to numeric classes\nCreate image data generators with pixel rescaling\n\n\nCodePROC_DIR = \"brain_tumor_classification/data/processed/kaggle_brain_tumor\"\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\ntrain_df = pd.read_csv(os.path.join(PROC_DIR, \"train_labels.csv\"))\nval_df = pd.read_csv(os.path.join(PROC_DIR, \"val_labels.csv\"))\n\nlabel_map = {label: idx for idx, label in enumerate(sorted(train_df['label'].unique()))}\ntrain_df[\"class\"] = train_df[\"label\"].map(label_map)\nval_df[\"class\"] = val_df[\"label\"].map(label_map)\n\ndatagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = datagen.flow_from_dataframe(\n    train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42\n)\n\nFound 4569 validated image filenames belonging to 4 classes.\n\nCodeval_gen = datagen.flow_from_dataframe(\n    val_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nFound 1143 validated image filenames belonging to 4 classes."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#visualize-dataset-distribution",
    "href": "posts/Brain Tumor/BrainTumor.html#visualize-dataset-distribution",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Visualize Dataset Distribution",
    "text": "Visualize Dataset Distribution\nDistribution of our four classes which are relatively very balanced\n\nCodetrain_counts = train_df['label'].value_counts().reset_index()\ntrain_counts.columns = ['Class', 'Count']\ntrain_counts['Dataset'] = 'Train'\n\nval_counts = val_df['label'].value_counts().reset_index()\nval_counts.columns = ['Class', 'Count']\nval_counts['Dataset'] = 'Validation'\n\ncounts_df = pd.concat([train_counts, val_counts])\n\nprint(\"Image counts per class:\")\n\nImage counts per class:\n\nCodeprint(counts_df.pivot(index='Class', columns='Dataset', values='Count').fillna(0))\n\nDataset     Train  Validation\nClass                        \nglioma       1056         265\nmeningioma   1052         287\nnotumor      1287         308\npituitary    1174         283\n\nCodeplt.figure(figsize=(8, 5))\nsns.barplot(data=counts_df, x='Class', y='Count', hue='Dataset')\nplt.title('Number of Images per Class in Train and Validation Sets')\nplt.ylabel('Number of Images')\nplt.xlabel('Class')\nplt.show()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#build-model-1",
    "href": "posts/Brain Tumor/BrainTumor.html#build-model-1",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Build Model",
    "text": "Build Model\nI chose to go with MobileNetV2 since it’s a lightweight, efficient CNN pretrained on ImageNet. This makes it ideal for transfer learning on smaller medical image datasets.\n\nInput Shape (224, 224, 3): This matches MobileNetV2’s expected input dimensions, enabling reuse of its pretrained weights without modification.\nLearning Rate (0.0001): A low learning rate ensures stable fine-tuning and prevents large gradient updates, which is important when using a pretrained base.\n\n\nCodebase_model = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\nbase_model.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\noutput = Dense(len(label_map), activation=\"softmax\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\")]\n)"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#train-model",
    "href": "posts/Brain Tumor/BrainTumor.html#train-model",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Train Model",
    "text": "Train Model\n\nEpochs (5): A small number of epochs was chosen to avoid overfitting and speed up training during initial experiments, especially since the base model is frozen.\n\n\nCodehistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=5\n)\nCodemodel.save(\"models/mobilenetv2_model.h5\")"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#visualize-model-architecture",
    "href": "posts/Brain Tumor/BrainTumor.html#visualize-model-architecture",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Visualize Model Architecture",
    "text": "Visualize Model Architecture\nThis is the MobileNetV2 architecture!\n\nCodeimg = visualkeras.layered_view(model, legend=True)\nplt.figure(figsize=(20, 8))\nplt.imshow(img)\nplt.axis('off')\n\n(np.float64(-0.5), np.float64(7822.5), np.float64(1232.5), np.float64(-0.5))\n\nCodeplt.show()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#plot-training-accuracy-over-epochs",
    "href": "posts/Brain Tumor/BrainTumor.html#plot-training-accuracy-over-epochs",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Plot Training Accuracy Over Epochs",
    "text": "Plot Training Accuracy Over Epochs\n\nCodetr_acc = history.history['accuracy']\ntr_loss = history.history['loss']\ntr_per = history.history['precision']\ntr_recall = history.history['recall']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nval_per = history.history['val_precision']\nval_recall = history.history['val_recall']\n\nindex_loss = np.argmin(val_loss)\nval_lowest = val_loss[index_loss]\nindex_acc = np.argmax(val_acc)\nacc_highest = val_acc[index_acc]\nindex_precision = np.argmax(val_per)\nper_highest = val_per[index_precision]\nindex_recall = np.argmax(val_recall)\nrecall_highest = val_recall[index_recall]\n\nEpochs = [i + 1 for i in range(len(tr_acc))]\nloss_label = f'Best epoch = {str(index_loss + 1)}'\nacc_label = f'Best epoch = {str(index_acc + 1)}'\nper_label = f'Best epoch = {str(index_precision + 1)}'\nrecall_label = f'Best epoch = {str(index_recall + 1)}'\n\nplt.figure(figsize=(20, 6))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_loss, 'r', label='Training loss')\nplt.plot(Epochs, val_loss, 'g', label='Validation loss')\nplt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\nplt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\nplt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.suptitle('Model Training Metrics - Loss and Accuracy', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nLoss consistently decreases over the 5 epochs for both training and validation.the model is learning effectively and not overfitting in the short term.\nValidation accuracy lags slightly behind training accuracy but follows a similar trend, with the best performance reached at epoch 5.\n\n\nCodeplt.figure(figsize=(20, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_per, 'r', label='Precision')\nplt.plot(Epochs, val_per, 'g', label='Validation Precision')\nplt.scatter(index_precision + 1, per_highest, s=150, c='blue', label=per_label)\nplt.title('Precision and Validation Precision')\nplt.xlabel('Epochs')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_recall, 'r', label='Recall')\nplt.plot(Epochs, val_recall, 'g', label='Validation Recall')\nplt.scatter(index_recall + 1, recall_highest, s=150, c='blue', label=recall_label)\nplt.title('Recall and Validation Recall')\nplt.xlabel('Epochs')\nplt.ylabel('Recall')\nplt.legend()\nplt.grid(True)\n\nplt.suptitle('Model Training Metrics - Precision and Recall', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nPrecision steadily improves for both training and validation, with training precision slightly higher than validation, so the model generalizes well but still performs better on the training data.\nRecall also increases consistently, with validation recall slightly higher than training recall in early epochs. This suggests the model became more sensitive to true positives over time, and maintained good generalization.\nEpoch 5 is marked as the best based on all metrics. More epochs could potentially further improve performance, but my Mac is quite slow already."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#evaluate-model-with-classification-report-and-confusion-matrix",
    "href": "posts/Brain Tumor/BrainTumor.html#evaluate-model-with-classification-report-and-confusion-matrix",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Evaluate Model with Classification Report and Confusion Matrix",
    "text": "Evaluate Model with Classification Report and Confusion Matrix\n\nCodefrom sklearn.metrics import classification_report, confusion_matrix\n\ny_true = val_gen.classes\ny_pred = model.predict(val_gen)\n\n\n[1m 1/36[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m32s[0m 931ms/step\n[1m 2/36[0m [32m━[0m[37m━━━━━━━━━━━━━━━━━━━[0m [1m10s[0m 300ms/step\n[1m 3/36[0m [32m━[0m[37m━━━━━━━━━━━━━━━━━━━[0m [1m9s[0m 294ms/step \n[1m 4/36[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m9s[0m 288ms/step\n[1m 5/36[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m8s[0m 289ms/step\n[1m 6/36[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m8s[0m 287ms/step\n[1m 7/36[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m8s[0m 286ms/step\n[1m 8/36[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m7s[0m 285ms/step\n[1m 9/36[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m7s[0m 284ms/step\n[1m10/36[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m7s[0m 282ms/step\n[1m11/36[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m7s[0m 281ms/step\n[1m12/36[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m6s[0m 280ms/step\n[1m13/36[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m6s[0m 279ms/step\n[1m14/36[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m6s[0m 278ms/step\n[1m15/36[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m5s[0m 278ms/step\n[1m16/36[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m5s[0m 277ms/step\n[1m17/36[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m5s[0m 276ms/step\n[1m18/36[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m4s[0m 276ms/step\n[1m19/36[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m4s[0m 275ms/step\n[1m20/36[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m4s[0m 275ms/step\n[1m21/36[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m4s[0m 275ms/step\n[1m22/36[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m3s[0m 275ms/step\n[1m23/36[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m3s[0m 275ms/step\n[1m24/36[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m3s[0m 275ms/step\n[1m25/36[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m3s[0m 275ms/step\n[1m26/36[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m2s[0m 275ms/step\n[1m27/36[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m2s[0m 275ms/step\n[1m28/36[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m2s[0m 275ms/step\n[1m29/36[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m1s[0m 275ms/step\n[1m30/36[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m1s[0m 276ms/step\n[1m31/36[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m1s[0m 278ms/step\n[1m32/36[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m1s[0m 279ms/step\n[1m33/36[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 280ms/step\n[1m34/36[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 280ms/step\n[1m35/36[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 280ms/step\n[1m36/36[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 290ms/step\n[1m36/36[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m11s[0m 290ms/step\n\nCodey_pred_classes = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_true, y_pred_classes, target_names=label_map.keys()))\n\n              precision    recall  f1-score   support\n\n      glioma       0.85      0.77      0.81       265\n  meningioma       0.73      0.66      0.69       287\n     notumor       0.90      0.93      0.91       308\n   pituitary       0.81      0.94      0.87       283\n\n    accuracy                           0.83      1143\n   macro avg       0.82      0.82      0.82      1143\nweighted avg       0.82      0.83      0.82      1143\n\n\n\nCodecm = confusion_matrix(y_true, y_pred_classes)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nBest performance was on “notumor” and “pituitary” classes, with F1-scores of 0.91 and 0.88 respectively.\n“Meningioma” was the most challenging class, with the lowest recall (0.61), so many meningioma images were misclassified.\nOverall model accuracy is 83%, with balanced precision and recall across most classes. It showed generally strong performance but room for improvement in class-specific sensitivity."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#grad-cam",
    "href": "posts/Brain Tumor/BrainTumor.html#grad-cam",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Grad-CAM",
    "text": "Grad-CAM\nThis function generates Grad-CAM heatmaps for several images in a batch, showing where the model is focusing when making predictions. It displays both the true and predicted class for each image, helping interpret model behavior visually.\nIn Grad-CAM: - Brighter (hotter) colors = more important - Darker (cooler) colors = less important\n\nCodeimport cv2\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        inputs=model.input,\n        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef display_gradcam(img, heatmap, alpha=0.4):\n    # Resize heatmap to match image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply colormap (you can change COLORMAP_JET to any OpenCV colormap)\n    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Convert image to BGR for OpenCV overlay\n    img_bgr = np.uint8(255 * img)\n    if img_bgr.shape[-1] == 1:\n        img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n\n    # Overlay heatmap on image\n    overlayed_img = cv2.addWeighted(img_bgr, 1 - alpha, heatmap_color, alpha, 0)\n    \n    # Convert back to RGB for matplotlib\n    overlayed_img = cv2.cvtColor(overlayed_img, cv2.COLOR_BGR2RGB)\n\n    # Show\n    plt.figure(figsize=(6, 6))\n    plt.imshow(overlayed_img)\n    plt.axis(\"off\")\n    plt.title(\"Grad-CAM Overlay\")\n    plt.show()\n\n\ndef gradcam_on_batch(generator, model, last_conv_layer_name=\"Conv_1\", num_images=5):\n    images, labels = next(generator)\n    class_names = list(generator.class_indices.keys())\n\n    displayed = 0\n    for i in range(len(images)):\n        if displayed &gt;= num_images:\n            break\n\n        img = images[i]\n        label_idx = np.argmax(labels[i])\n        true_label = class_names[label_idx]\n\n        img_exp = np.expand_dims(img, axis=0)\n        preds = model.predict(img_exp)\n        pred_idx = np.argmax(preds[0])\n        pred_label = class_names[pred_idx]\n\n        if pred_label == true_label:\n            heatmap = make_gradcam_heatmap(img_exp, model, last_conv_layer_name)\n            print(f\"Image {i+1}: True = {true_label}, Predicted = {pred_label} (Correct)\")\n            display_gradcam(img, heatmap)\n            displayed += 1\n\ngradcam_on_batch(val_gen, model, last_conv_layer_name=\"Conv_1\", num_images=3)\n\n\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 450ms/step\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 460ms/step\nImage 1: True = glioma, Predicted = glioma (Correct)\n\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 25ms/step\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 34ms/step\nImage 2: True = pituitary, Predicted = pituitary (Correct)\n\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 25ms/step\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 34ms/step\n\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 24ms/step\n[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 33ms/step\nImage 4: True = notumor, Predicted = notumor (Correct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the area differs in each image: - Model focuses on features it finds most discriminative, which vary by image. - Differences in predictions, confidence, or tumor appearance shift attention. - Grad-CAM uses low-res feature maps, making heatmaps coarse and inconsistent."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#conclusions",
    "href": "posts/Brain Tumor/BrainTumor.html#conclusions",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Conclusions",
    "text": "Conclusions\nThe MobileNetV2-based model achieved an overall classification accuracy of approximately 83% on the validation set. It demonstrated promising performance for this multi-class brain tumor classification task with limited computing power. The model showed strong precision and recall for “no tumor” and “pituitary tumor” classes. However, the “meningioma” class proved more challenging, with the lowest recall score, suggesting room for improvement in identifying this tumor type.\nVisualization tools such as Grad-CAM provided useful interpretability, highlighting the regions of MRI scans the model focuses on during classification. This aligns with the goal of developing more transparent and explainable AI models in medical imaging.\nOverall, the project validated the feasibility of transfer learning for brain tumor classification and highlighted the importance of dataset balancing, preprocessing, and careful evaluation in medical image analysis. It was a great learning project"
  },
  {
    "objectID": "posts/This Website/ThisWebsite.html",
    "href": "posts/This Website/ThisWebsite.html",
    "title": "This Website!",
    "section": "",
    "text": "This was my first ‘frontend’ project that was done through lots of trial and erorr, tutorials, and reading. Overall, I am very pleased with the end result and would recommend Quarto for a portfolio. Once the website was created, it has been very easy to upload new files."
  },
  {
    "objectID": "datafest_file/CourseKata.html",
    "href": "datafest_file/CourseKata.html",
    "title": "CourseKata",
    "section": "",
    "text": "24 hour analysis on CourseKata, an online book on statistics. Data cleaning, exploratory data analysis, XGBoost, and suggestions for improvement ## Overview of Project Our initial area of interest upon reception of the data was the subjective student responses in the checkpoints_pulse table, as the developers of CourseKata, Jim Stigler and Ji Son, were primarily interested in the students’ opinions of the textbook. Unfortunately, this data proved unfruitful, as there was no variation in response for any factor we could find. Since subjective measures had no analytical value, we pivoted to looking at objective measures, starting with the EOC variable in the checkpoints_eoc table, which is the final percentage of questions each student answered correctly on the end of chapter (EOC) quiz. This data was much more diverse and had some interesting potential factors that might influence it. We ended up modifying the EOC data into a binary pass/fail variable with a division at 0.6 (for a 60% pass rate) that focused on book College(ABC).\nOur group used a gradient-boosted classification tree to chunk down the variables due to the high cardinality so we could use it as an exploratory model. The model started with 20 gradient classification models and picked the best one utilizing racing anova. The final model had 1649 trees. The importance of variables was calculated by how often they were utilized in the final fitted model to make a decision. The important variables were sum of engagement, average attempt, institution, and chapter. The model produced an AUC of 0.847 with an accuracy of 78.3%.\nOf the top 4 variables our model found to be important, two were student-determined variables and two were environment-determined variables. The total engagement time as well as the average attempts per question, the latter of which we engineered ourselves based on n_possible and n_attempted, were the two most influential variables regarding student pass/fail rate, and were the two student-determined variables. Students who obtained over a 60% on the EOC quizzes spent more time utilizing the textbook than students who obtained less than a 60%. Additionally, students with an average attempts per question over 3 were more likely to have an EOC score below 60%, with less students being over that threshold the higher the average attempts were. The book version for College (ABC) is important to the pass/fail rate of the students with more students improving with newer book versions. There is a large variance in pass/fail rates for institutions with some passing at 75% and others failing at 75%.\nOur next steps, if there was more time, would be to remove or rework the subjective “pulse” questions, looking into different book versions, and investigate discrepancies amongst institutions.\n\n\n\nWe Won!\nOut of 20 teams, we were able to take the prize of best in show!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Brain Tumor Classification with TensorFlow and MobileNetV2\n\n\n\nMedical Imaging\n\nDeep Learning\n\nTensorFlow\n\nMobileNetV2\n\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nMRI Super-Resolution with Diffusion Models\n\n\n\nMedical Imaging\n\nDeep Learning\n\nPyTorch\n\nDiffusion Models\n\n\n\n\n\n\n\n\n\nJul 2, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nCollege Football Playoffs Ticket Price Analysis\n\n\n\nAWS Lambda\n\nAWS Eventbridge\n\nAWS S3\n\nPython\n\nWebscraping\n\nR\n\n\n\n\n\n\n\n\n\nJan 3, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nData Salary Analysis Dashboard\n\n\n\nR-Shiny\n\nTidyverse\n\nPlotly\n\nRegression\n\nPython\n\nWebscraping\n\nR\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nNathan Bresette, Bek Usmonov, Riccardo Crapanzano\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Nate Silver’s Polling Outcomes:\n\n\n\nBinomial Distributions\n\nVisualizations\n\nWeights\n\nR\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nReal Estate R-Shiny Dashboard\n\n\n\nDashboard\n\nData Visualization\n\nMaps\n\nInteractive\n\nR\n\nR-Shiny\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nPlayfair’s Recreation & Improvement\n\n\n\nGraphical Critique\n\nData Visualization\n\nR\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nInquisitR\n\n\n\nPackage Creation\n\nFunctions\n\nR\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nThis Website!\n\n\n\nQuarto\n\nHTML\n\nSCSS\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nNCAA Basketball Analysis\n\n\n\nWebscraping\n\nPCA\n\nNeural Network\n\nXGBoost\n\nR\n\nPlotly\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nNathan Bresette\n\n\n\n\n\nNo matching items"
  }
]