[
  {
    "objectID": "aboutme_index.html",
    "href": "aboutme_index.html",
    "title": "Nathan Bresette",
    "section": "",
    "text": "I am a PhD student in Data Science & Informatics (Bioinformatics) at the University of Missouri, where my research focuses on applying AI and advanced analytics to healthcare challenges. In Dr.¬†Ai-Ling Lin‚Äôs lab, I analyze multi-omic datasets to identify biomarkers driving Alzheimer‚Äôs disease and explore AI-driven drug repurposing strategies. In Dr.¬†Changyu Sun‚Äôs lab, I work on improving MRI image acquisition methods to reduce scan times and enhance diagnostic accuracy.\n¬†\nI hold a B.S. in Statistics (Data Science concentration) with a minor in Mathematics from Truman State University, where I also served as a Teaching Assistant and Student Advisor. During my time there, I gained practical experience through internships at SelectQuote, applying NLP, optimization, and predictive modeling to business problems, and through research and consulting projects in data analysis and visualization.\n¬†\nIn addition, I founded RgentAI LLC, where I developed AI-powered data science assistants and full-stack applications that help bridge the gap between machine learning research and practical use cases. This portfolio highlights a diverse range of my projects‚Äîfrom statistical modeling and NLP pipelines to healthcare research and software development‚Äîthat showcase both my technical skills and my drive to make a meaningful impact through data science."
  },
  {
    "objectID": "datafest.html",
    "href": "datafest.html",
    "title": "DataFest",
    "section": "",
    "text": "DataFest is a competition of data in which teams of undergraduates work ‚Äúaround the clock‚Äù to discover and share meaning in a large, rich, and complex data set. It is a nationally coordinated weekend-long data analysis competition and challenges students to find their own story to tell with the data that is meaningful to the data donor.\nAt the competition, students received a large data set and were given less than 24 hours to create an executive summary report and five-minute presentation that tells a story from the data, complete with statistical analysis and interesting visualizations. Teams were asked to explore real, proprietary data.\nüèÜ Best in Show Winner - 2024 & 2025\nAt Truman, we had a group of four that worked on several smaller projects before the actual competition. Each year, we participated in both a practice DataFest and the actual competition, winning Best in Show in both 2024 and 2025.\nDue to 4 people writing code with a limited timeframe, it is not in the cleanest format. I would recommend looking at the slideshow for our finished projects.\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual DataFest - Real Estate Analysis\n\n\n\nRandomForest\n\nALE Plots\n\nMaps\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nNathan Bresette, Matthew Wolz, Bek Usmonov, Jossua Chartier\n\n\n\n\n\n\n\n\n\n\n\n\nPractice DataFest - Royals Analysis\n\n\n\nRandomForest\n\nVIP\n\nVisualizations\n\n\n\n\n\n\n\n\n\nMar 23, 2025\n\n\nNathan Bresette, Matthew Wolz, Bek Usmonov, Jossua Chartier\n\n\n\n\n\n\n\n\n\n\n\n\nActual DataFest - CourseKata\n\n\n\nData Cleaning\n\nXGBoost\n\nPresentation\n\nR\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\n\n\n\n\n\n\n\nPractice DataFest - Traffic Impact\n\n\n\nData Cleaning\n\nCART\n\nR\n\nPresentation\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datafest_file/Traffic Impact.html",
    "href": "datafest_file/Traffic Impact.html",
    "title": "Practice DataFest - Traffic Impact",
    "section": "",
    "text": "Analysis on Missouri crashes from 2021 to 2023 with a two-stage predictive model‚Äîcomprising a decision tree and a CART regression tree was developed."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#overview-of-project",
    "href": "datafest_file/Traffic Impact.html#overview-of-project",
    "title": "Practice DataFest - Traffic Impact",
    "section": "Overview of Project",
    "text": "Overview of Project\nThe traffic dataset contains 7.7 million cases of crashes across the United States, from 2016 to 2024, as well as the locations of the crash, weather conditions, features of surrounding road, and the severity of the impact of the crash on traffic conditions. For purposes of our analysis, we focused on crashes in Missouri during the years 2021, 2022, and 2023. To enhance analysis, our group engineered new features within the dataset, highlighted by a new feature we called Traffic Impact. This variable was based on how long traffic was inhibited, as well as the total distance of road over which traffic was impacted. Our other main enhancement was refactoring and separating the weather categories into two separate features: one for the type of weather, and another for the severity of the weather.\nExploratory analysis of the new feature Traffic Impact to the weather conditions during which crashes occurred, we found that crashes that occurred during conditions involving snow and ice had a significantly higher Traffic Impact score than crashes in any other conditions. Additionally, when controlling for conditions with snow and ice, we found that temperature had no significant effect on the Traffic Impact Score.\nTo predict the severity of traffic impact, we utilized a two-stage model: one which categorizes if the crash has an impact, and another that predicts its Traffic Impact score. Models were trained on crashes from 2021 and tested on the crashes from 2022. The first stage model utilized is a categorical decision tree which resulted in an accuracy of 81.51%, a sensitivity of 91.41%, and specificity of 21.92%. The second stage model is a CART regression tree that resulted in a RMSEtrain of 1.32 and RMSEtest of 1.578. The second stage model predicted values had a correlation of 0.468 with the actual values."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#slideshow",
    "href": "datafest_file/Traffic Impact.html#slideshow",
    "title": "Practice DataFest - Traffic Impact",
    "section": "Slideshow",
    "text": "Slideshow"
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html",
    "href": "posts/DDPM/DDPM_Port.html",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "",
    "text": "MRI super-resolution using diffusion models on limited hardware with data augmentation and evaluation metrics."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#purpose-of-project",
    "href": "posts/DDPM/DDPM_Port.html#purpose-of-project",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "Purpose of Project",
    "text": "Purpose of Project\nMagnetic Resonance Imaging (MRI) is vital for neuroscience and medical diagnostics. However, high-resolution MR images is time-consuming and expensive. It is also constrained by hardware limitations and patient comfort. Super-resolution techniques offer a powerful solution by reconstructing high-quality images from lower-resolution inputs which enhances clinical utility without increasing scan time. In this project, I explore the application of diffusion-based deep learning models to MRI super-resolution, leveraging the OASIS dataset."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#methods",
    "href": "posts/DDPM/DDPM_Port.html#methods",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "Methods",
    "text": "Methods\nThe primary model used in this project is a UNet-based Denoising Diffusion Probabilistic Model (DDPM), implemented using HuggingFace‚Äôs diffusers library. The model learns to iteratively remove noise from high-resolution images while being conditioned on upsampled low-resolution inputs. Compared to GANs, diffusion models offer improved training stability and produce more diverse, less artifact-prone outputs‚Äîkey benefits in medical imaging tasks where realism and detail are critical. See ‚ÄòNathan‚Äôs Notes‚Äô for my detailed notes on the differences."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#computing-limitations",
    "href": "posts/DDPM/DDPM_Port.html#computing-limitations",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "Computing Limitations",
    "text": "Computing Limitations\nOne of the core challenges of this project was working on a Mac machine with limited computational power. This constraint required significant adaptations to standard approaches‚Äîsuch as reducing image dimensions, limiting batch sizes, and restricting the number of training epochs. I extracted and saved individual 2D grayscale slices from 3D MRI volumes, a practical compromise that preserved key anatomical information while reducing memory load. To further address data scarcity and prevent overfitting, I applied a variety of augmentation techniques, including rotation, flipping, affine transforms, and brightness/contrast jittering. This helped increase my tiny dataset of 8 images significantly\nI would explore larger batch sizes, deeper UNets, and longer diffusion chains for improved image fidelity. In particular, models like SR3 (Super-Resolution via Repeated Refinement) or latent diffusion models (LDMs) trained with multi-resolution or multi-slice 3D context could significantly enhance output quality. Additionally, integrating perceptual loss functions (e.g., VGG-based) and domain-specific priors (like anatomical landmarks) could offer further gains in realism and utility."
  },
  {
    "objectID": "posts/DDPM/DDPM_Port.html#takeaways",
    "href": "posts/DDPM/DDPM_Port.html#takeaways",
    "title": "MRI Super-Resolution with Diffusion Models",
    "section": "Takeaways",
    "text": "Takeaways\nDespite the poor performance of the model‚Äîevidenced by noisy, blurry outputs and low evaluation scores (PSNR ‚âà 8.09 dB, SSIM ‚âà 0.165), this project was an invaluable learning experience. I successfully implemented a complete diffusion-based super-resolution pipeline from scratch, including dataset preparation, augmentation, model training, and inference, all within the limitations of a Mac. If I had access to more computing power, I would experiment with longer training schedules, more complex U-Net backbones, and potentially newer models like DDIM or StableSR to achieve higher-quality reconstructions."
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html",
    "href": "posts/ticketdata/ticketanalysis.html",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "",
    "text": "College Football Playoff Semifinals Ticket Price Analysis using AWS, Python, and R"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#data-cleaning",
    "href": "posts/ticketdata/ticketanalysis.html#data-cleaning",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nCodelibrary(tidyverse)\nTickets &lt;- read.csv(\"~/Downloads/tickets_grouped_by_url (22).csv\")\n\nTickets_clean &lt;- Tickets %&gt;% \n  filter(Section != \"N/A\") %&gt;% \n  mutate(Event.Name = case_when(\n    Event.Name == \"Fiesta Bowl: Boise State vs Penn State/SMU - CFP Quarterfinal\" ~ \"Fiesta Bowl: Boise State vs Penn State - CFP Quarterfinal\",\n    Event.Name == \"Sugar Bowl: Georgia vs Notre Dame/Indiana - CFP Quarterfinal\" ~ \"Sugar Bowl: Georgia vs Notre Dame - CFP Quarterfinal\",\n    Event.Name == \"Peach Bowl: Arizona State vs Texas/Clemson - CFP Quarterfinal\" ~ \"Peach Bowl: Arizona State vs Texas - CFP Quarterfinal\",\n    Event.Name == \"Rose Bowl: Oregon vs Ohio State/Tennessee - CFP Quarterfinal\" ~ \"Rose Bowl: Oregon vs Ohio State - CFP Quarterfinal\",\n    TRUE ~ Event.Name\n  )) %&gt;%\n  mutate(Event.Name = case_when(\n    Event.Name == \"Sugar Bowl: Georgia vs Notre Dame - CFP Quarterfinal\" ~ \"Sugar Bowl: Georgia vs Notre Dame\",\n    Event.Name == \"Fiesta Bowl: Boise State vs Penn State - CFP Quarterfinal\" ~ \"Fiesta Bowl: Boise State vs Penn State\",\n    Event.Name == \"Peach Bowl: Arizona State vs Texas - CFP Quarterfinal\" ~ \"Peach Bowl: Arizona State vs Texas\",\n    Event.Name == \"Rose Bowl: Oregon vs Ohio State - CFP Quarterfinal\" ~ \"Rose Bowl: Oregon vs Ohio State\",\n  )) %&gt;% \n  separate(Row_Details, into = c(\"section_area\", \"row_number\"), sep = \",\") %&gt;%\n  mutate(row_number = gsub(\"Row \", \"\", row_number)) %&gt;% \n  mutate(Price = as.numeric(Price)) %&gt;% \n    mutate(\n    Time.Scraped = ymd_hms(Time.Scraped),  # Convert to datetime\n    Scraped.Date = as.Date(Time.Scraped),  # Extract date\n    Scraped.Hour = hour(Time.Scraped),     # Extract hour\n    Scraped.Day = wday(Time.Scraped, label = TRUE)  # Extract day of the week\n  ) %&gt;% \n  filter(!is.na(Price))\n\n\n### Separate into different bowls for analysis\nSugar &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Sugar Bowl: Georgia vs Notre Dame\")\n\nFiesta &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Fiesta Bowl: Boise State vs Penn State\")\n\nPeach &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Peach Bowl: Arizona State vs Texas\")\n\nRose &lt;- Tickets_clean %&gt;% \n  filter(Event.Name == \"Rose Bowl: Oregon vs Ohio State\")"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#mean-ticket-price-by-bowl-game",
    "href": "posts/ticketdata/ticketanalysis.html#mean-ticket-price-by-bowl-game",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Mean Ticket Price by Bowl Game",
    "text": "Mean Ticket Price by Bowl Game\n\nCode# Summarize mean price over time for each bowl\nSugar_summary &lt;- Sugar %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Sugar Bowl\")\n\nFiesta_summary &lt;- Fiesta %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Fiesta Bowl\")\n\nPeach_summary &lt;- Peach %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Peach Bowl\")\n\nRose_summary &lt;- Rose %&gt;%\n  group_by(Time.Scraped) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Rose Bowl\")\n\n# Combine all summaries\nBowl_summary &lt;- bind_rows(Sugar_summary, Fiesta_summary, Peach_summary, Rose_summary)\n\n\nggplot(Bowl_summary, aes(x = Time.Scraped, y = mean_price, color = Bowl)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time for Bowl Games\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Bowl Game\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#section-mean-price-by-bowl-game",
    "href": "posts/ticketdata/ticketanalysis.html#section-mean-price-by-bowl-game",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Section Mean Price by Bowl Game",
    "text": "Section Mean Price by Bowl Game\n\nCodeSugar_summary &lt;- Sugar %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Sugar Bowl\")\n\nFiesta_summary &lt;- Fiesta %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Fiesta Bowl\")\n\nPeach_summary &lt;- Peach %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Peach Bowl\")\n\nRose_summary &lt;- Rose %&gt;%\n  group_by(Time.Scraped, Section) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Bowl = \"Rose Bowl\")\n\n\n# Plotting the data by Section\nggplot(Sugar_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Sugar Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Peach_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Peach Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Fiesta_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Fiesta Bowl Games\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nCodeggplot(Rose_summary, aes(x = Time.Scraped, y = mean_price, color = as.factor(Section))) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Mean Ticket Prices Over Time by Section for Rose Bowl Game\",\n    x = \"Time Scraped\",\n    y = \"Mean Price\",\n    color = \"Section\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "posts/ticketdata/ticketanalysis.html#row-number-analysis",
    "href": "posts/ticketdata/ticketanalysis.html#row-number-analysis",
    "title": "College Football Playoffs Ticket Price Analysis",
    "section": "Row Number Analysis",
    "text": "Row Number Analysis\n\nCodeSugar_summary_row &lt;- Tickets_clean %&gt;%\n  group_by(row_number) %&gt;%\n  summarise(mean_price = mean(Price, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(row_number = as.numeric(trimws(as.character(row_number)))) %&gt;%\n  arrange(row_number) %&gt;%\n  mutate(row_number = factor(row_number, levels = sort(unique(row_number)))) %&gt;% \n  filter(!is.na(row_number))\n\n\nggplot(Sugar_summary_row, aes(x = as.numeric(row_number), y = mean_price)) +\n  geom_col(fill = \"red\") +\n  labs(title = \"Ticket Prices by Row for All Bowls\", x = \"Row Number\", y = \"Mean Price\") +\n  scale_x_continuous(breaks = seq(min(as.numeric(Sugar_summary_row$row_number)), \n                                  max(as.numeric(Sugar_summary_row$row_number)), \n                                  by = 5)) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html",
    "href": "posts/NCAA Basketball/basketball.html",
    "title": "NCAA Basketball Analysis",
    "section": "",
    "text": "Performed web scraping using Selenium and BeautifulSoup, followed by an in-depth analysis in that included Principal Component Analysis, XGBoost, and neural networks.\nI unfortunately lost my data and do not want to rescrape it since it is now a year later but I will leave the project here!"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#abstract",
    "href": "posts/NCAA Basketball/basketball.html#abstract",
    "title": "NCAA Basketball Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project presents an analysis of college basketball team performance based on data from men‚Äôs NCAA Basketball. After merging the datasets from haslametrics and teamrankings along with data cleaning and feature engineering in R, the dataset consisted of 16 columns with 361 rows.\nExploratory data analysis includes correlation analysis, visualization of distributions, and principal component analysis (PCA) to address collinearity among variables. Although PCA had good insights, it was not utilized due to its limited account for variance.\nThe feature engineered variable, ‚ÄòRank_Category‚Äô, classifies teams into three categories based on their ‚ÄòRank‚Äô column: Rank (0-25), Top 50% (excluding Rank), and Bottom 50%. Modeling efforts focused on predicting ‚ÄòRank_Category‚Äô using XGBoost with racing ANOVA tuning which resulted in an accuracy of 79.12% and an AUC of 0.918. Variable importance analysis showed key predictors including defensive efficiency, win rate, defensive field goal percentage, offensive 2-point percentage, and offensive turnovers. Additionally, a neural network model achieved a higher accuracy of 97.80%."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "href": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "title": "NCAA Basketball Analysis",
    "section": "Webscraping Data in Python",
    "text": "Webscraping Data in Python\nScraping the first website Halsametrics.com with selenium\n\nCodefrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\n\n\n# Set up the WebDriver with ChromeOptions\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('/usr/local/bin/chromedriver')  # Add the path to chromedriver executable\n\n# Initialize the WebDriver\ndriver = webdriver.Chrome(options=chrome_options)\n\n# Navigate to the webpage\ndriver.get('https://haslametrics.com/')\n\n# Wait for the page to load and for the 'Defense' button to be clickable\nwait = WebDriverWait(driver, 20)\ndefense_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cboRatings\"]/option[@value=\"Defense\"]')))\n\n# Click the 'Defense' button to load the defensive ratings\ndefense_button.click()\n\n# Wait for the table to load\nwait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"myTable\"]')))\n\n# Scrape the table\ntable = driver.find_element(By.XPATH, '//*[@id=\"myTable\"]')\nhasla = pd.read_html(table.get_attribute('outerHTML'))[0]\n\n# Flatten the MultiIndex columns\nhasla.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in hasla.columns.values]\n\n# Rename 'Unnamed: 1_level_0 Team' to 'Team'\nhasla.rename(columns={'Unnamed: 1_level_0 Team': 'Team'}, inplace=True)\n\n# Extracting win/loss information and creating new columns\nhasla['Win'] = hasla['Team'].str.extract(r'\\((\\d+)-\\d+\\)')\nhasla['Loss'] = hasla['Team'].str.extract(r'\\(\\d+-(\\d+)\\)')\n\n# Remove parentheses and numbers from 'Team' column\nhasla['Team'] = hasla['Team'].replace(regex={'\\([^)]*\\)': '', '\\d+': ''})\n\nhasla['Team'] = hasla['Team'].str.strip()\n\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nhasla.to_excel(f'{desktop_path}/findhasla.xlsx', index=False)\n\n# Close the browser\ndriver.quit()\n\n\nScraping the second website teamrankings.com with BeautifulSoup\n\nCodedef scrape_and_merge(urls, new_column_names):\n    dfs = []\n\n    for url in urls:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        table = soup.find('table')\n        df = pd.read_html(StringIO(str(table)))[0]\n\n        if url in new_column_names:\n            df.columns = new_column_names[url]\n\n        for col in df.columns:\n            if col not in ['Rank', 'Team']:\n                df[col] = pd.to_numeric(df[col].replace('%', '', regex=True), errors='coerce')\n\n        dfs.append(df)\n\n    # Merge all DataFrames dynamically\n    combined_df = dfs[0]\n    for i, df in enumerate(dfs[1:], start=2):\n        combined_df = pd.merge(combined_df, df, on='Team', how='outer', suffixes=('', f'_{i}'))\n\n    # Drop duplicate 'Team' columns\n    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n\n    return combined_df\n\n# Define the URLs\nurls = [\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate',\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game',\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession'\n]\n\n# Create a dictionary with new column names for certain URLs\nnew_column_names = {\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate': ['Rank', 'Team', 'FTR_2023', 'FTR_L3', 'FTR_L1', 'FTR_Home', 'FTR_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct': ['Rank', 'Team', 'ORB_2023', 'ORB_L3', 'ORB_L1', 'ORB_Home', 'ORB_Away', 'ORB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct': ['Rank', 'Team', 'DRB_2023', 'DRP_L3', 'DRB_L1', 'DRB_Home', 'DRB_Away', 'DRB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct': ['Rank', 'Team', 'opp3_2023', 'opp3_L3', 'opp3_L1', 'opp3_Home', 'opp3_Away', 'opp3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct': ['Rank', 'Team', 'p3_2023', 'p3_L3', 'p3_L1', 'p3_Home', 'p3_Away', 'p3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct': ['Rank', 'Team', 'o2p_2023', 'o2p_L3', 'op2_L1', 'op2_Home', 'op2_Away', 'op2 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct': ['Rank', 'Team', '2p_2023', '2p_L3', '2p_L1', '2p_Home', '2p_Away', '2p 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game': ['Rank', 'Team', 'Pace_2023', 'Pace_L3', 'Pace_L1', 'Pace_Home', 'Pace_Away', 'Pace 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession': ['Rank', 'Team', 'TO_2023', 'TO_L3', 'TO_L1', 'TO_Home', 'TO_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession': ['Rank', 'Team', 'oppTO_2023', 'oppTO_L3', 'oppTO_L1', 'oppTO_Home', 'oppTO_Away', 'oppTO 2022']\n}\n\n\nCombining the data frames and saving to desktop\n\nCode# Call the function to scrape and merge data\ncombined_df = scrape_and_merge(urls, new_column_names)\n\ncombined_df['Team'] = combined_df['Team'].replace({\n'Miami (OH)' : 'Miami'\n\n})\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\ncombined_df.to_excel(f'{desktop_path}/findme.xlsx', index=False)\n\n\n\nneutral_input = input(\"Is it a neutral site game (Yes/No): \")\n\n\n# Drop duplicate team names in hasla\nhasla = hasla.drop_duplicates(subset=['Team'])\n\n# Drop duplicate team names in combined_df\ncombined_df = combined_df.drop_duplicates(subset=['Team'])\n\n# Merge the DataFrames based on 'Team'\nmerged_df = pd.merge(hasla, combined_df, on='Team', how='inner')\n\n# Save the merged DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nmerged_df.to_excel(f'{desktop_path}/merged_data.xlsx', index=False)"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "href": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "title": "NCAA Basketball Analysis",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nAll further code performed in R:\nOnce the data has been combined into one data frame, there are over 100 variables. Using dplyr, 16 columns are selected, renamed for easier readability, mutated to correct variable type (numeric, factor, etc), and a new variable is feauture engineered to split the ranks into three categories of Ranked, Top 50%, and Bottom 50%.\n\nCodelibrary(readxl)\nlibrary(tidyverse)\nmerged_data &lt;- read_excel(\"~/Desktop/merged_data.xlsx\")\n\nclean_data &lt;- merged_data %&gt;%\n  select(`Unnamed: 0_level_0 Rk`, `Win`, `Loss`, `DEFENSIVE SUMMARY Eff`, `DEFENSIVE SUMMARY 3P%`, `DEFENSIVE SUMMARY FG%`, `DEFENSIVE SUMMARY MR%`, `DEFENSIVE SUMMARY NP%`, FTR_2023, TO_2023, ORB_2023, DRB_2023, p3_2023, `2p_2023`, Pace_2023, TO_2023) %&gt;%\n  rename(\n    Rank = `Unnamed: 0_level_0 Rk`,\n    `Def_Eff` = `DEFENSIVE SUMMARY Eff`,\n    `Def_3P` = `DEFENSIVE SUMMARY 3P%`,\n    `Def_FG` = `DEFENSIVE SUMMARY FG%`,\n    `Def_MR` = `DEFENSIVE SUMMARY MR%`,\n    `Def_NP` = `DEFENSIVE SUMMARY NP%`,\n    Off_FTR = FTR_2023,\n    Off_TO = TO_2023,\n    Off_ORB = ORB_2023,\n    Def_DRB = DRB_2023,\n    Off_3P = p3_2023,\n    Off_2P = `2p_2023`,\n    Pace = Pace_2023\n  ) %&gt;% \n  mutate(Win = as.numeric(Win),\n         Loss = as.numeric(Loss)) \n\nclean_data$Rank_Category &lt;- ifelse(clean_data$Rank &gt;= 0 & clean_data$Rank &lt;= 25, \"Ranked\",\n                                   ifelse(clean_data$Rank &gt; 25 & clean_data$Rank &lt;= 181, \"Top 50%\", \"Bottom 50%\"))\nclean_data &lt;- clean_data %&gt;%\n  mutate(Rank_Category = as.factor(Rank_Category))\n\n\nThe final data cleaning step is checking total NA values for each variable which there are none\n\nCodecbind(lapply(lapply(clean_data, is.na), sum))"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "href": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "title": "NCAA Basketball Analysis",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCorrelation and Scatter Plots\nThe data exploration begins by looking at the correlation between variables. I created a function to make a correlation plot then if the correlation is above the absolute value of 0.6, it will plot the scatter plot of the two correlated variables. Due to the high correlation in this data, I have limited the output to only two of the scatterplots.\n\nCodelibrary(corrplot)\n\ncompute_and_plot_correlation &lt;- function(data, threshold = 0.6) {\n  # Select numeric columns\n  numeric_data &lt;- data[, sapply(data, is.numeric)]\n  \n  # Remove rows with missing values\n  numeric_data &lt;- numeric_data[complete.cases(numeric_data), ]\n  \n  # Compute correlation matrix\n  correlation_matrix &lt;- cor(numeric_data)\n  \n  # Find pairs of variables with correlation above or below the threshold\n  high_correlation_pairs &lt;- which(abs(correlation_matrix) &gt; threshold & upper.tri(correlation_matrix), arr.ind = TRUE)\n  \n  # Create scatter plots for high correlation pairs\n  plots &lt;- list()\n  for (i in 1:nrow(high_correlation_pairs)) {\n    var_x &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 1]]\n    var_y &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 2]]\n    \n    plot &lt;- ggplot(data = numeric_data, aes_string(x = var_x, y = var_y)) +\n      geom_point() +\n      labs(title = paste(\"Scatter Plot of\", var_y, \"vs\", var_x), x = var_x, y = var_y) + \n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    plots[[paste(var_x, var_y, sep = \"_\")]] &lt;- plot\n  }\n  \n  # Plot correlation matrix\n  corrplot(correlation_matrix, method = \"shade\", type = \"lower\", diag = FALSE, addCoef.col = \"black\", number.cex = 0.5)\n  \n  return(plots)\n}\n\n#Example call to function\nscatter_plots &lt;- compute_and_plot_correlation(clean_data)\n\nfor (i in seq_along(scatter_plots)) {\n  if (i &gt; 2) break\n  print(scatter_plots[[i]])\n}\n\n\nDistributions - Histograms\nI also made a function to make histograms for all numeric variables to view the distributions. Because all of our variables are numeric, no bar charts were made to view the distribution of categorical variables.\n\nCodecreate_histograms_ggplot &lt;- function(data) {\n  # Get numeric variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n  \n  # Initialize an empty list to store ggplot objects\n  plots &lt;- list()\n  \n  # Loop through each numeric variable and create a histogram using ggplot\n  for (var in numeric_vars) {\n    # Create ggplot object for histogram\n    plot &lt;- ggplot(data, aes_string(x = var)) +\n      geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n      labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    # Append ggplot object to the list\n    plots[[var]] &lt;- plot\n  }\n  \n  return(plots)\n}\n\n# Example call to function\nhist_plots &lt;- create_histograms_ggplot(clean_data)\n\n\n  print(hist_plots[[2]])\n  print(hist_plots[[3]])"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "href": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "title": "NCAA Basketball Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nIn our data exploration, the correlation plot showed the high correlation between our variables. Because of this, a principal component analysis was performed to reduce collinearity. A scree plot was used to determine the number of components. Figure 5 shows the scree plot.\n\nCodelibrary(htmlwidgets)\nlibrary(plotly)\n\nX &lt;- subset(clean_data, select = -c(Rank_Category, Win, Loss))\n\nprin_comp &lt;- prcomp(X, center = TRUE, scale. = TRUE)\n\n\nScree Plot\nThe scree plot shows there should be around 3 components to account for the most variance while also reducing the dimensions.\n\nCodeplot(prin_comp, type = \"l\", main = \"Scree Plot\")\n\n\n3D PCA\nA 3d plot with the three axes of the plot representing the first three principal components (PC1, PC2, and PC3). It also clusters the variable Ranked_Category very accurately. Although it clusters Ranked_Category well, it only accounts for 61.67% of the variance so we will not use it.\n\nCodesumm &lt;- summary(prin_comp)\nsumm$importance[2,]\n\ncomponents &lt;- prin_comp[[\"x\"]]\ncomponents &lt;- data.frame(components)\ncomponents$PC2 &lt;- -components$PC2\ncomponents$PC3 &lt;- -components$PC3\ncomponents = cbind(components, clean_data$Rank_Category)\n\n# Combine components with Ranked labels\ncomponents &lt;- cbind(components, Rank_Category = clean_data$Rank_Category)\n\n# Create Plotly figure\nfig &lt;- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Rank_Category,\n               colors = c('#636EFA','#EF553B','#00CC96'), type = \"scatter3d\", mode = \"markers\",\n               marker = list(size = 4))\n\n\n# Customize layout\nfig &lt;- fig %&gt;% layout(\n  title = \"61.67% Variance Explained\",\n  scene = list(bgcolor = \"#e5ecf6\")\n)\n\n# Show the plot\nfig\nsaveWidget(fig, \"interactive_plot.html\")"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "href": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "title": "NCAA Basketball Analysis",
    "section": "XGBoost Classification Model",
    "text": "XGBoost Classification Model\nDue to the high colinearity between our variables, our model must be able to take it into account. This model will be for exploration use rather than predictive so that we can see what variables are important to be ranked higher at the end of the season.\n\nCode#libs\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(pROC)\nlibrary(data.table)\nlibrary(kableExtra)\n\n\nSplitting into Training/Testing\n\nCodeDATA &lt;- clean_data %&gt;% \n  select(-Rank)\n\nset.seed(123)\nDATA_SPLIT &lt;- DATA %&gt;%\n  initial_split(strata = Rank_Category)\n\nDATA_TRAIN &lt;- training(DATA_SPLIT)\nDATA_TEST &lt;- testing(DATA_SPLIT)\n\nset.seed(234)\nDATA_folds &lt;- vfold_cv(DATA_TRAIN, strata = Rank_Category)\nDATA_folds\n\n\nRecipe\n\nCodeDATA_rec &lt;-\n  recipe(Rank_Category ~ ., data = DATA_TRAIN) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nprep(DATA_rec) # checking prep\n\n\nTuning Model\n\nCodexgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n#workflow\nxgb_workfl &lt;- workflow(DATA_rec, xgb_spec)\n\n\nRace Anova\nOur model will be an XGBoost that utilizes a racing ANOVA. Tidymodels will also be used so that we can tune trees, min_n, mtry, tree_depth, learn_rate, and loss_reduction.\n\nCodelibrary(finetune)\ndoParallel::registerDoParallel()\n\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_workfl,\n  resamples = DATA_folds,\n  grid = 20,\n  metrics = metric_set(accuracy),\n  control = control_race(verbose_elim = TRUE)\n)\n\n\nComparing Models\nThe plot below shows the racing ANOVA as it picks out the best model\n\nCodeanova &lt;- plot_race(xgb_rs)\n\nanova +\n  labs(title = \"Model Race ANOVA\",\n       y = \"Model Accuracy\") +\n  theme_minimal() +\n  theme(plot.title = (element_text(hjust = 0.5)))\n\n\nBest Model\nThe following code is used to extract the best model\n\nCodeshow_best(xgb_rs)\n\n\nMetrics\n\nCodexgb_last &lt;- xgb_workfl %&gt;%\n  finalize_workflow(select_best(xgb_rs, metric = \"accuracy\")) %&gt;%\n  last_fit(DATA_SPLIT)\n\nxgb_last$.metrics\n\n\nConfusion Matrix\nThe final model had an accuracy of 83.52 % for predicting ‚ÄòRanked_Category‚Äô and an AUC of 0.914. Although this is not the highest accuracy, the more important part is the importance of each variable for the model are Def_Eff, Win, Def_FG, Off_2p, and Off_TO as seen in the plot below.\n\nCodeDATA_pred &lt;- collect_predictions(xgb_last)$.pred_class\n\nDATA_act &lt;- DATA_TEST$Rank_Category\n\nconfusionMatrix(DATA_pred, DATA_act)\n\n\nVIP\n\nCodelibrary(vip)\nvip &lt;- extract_workflow(xgb_last) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\", num_features = 10, mapping = aes(fill = Variable))\nvip"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#neural-network",
    "href": "posts/NCAA Basketball/basketball.html#neural-network",
    "title": "NCAA Basketball Analysis",
    "section": "Neural Network",
    "text": "Neural Network\nNow that we know the important variables for ‚ÄòRanked Category‚Äô, a neural network was performed to see how well we could predict it. The neural network was made in R using the command neuralnet(). The best neural network is seen in Figure 9 which had a 97.80% accuracy.\n\nCodelibrary(neuralnet)\nlibrary(caret)\nlibrary(tidymodels)\n\nnndata &lt;- clean_data \nset.seed(123)\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(nndata, prop = 3/4, strata = Rank_Category)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nNN &lt;- neuralnet(Rank_Category ~ ., train_data, hidden = c(5,3), linear.output = TRUE)\nplot(NN, rep = \"best\")\n\n\nConfusion Matrix\n\nCodepredicted_classes &lt;- predict(NN, test_data)\n# Extract predicted class labels\npredicted_classes &lt;- max.col(predicted_classes)\n\n# Convert the indices to class labels\npredicted_classes &lt;- levels(test_data$Rank_Category)[predicted_classes]\nactual_classes &lt;- test_data$Rank_Category\n\npredicted_classes &lt;- factor(predicted_classes, levels = levels(actual_classes))\n\n# length(predicted_classes)\n# print(predicted_classes)\n\n# Extract actual class labels from the test data\n# length(actual_classes)\n# print(actual_classes)\n\n# Create a confusion matrix\nconfusionMatrix(predicted_classes, test_data$Rank_Category)"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html",
    "href": "posts/InquisitR/InquisitR.html",
    "title": "InquisitR",
    "section": "",
    "text": "Designed to simplify the initial phases of data exploration while enhancing the visual appeal of graphs. Currently, it offers three essential functions listed below. I am also actively developing a fourth to streamline data type conversions."
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#boxplotr",
    "href": "posts/InquisitR/InquisitR.html#boxplotr",
    "title": "InquisitR",
    "section": "boxplotR",
    "text": "boxplotR\nThe function generates detailed boxplots for various combinations of factor and numeric variables within your dataset\n\nCodelibrary(ggplot2)\n\nboxplotR &lt;- function(data) {\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric and one factor variable.\")\n  }\n\n  factor_vars &lt;- names(data)[sapply(data, is.factor)]  # Get factor variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]  # Get numeric variable names\n\n  plots &lt;- list()\n\n  # Loop through each combination of factor and numeric variable\n  for (x in factor_vars) {\n    for (y in numeric_vars) {\n      plot_title &lt;- paste(\"Boxplot of\", y, \"by\", x)\n      plot &lt;- ggplot(data, aes_string(x = x, y = y)) +\n        geom_boxplot(fill = \"skyblue\", color = \"black\") +\n        labs(title = plot_title) +\n        theme_minimal()\n      plots[[paste(x, y, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\nboxplotR(iris)\n\n$Species_Sepal.Length\n\n\n\n\n\n\n\n\n\n$Species_Sepal.Width\n\n\n\n\n\n\n\n\n\n$Species_Petal.Length\n\n\n\n\n\n\n\n\n\n$Species_Petal.Width"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#correlationr",
    "href": "posts/InquisitR/InquisitR.html#correlationr",
    "title": "InquisitR",
    "section": "correlationR",
    "text": "correlationR\nThe function creates customizable pairwise plot matrices using GGally, allowing you to tailor plots for upper triangles, lower triangles, and diagonal views.\n\nCodelibrary(GGally)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nupperFn &lt;- function(data, mapping, ...) {\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  cor_value &lt;- round(cor(x, y), 2)\n  cor_label &lt;- format(cor_value, nsmall = 2)\n\n  data.frame(x = 1, y = 1, cor = cor_value) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_tile(aes(fill = cor), color = \"white\", width = 1, height = 1) +\n    geom_text(aes(label = cor_label), color = \"black\", size = 5, vjust = 0.5) +\n    scale_fill_gradient2(low = \"blue2\", high = \"red2\", mid = \"white\", midpoint = 0, limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n    theme_void() +\n    theme(legend.position = \"none\",\n          plot.margin = unit(c(0, 0, 0, 0), \"cm\")) +\n    theme_minimal()\n}\n\nlowerFn &lt;- function(data, mapping, method = \"lm\", ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(colour = \"skyblue3\") +\n    geom_smooth(method = method, color = \"black\", ...) +\n    theme_minimal()\n}\n\ndiagFn &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_density(aes(y = ..density..), colour = \"red\", fill = \"red\", alpha = 0.2) +\n    geom_histogram(aes(y = ..density..), colour = \"blue\", fill = \"skyblue3\", alpha = 0.5) +\n    theme_minimal()\n}\n\ncorrelationR &lt;- function(df) {\n  numeric_cols &lt;- df %&gt;%\n    dplyr::select(where(is.numeric)) %&gt;%\n    colnames()\n\n  if (length(numeric_cols) &lt; 2) {\n    stop(\"The dataframe must contain at least two numeric columns.\")\n  }\n\n  ggpairs(\n    df[, numeric_cols],\n    lower = list(continuous = wrap(lowerFn, method = \"lm\")),\n    diag = list(continuous = wrap(diagFn)),\n    upper = list(continuous = wrap(upperFn))\n  ) + theme_minimal()\n}\n\ndata(iris)\ncorrelationR(iris)"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#distributionr",
    "href": "posts/InquisitR/InquisitR.html#distributionr",
    "title": "InquisitR",
    "section": "distributionR",
    "text": "distributionR\nThis function easily visualize distributions of both numeric and categorical variables, with flexible options to suit your analysis needs.\n\nCodeutils::globalVariables(c(\"Category\", \"Frequency\"))\n\n[1] \"Category\"  \"Frequency\"\n\nCodelibrary(ggplot2)\n\ndistributionR &lt;- function(data, plot_bars = TRUE, plot_histograms = TRUE) {\n  # Check if input is a data frame\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.numeric(x) || is.character(x) || is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric or categorical (character/factor) variable.\")\n  }\n\n  plots &lt;- list()\n\n  # Create bar plots for categorical variables\n  if (plot_bars) {\n    char_factor_cols &lt;- names(data)[sapply(data, function(x) is.character(x) || is.factor(x))]\n    for (col in char_factor_cols) {\n      freq_table &lt;- table(data[[col]])\n      plot_data &lt;- as.data.frame(freq_table)\n      names(plot_data) &lt;- c(\"Category\", \"Frequency\")\n      plot &lt;- ggplot(plot_data, aes(x = Category, y = Frequency)) +\n        geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Bar Plot of\", col), x = col, y = \"Frequency\") +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n        theme_minimal()\n      plots[[paste(\"barplot\", col, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  # Create histograms for numeric variables\n  if (plot_histograms) {\n    numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n    for (var in numeric_vars) {\n      plot &lt;- ggplot(data, aes(x = .data[[var]])) +\n        geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n        theme_minimal()\n      plots[[paste(\"histogram\", var, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\ndistributionR(iris, plot_bars = TRUE, plot_histograms = TRUE)\n\n$barplot_Species\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Width\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Width\n\n\n\n\n\n\n\n\nGithub link to use my package https://lnkd.in/gR2h7gKB\nor just use: library(devtools) install_github(‚ÄúNathanBresette/InquisitR‚Äù)"
  },
  {
    "objectID": "posts/Playfair/Playfair.html",
    "href": "posts/Playfair/Playfair.html",
    "title": "Playfair‚Äôs Recreation & Improvement",
    "section": "",
    "text": "Critiquing, recreating, and improving playfair‚Äôs famous graph\n\nCodeknitr::include_graphics(\"pf.png\")\n\n\n\n\n\n\n\nPart 1: Critique\nPlayfair‚Äôs chart works well in several ways. First, it uses bars as a simple and effective visual representation to compare taxation and revenue across nations. The use of height to represent quantitative values allows viewers to easily see differences between nations. Additionally, the connecting line between the two bars allow for an easier comparison when looking at one specific county. The categorical arrangement of the nations by population size also helps in maintaining clarity and comparison. The use of circle size to represent population size is simple yet effective. The chart combines several forms of data in one visualization. Playfair‚Äôs introduction an effective method of showing patterns and trends visually. Despite its innovation, the chart has limitations that detract from its effectiveness. One of the main issues is the overloading of visual elements‚Äîhaving multiple variables in a single visual without clear legends. Without reading more about the graph, it is impossible to tell what is taxation and revenue. The pie charts inside the countries circles are too small to see. The Venn diagram is also extremely small and I had to zoom in to see it. All things considered, Playfair‚Äôs chart effectively communicates its purpose‚Äîto compare the economic and geographical standings of European nations. While there are some things that are not ideal in its ability to precisely communicate all the data elements, the chart‚Äôs innovative use of bars for comparison was groundbreaking for its time. It set the foundation for modern data visualization, even though it lacks some refinements that we would expect today especially in DATA324! # Part 2 - Recreation\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\nlibrary(stringr) # str_wrap\neurope &lt;- read.csv(\"~/Downloads/playfair_european_nations.csv\")\n# Calculate radius\nRadius &lt;- (sqrt(europe$Area / 3.1459)) / 140  # Scale\n# Dynamic center calculation\nnum_countries &lt;- nrow(europe)\nspacing &lt;- 3  \ncenter &lt;- numeric(num_countries)  # Initialize\n# Position of the first circle\ncenter[1] &lt;- 10 \n# Calculate the center positions based on the radius of the circles\nfor (i in 2:num_countries) {\n  center[i] &lt;- center[i - 1] + Radius[i - 1] + Radius[i] + spacing\n}\n# Axisbreak based on the center\naxisbreak &lt;- unique(center)\n# Wrapping x-axis labels\nwrapped_labels &lt;- str_wrap(europe$Country, width = 15)\nPlayfair &lt;- ggplot(europe) +\n  geom_circle(aes(x0 = center, y0 = 0, r = Radius)) +\n  \n  # Yellow line for Taxation\n  geom_segment(aes(x = center + Radius, xend = center + Radius, y = 0, yend = Taxation),\n               size = 1, color = \"yellow\") +\n  \n  # Red line for Population\n  geom_segment(aes(x = center - Radius, xend = center - Radius, y = 0, yend = Population),\n               size = 1, color = \"red\") +\n  \n  # Dashed line to connect red and yellow\n  geom_segment(aes(x = center - Radius, xend = center + Radius, \n                   y = Population, yend = Taxation), linetype = \"longdash\") +\n  \n  # Solid black lines at y = 0, 10, and 20\n  geom_hline(yintercept = c(0, 10, 20), color = \"black\", size = 0.5) +\n  \n  # Apply wrapped labels and adjust x-axis text\n  scale_x_continuous(breaks = axisbreak, labels = wrapped_labels) +\n  \n  # Set y-axis to display 0 to 30 with every integer as a tick mark\n  scale_y_continuous(\n    limits = c(-10, 30), \n    breaks = seq(0, 30, by = 1),\n    sec.axis = sec_axis(trans = ~ ., \n                        breaks = seq(0, 30, by = 1))  # Add name for secondary axis\n  ) +\n  \n  # Custom old paper-like background\n  theme(\n    # Old paper color\n    panel.background = element_rect(fill = \"#d3b58f\"),  \n    plot.background = element_rect(fill = \"#d3b58f\"),   \n    panel.grid.major = element_line(color = \"darkgrey\", size = 0.5), \n    panel.grid.minor = element_blank(), \n    \n    # Axis stuff\n    axis.text.x = element_text(angle = 90, face = \"bold\", hjust = 1),\n    axis.text.y = element_text(size = 5),\n    axis.text.y.right = element_text(size = 5),\n    plot.title = element_text(hjust = 0.5, size = 7, face = \"bold\"),\n    panel.border = element_rect(color = \"black\", fill = NA) # black border\n  ) +\n  labs(title = \"Chart Representing the Extent, Population & Revenues, of the Principal Nations in Europe, after the division of Poland & Treaty of Luneville\", x = \"\", y =\"\")\nPlayfair\n\n\n\n\n\n\n\nPart 3: New Data Visualization\n\nCodeRecreation &lt;- ggplot(europe, aes(x = Population, y = Taxation)) + \n  geom_point(aes(col = Country, size = Area)) + \n  ylim(c(0, 30)) + \n  labs(title = \"New Playfair Data Visualization\",\n       y = \"Taxation (Millions)\", \n       x = \"Population (Millions)\",\n       size = \"Land Area\",    \n       color = \"Country\") +         \n  scale_size_continuous(breaks = c(62000, 200000, 500000, 1000000, 2000000, 4720000),  \n                        labels = c(\"62k\", \"200k\", \"500k\", \"1M\", \"2M\", \"4.7M\"),\n                        range = c(2, 12)) +                # Better visibility\n  geom_text(aes(label = Country),  # Data labels\n            vjust = -0.5,           # Adjust vertical position\n            size = 3,               # Size of the text\n            check_overlap = TRUE) +   \n  guides(color = \"none\") +           # Remove legend for Country\n  theme(legend.key = element_blank()) + # Remove legend background\n  theme_minimal()\nRecreation\n\n\n\n\n\n\n\nPart 4: Concluding Explanation\nIn creating this data visualization, I chose to map Population to the x-axis and Taxation to the y-axis. This allows viewers to easily observe the relationship between a country‚Äôs population size and its tax revenue, which is critical for understanding how population might influence taxation. Each point represents a different country that allows for a direct visual comparison across multiple nations. The choice to use size for the Area of each country is also significant. Larger countries are represented by larger points which enables viewers to quickly assess the relative land size of each nation with its population and taxation. The use of color to differentiate countries adds another layer of information, helping viewers quickly identify which point corresponds to which country. However, I opted to remove the color legend for Country to streamline the visualization and avoid clutter. Instead, I added data labels directly to the points, ensuring that viewers can easily discern which country each point represents. This decision supports clarity and allows for immediate identification without needing to look at a legend. In terms of design principles, I used a minimalist theme (using theme_minimal()), which eliminates unnecessary gridlines and distractions and helps the viewer‚Äôs attention on the data The vertical adjustment of text labels helps to ensure that country names do not overlap with the points as much. I also carefully selected the size breaks for the Area scale and used significant thresholds that correspond to the actual data values to avoid misleading interpretations. The breaks provide a clear representation of how land area varies among the countries and allow for a more intuitive understanding of size differences. The choice of colors could be more intentional by using a color palette that is colorblind friendly which could enhance accessibility. There are a few data labels that overlap and does not show in the graph. I would also add a footnote of where I got the data. My graph effectively illustrates that larger populations do not always correlate with higher taxation levels. This visualization aligns with Playfair‚Äôs assertion that visual data can reveal complex relationships that might not be immediately apparent through numerical data alone. Overall, the design choices made in this visualization work to highlight these relationships"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "During my time at Truman, I‚Äôve engaged in three rewarding consulting experiences through STAT 310 - Data Collection & Statistical Communication and STAT 392 - Center for Applied Statistics and Evaluation. These consulting endeavors involved extensive client interaction, survey design, data analysis, and the delivery of comprehensive presentations."
  },
  {
    "objectID": "consulting.html#international-admissions-office",
    "href": "consulting.html#international-admissions-office",
    "title": "Consulting",
    "section": "International Admissions Office",
    "text": "International Admissions Office\nSTAT 310 afforded me the opportunity to collaborate with two peers on a semester-long consulting project for the International Admissions Office. Our primary objective was to assist our client in understanding the factors driving international students to choose Truman. This involved delving into various aspects such as the admissions process, initial awareness of the university, decision-making factors, and current perceptions. Our findings were compiled into a 22-page report supplemented with insightful data visualizations, which we presented to our client in a concise and impactful 15-minute presentation."
  },
  {
    "objectID": "consulting.html#compliance-office",
    "href": "consulting.html#compliance-office",
    "title": "Consulting",
    "section": "Compliance Office",
    "text": "Compliance Office\nSTAT 392 is a semester long class where we managed multiple client projects simultaneously, ensured clear communication and meet all deadlines. The first client was for Truman State University‚Äôs Institutional Compliance Office. The objective of the survey was to evaluate the office‚Äôs effectiveness in meeting the needs of the university community and to investigate areas that could benefit from further development. This was done through creating a survey, analyzing the results, and compiling a report for the office. I joined this project after the survey had been created so my main task was analyzing the survey and communicate results. After, a paper was compiled which focused on the respondents based on whether they have or have not interacted with the office."
  },
  {
    "objectID": "consulting.html#voice-diagnostics-survey",
    "href": "consulting.html#voice-diagnostics-survey",
    "title": "Consulting",
    "section": "Voice Diagnostics Survey",
    "text": "Voice Diagnostics Survey\nAnother project in STAT 392 was creating a nationwide survey about the protocols used for instrumental assessment of voice. I had the lead in this project and my clients were a professor and a graduate student. Unlike the past surveys, I had no background in protocols used for instrumental assessment of voice. This was a new challenge for me to become familiar enough in the field to help my clients get the best results. Although this project is still on going, with the help of my clients, we created a nationwide survey after several meetings of tweaking the survey, adding sections, and getting the best ordering of questions. This survey will be sent out soon and will be analzed after"
  },
  {
    "objectID": "posts/Polling/Polling.html",
    "href": "posts/Polling/Polling.html",
    "title": "Analysis of Nate Silver‚Äôs Polling Outcomes:",
    "section": "",
    "text": "Analysis of Nate Silver‚Äôs Polling Outcomes: Observed vs.¬†Expected Results\nGoal:\nExamine, analyze, and enhance Nate Silver‚Äôs paper on presidential polling and why the polls are wrong\nOverview:\nNate Silver‚Äôs recent article (https://www.natesilver.net/p/theres-more-herding-in-swing-state) about polling inaccuracies caught my attention, especially his insights on how polling firms adjust weights in their models to create narratives of close races. He pointed out that there‚Äôs a staggering 1 in 9.5 trillion chance of so many polls showing such tight margins. Inspired by this, I decided to recreate his results using the same data and present some visually appealing graphs.\nI used a binomial cumulative distribution function to calculate the probability of observing 193 or more close polls, assuming a null hypothesis of 0.55. The result? A jaw-dropping 1 in 8 trillion chance of seeing such close outcomes by random chance alone. This suggests there‚Äôs almost zero probability that these polls reflect the true state of the race, indicating that some results may be manipulated to appear closer than they are. This discrepancy raises serious questions about the reliability of polling outcomes.\nTo illustrate how misleading these polls can be, I created three graphs: Graph 1: A histogram showing simulated outcomes, with a red dashed line marking the observed success of 193 close polls. This emphasizes just how rare these results are. Graph 2: A bar chart comparing expected vs.¬†observed close polls, highlighting the significant deviation from historical norms. Graph 3: A cumulative distribution function (CDF) plot that gives a broader perspective on the likelihood of various polling outcomes, further reinforcing the unusual nature of what we‚Äôre seeing.\nThese findings underline the importance of critically examining polling methods and understanding how weight adjustments can skew results. As we head into this election season, let‚Äôs stay aware of how these factors shape our perceptions and take all predictions with a grain of salt. Most importantly, go vote!\nIntroduction\nIn examining the polling data, we start with the foundational parameters that govern our analysis from Nate Silver‚Äôs paper: https://www.natesilver.net/p/theres-more-herding-in-swing-state. We have a total of 249 polls in the database, with an expected probability of a poll showing a close result under the null hypothesis set at 0.55. This probability is based on Nate Silver‚Äôs analysis, which indicates that, in a tied race, we would expect 55% of the polls to show results within ¬±2.5 points. This expectation arises from the margin of error formula and varies significantly depending on sample size but for a more in depth reasoning, check out his paper. In the data, the actual number of polls that showed a close result is 193 so 193/249 polls or 78% showed a close race.\nBinomial Distribution\nUsing these parameters, we calculate the probability of observing 193 or more close polls under the null hypothesis. This is accomplished through the binomial cumulative distribution function (CDF):\n\nCode# Number of polls in the database\nn &lt;- 249\n\n# Expected probability of a poll showing a close result under the null hypothesis\np &lt;- 0.55\n\n# Number of polls that actually showed a close result\nobserved_successes &lt;- 193\n\n# Calculate the probability using the binomial cumulative distribution function\nprobability &lt;- pbinom(observed_successes - 1, n, p, lower.tail = FALSE)\n\n# Convert the probability into a \"1 in X\" number\none_in_x &lt;- 1 / probability\n\n# Print the result without scientific notation\nformatted_one_in_x &lt;- format(one_in_x, scientific = FALSE)\npaste(\"One in\", formatted_one_in_x, \"or about 8 trillion which is close to Nate Silver's 9 trillion probability\")\n\n[1] \"One in 8316941692884 or about 8 trillion which is close to Nate Silver's 9 trillion probability\"\n\n\nThe calculation indicates the rarity of observing such a result under the stated null hypothesis. This perspective aligns with Nate Silver‚Äôs approach to polling, where understanding the significance of results is crucial to interpreting their implications in real-world scenarios.\nVisualizing Simulated Outcomes\nTo further illustrate the significance of our findings, we conduct 10,000 simulations to model the distribution of close poll outcomes. By creating a histogram of these simulated outcomes, we can visually assess where the observed success fits within the broader distribution. In Graph 1, the red dashed line signifies the number of observed close polls. This visual representation allows us to contextualize our findings within the realm of simulated expectations and see how rare it would be if the polls are correct.\n\nCodelibrary(ggplot2)\nset.seed(123)\nsimulations &lt;- 10000\nsimulated_outcomes &lt;- rbinom(simulations, n, p)\n\n# Create a histogram of simulated outcomes\nggplot(data.frame(simulated_outcomes), aes(x = simulated_outcomes)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", x = observed_successes - 10, y = max(table(simulated_outcomes)) * 0.9,\n           label = bquote(bold(\"Observed: \") * bold(.(observed_successes))), color = \"red\") +\n  scale_x_continuous(limits = c(min(simulated_outcomes) - 5,200)) +  # Expand x-axis range\n  labs(\n    title = \"Graph 1: Simulated Distribution of Close Poll Outcomes\",\n    x = \"Number of Close Polls\",\n    y = \"Frequency\",\n    subtitle = \"The blue distrubition is the expected number of close polls from the simulation\nand the red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nComparing Expected vs.¬†Observed Outcomes\nNext, we can create a bar chart, Graph 3, to compare the expected number of close polls against the observed number. The expected number is derived from the initial probability (0.55), while the observed number reflects real-world polling results. This comparison highlights the disparity between expected outcomes based on historical data and the actual observed results, providing insight into the polling landscape.\n\nCode# Calculate the expected number of close polls under the null hypothesis\nexpected_successes &lt;- n * p\n\n# Create a data frame for plotting\ncomparison_data &lt;- data.frame(\n  Category = c(\"Expected\", \"Observed\"),\n  Count = c(expected_successes, observed_successes)\n)\n\n# Plot\nggplot(comparison_data, aes(x = Category, y = Count, fill = Category)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  scale_fill_manual(values = c(\"skyblue\", \"red\")) +\n  labs(\n    title = \"Graph 2: Expected vs. Observed Number of Close Polls\",\n    y = \"Number of Close Polls\",\n    x = \"\",\n    caption = \"Expected is based on a probability of 0.55. Observed is the actual count.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nCumulative Distribution Function Analysis\nLastly, we can examine the cumulative distribution function (CDF) of the binomial distribution for further insights. The CDF plot provides a comprehensive view of the probability of observing various numbers of close polls, further emphasizing the significance of our observed results in relation to expectations.\n\nCode# Calculate the CDF for the binomial distribution\nx_vals &lt;- 0:n\ncdf_vals &lt;- pbinom(x_vals, n, p)\n\n# Create a data frame for plotting\ncdf_data &lt;- data.frame(x_vals, cdf_vals)\n\n# Plot the CDF\nggplot(cdf_data, aes(x = x_vals, y = cdf_vals)) +\n  geom_line(color = \"blue\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Graph 3: Cumulative Distribution Function (CDF) of Binomial Distribution\",\n    x = \"Number of Close Polls\",\n    y = \"Cumulative Probability\",\n    caption = \"The red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nConclusion\nThe analysis of polling data reveals a stark contrast between the expected outcomes based on Nate Silver‚Äôs framework and the actual results observed in recent polls. With an expected probability of p = 0.55, representing the likelihood that a poll should show a close result within ¬±2.5 points in a tied race, the finding of 193 close polls out of 249 conducted indicates a significant deviation from this norm. This discrepancy translates into an extraordinarily low probability, quantifying the actual observation as a 1 in 9 trillion scenario. Such results raise critical questions about the accuracy and reliability of the polling methodologies in use.\nA noteworthy aspect of this analysis is the way polling firms often adjust weights in their models to achieve closer race outcomes. By calibrating the weights based on historical voting patterns and demographic data, pollsters can create an artificial tightness in the race. This practice might stem from a desire to generate more competitive narratives or to align with expected electoral dynamics, but it can inadvertently lead to inflated expectations for how close the race actually is."
  },
  {
    "objectID": "posts/dash data jobs/datadash.html",
    "href": "posts/dash data jobs/datadash.html",
    "title": "Data Salary Analysis Dashboard",
    "section": "",
    "text": "RShiny dashboard that explores and visualizes data science salary trends from 2020-2024.\nLink to dashboard: https://nathanbresette.shinyapps.io/data324_project/\nOverview of Project\nThe project utilized a Kaggle dataset containing anonymized salary data from the data field, with variables such as job title, experience level, company size, and geographic location. Our goal was to provide users with an interactive tool to better understand the factors influencing salaries across different roles and regions. Through various visualizations like boxplots, bar charts, and maps, the app allows users to filter the data by other criteria, making it easy to identify patterns and draw meaningful insights. This project was a valuable opportunity to apply our skills in R, data manipulation, and visualization while working collaboratively to ensure the dashboard was functional and user-friendly.\nWe highly recommend visiting the ‚ÄúDocumentation‚Äù tab within the app for a more detailed explanation of the dataset, the features available, and how to navigate the dashboard. The app not only visualizes salary trends but also aims to answer key questions such as how location, experience, and employment type impact earnings. Users can explore whether they are being compensated fairly by leveraging the linear model integrated into the app. This collaborative effort highlights the power of data visualization in making informed career decisions, and we hope it serves as a useful resource for those entering or advancing within the data field\nGroup Code\n\nCode# Libraries\nlibrary(shiny)\nlibrary(DT)\nlibrary(viridis)\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(rsconnect)\n\nworld &lt;- st_read(\"data/world.gpkg\")\n\n# # Load country geometries from rnaturalearth\n#  world &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n# \n#  world &lt;- world %&gt;%\n#    dplyr::select(\"iso_a2\", \"name\", \"continent\")\n# \n#  st_write(world, \"~/Downloads/world.gpkg\")\n \n# Load Salaries dataset and clean it\nSalaries &lt;- read.csv(\"data/datafile.csv\")\n\nSalaries_Clean &lt;- Salaries %&gt;% \n  mutate(remote_ratio = factor(case_when(\n    remote_ratio == 0 ~ \"On Site\",\n    remote_ratio == 100 ~ \"Remote\",\n    remote_ratio == 50 ~ \"Hybrid\",\n    TRUE ~ \"Other\"\n  ), levels = c(\"On Site\", \"Hybrid\", \"Remote\"))) %&gt;% \n  mutate(experience_level = factor(case_when(\n    experience_level == \"EN\" ~ \"Entry-Level\",\n    experience_level == \"MI\" ~ \"Mid-Level\",\n    experience_level == \"SE\" ~ \"Senior-Level\",\n    experience_level == \"EX\" ~ \"Executive-Level\",\n    TRUE ~ \"Other\"\n  ), levels = c(\"Entry-Level\", \"Mid-Level\", \"Senior-Level\", \"Executive-Level\"))) %&gt;% \n  mutate(company_size = factor(case_when(\n    company_size == \"S\" ~ \"Small\",\n    company_size == \"M\" ~ \"Medium\",\n    company_size == \"L\" ~ \"Large\",\n    TRUE ~ as.character(company_size)\n  ), levels = c(\"Small\", \"Medium\", \"Large\"))) %&gt;% \n  dplyr :: select(-c(\"employment_type\", \"salary_currency\")) %&gt;%\n  rename(\n    WorkLocation = remote_ratio,\n    ExperienceLevel = experience_level,\n    CompanySize = company_size,\n    Salary = salary,\n    WorkYear = work_year,\n    JobTitle = job_title\n  )\n\n# Employee residence for mapping\nSalaries_Residence &lt;- Salaries %&gt;%\n  group_by(employee_residence) %&gt;%\n  summarize(count = n(), \n            mean_salary = mean(salary_in_usd, na.rm = TRUE), \n            median_salary = median(salary_in_usd, na.rm = TRUE))\n\n\n\n\nSalaries_Clean_Regression &lt;- Salaries_Clean %&gt;% \n  mutate(salary_in_usd = as.numeric(salary_in_usd),\n         ExperienceLevel = as.factor(ExperienceLevel),\n         JobTitle = as.factor(JobTitle),\n         employee_residence = as.factor(employee_residence),\n         company_location = as.factor(company_location))\n\ndataset &lt;- Salaries_Clean_Regression\n\n# Stepwise Regression model\nstepwise_model &lt;- stepAIC(lm(salary_in_usd ~ ExperienceLevel + \n                               JobTitle + employee_residence + WorkLocation + company_location + \n                               CompanySize, data = dataset), direction = \"both\")\n\n# World geometry join\nmap_data &lt;- world %&gt;%\n  left_join(Salaries_Residence, by = c(\"iso_a2\" = \"employee_residence\"))\n\n\n\n\n\n\nui &lt;- fluidPage(\n  titlePanel(\"Data Salary (USD) Analysis (2020-2024)\"),\n  tabsetPanel(\n    tabPanel(\"Data Frame\",\n             h4(\"Data Salary Data Frame\"),\n             DTOutput(\"salaries_table\")),\n    tabPanel(\"Location Map\",\n             h4(\"Employee Residence by Country or Continent\"),\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"map_var\", \"Select Variable to Fill Map:\",\n                             choices = c(\"Employee Count\" = \"count\", \n                                         \"Mean Salary\" = \"mean_salary\", \n                                         \"Median Salary\" = \"median_salary\")),\n                 selectInput(\"group_var\", \"Group By:\",\n                             choices = c(\"Country\", \"Continent\"))\n               ),\n               mainPanel(\n                 plotOutput(\"ggplot_map\"),\n                 h4(\"Grouped Table\"),\n                 DTOutput(\"continent_table\")\n               )\n             )\n    ),\n    tabPanel(\"Salary Box Plot\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"color_var\", \"Choose X Input:\", \n                             choices = c(\"ExperienceLevel\", \"WorkLocation\", \"CompanySize\")),\n                 selectizeInput(\"filter_experience\", \"Experience Level:\", \n                                choices = levels(Salaries_Clean$ExperienceLevel), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$ExperienceLevel)),\n                 selectizeInput(\"filter_location\", \"Work Location:\", \n                                choices = levels(Salaries_Clean$WorkLocation), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$WorkLocation)),\n                 selectizeInput(\"filter_company\", \"Company Size:\", \n                                choices = levels(Salaries_Clean$CompanySize), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$CompanySize))\n               ),\n               mainPanel(\n                 plotlyOutput(\"salary_box_plot\"),\n                 h4(\"Mean and Median Salary\"),\n                 DTOutput(\"summary_table\")\n               )\n             )),\n    tabPanel(\"Work Location Proportions\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"job_title\", \n                             \"Select Job Title:\", \n                             choices = sort(unique(Salaries_Clean$JobTitle)), \n                             selected = sort(unique(Salaries_Clean$JobTitle))[1],\n                             multiple = TRUE),\n                 selectizeInput(\"filter_experience_proportions\", \n                                \"Experience Level:\", \n                                choices = levels(Salaries_Clean$ExperienceLevel), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$ExperienceLevel)),\n                 selectizeInput(\"filter_company_proportions\", \n                                \"Company Size:\", \n                                choices = levels(Salaries_Clean$CompanySize), \n                                multiple = TRUE, \n                                selected = levels(Salaries_Clean$CompanySize))\n               ),\n               mainPanel(\n                 plotOutput(\"remote_ratio_plot\")\n               )\n             )),\n    tabPanel(\"Salary Comparison by Residence\",\n             sidebarLayout(\n               sidebarPanel(\n                 selectInput(\"company_size_input\", \"Company Size:\",\n                                        choices = c(\"Small\" = \"S\", \"Medium\" = \"M\", \"Large\" = \"L\"),\n                                        multiple = TRUE,\n                                        selected = c(\"S\", \"M\", \"L\")),\n                            \n                 selectInput(\"experience_level_input\", \"Experience Level:\",\n                                        choices = c(\"Entry-Level\" = \"EN\", \"Mid-Level\" = \"MI\", \n                                                    \"Senior-Level\" = \"SE\", \"Executive-Level\" = \"EX\"),\n                                        multiple = TRUE,\n                                        selected = c(\"EN\", \"MI\", \"SE\", \"EX\")),\n                 selectInput(\n                   \"work_location_input\",\n                   \"Work Location:\",\n                   choices = c(\"On Site\" = \"100\", \"Hybrid\" = \"50\", \"Remote\" = \"0\"), # Dynamically generate location choices\n                   multiple = TRUE,\n                   selected = c(\"100\", \"50\", \"0\"))\n               ),\n               \n               mainPanel(\n                 plotlyOutput(\"residence_plot\")\n               )\n             )\n    ),\n    \n    \n    tabPanel(\"Salary Prediction\",\n    sidebarLayout(\n      sidebarPanel(\n        selectInput(\"experience_level\", \"Experience Level:\", choices = levels(dataset$ExperienceLevel), selected = \"EX\"),\n        selectInput(\"worklocation\", \"Work Location:\", choices = levels(dataset$WorkLocation), selected = \"Hybrid\"),\n        selectInput(\"employee_residence\", \"Employee Country Residence:\", choices = levels(dataset$employee_residence), selected = \"US\"),\n        selectInput(\"companysize\", \"Company Size:\", choices = levels(dataset$CompanySize), selected = \"Medium\"),\n                selectInput(\"jobtitle\", \"Job Title:\", choices = levels(dataset$JobTitle), selected = \"Admin & Data Analyst\"),\n        \n        # Trigger prediction\n        actionButton(\"predict\", \"Predict Salary\")\n      ),\n      \n      mainPanel(\n        tags$style(HTML(\"\n    h3 {\n      font-size: 30px;\n    }\n    #predicted_salary {\n      font-size: 24px;\n    }\n  \")),\n        h3(\"Press the button to predict the salary\"),\n        textOutput(\"predicted_salary\")\n      )\n      \n    )\n    ),\n    \n    \n    \n    # Documentation Tab UI (if you're using `tabPanel` in Shiny)\n    # Documentation Tab UI (using tags$ul and tags$li for lists)\n    tabPanel(\n      \"Documentation\",\n      h3(\"Introduction to the Data Set\"),\n      p(\"This dataset is from Kaggle and provides anonymized information about salaries in the data field for 2024. It has a variety of factors that influence salaries, including experience level, job title, employment type (Remote/Onsite/Hybrid), and company size (Small/Medium/Large). The data also includes geographic location information.\"),\n      \n      p(\"Key variables included in this dataset are:\"),\n      tags$ul(\n        tags$li(strong(\"Experience Level:\") , \" The employees experience level in the data field, categorized as 'Entry-level', 'Mid-level', and 'Senior-level', and 'Executive-level'.\"),\n        tags$li(strong(\"Job Title:\") , \" The specific job title held by the individual, such as Data Scientist, Data Analyst, or Machine Learning Engineer.\"),\n        tags$li(strong(\"Company Size:\") , \" The size of the company where the individual is employed, classified into 'Small', 'Medium', and 'Large' companies\"),\n        tags$li(strong(\"Employment Type:\") , \" Whether the individual is working remotely, onsite, or hybrid.\"),\n        tags$li(strong(\"Location:\") , \" The geographical location of the individual,  representing a country or continent.\"),\n        tags$li(strong(\"Salary:\") , \" The compensation (in USD) received by the individual, representing their annual income.\")\n      ),\n      \n      h3(\"Motivation for Creating the App\"),\n      p(\"The primary motivation behind this app is to allow users to explore and analyze the factors that influence data salaries. With the increasing interest in data as a career, there is a growing need to understand what variables affect salary ranges in this field. By visualizing the salary data across various factors such as experience level, job title, company size, and geographic location, users can gain insights into the salary landscape for data professionals.\"),\n      \n      p(\"Specific questions that motivated the creation of the app include:\"),\n      tags$ul(\n        tags$li(\"How do salaries vary by geographic location, and what factors are most influential in determining location-based pay differences?\"),\n        tags$li(\"How does salary differ by experience level, job title, and company size?\"),\n        tags$li(\"What impact does working remotely or in person have on data salaries?\"),\n        tags$li(\"Based on a linear model, are you being payed fairly?\"),\n        \n      ),\n      \n      p(\"By providing these insights, the app aims to empower users with the ability to see their own salaries, evaluate job market trends, and make data-driven career decisions in the field of data.\"),\n      \n      h3(\"How the App Works\"),\n      p(\"This app allows users to interactively explore the relationships between salary and various factors. The key features of the app are:\"),\n      tags$ul(\n        tags$li(\"Dynamic Filtering: Users can filter the data by experience level, job title, company size, and work location. This helps to focus on specific subsets of the data.\"),\n        tags$li(\"Visualizing Salary Trends: The app generates various visualizations, such as maps, boxplots, and bar charts, to show how salaries are influenced by different variables. Users can easily see how compensation changes with different levels of experience, company size, and job titles.\"),\n        tags$li(\"Geographic Insights: Location-based salary data is displayed, allowing users to compare compensation trends across different geographic areas. Due to limited data, there is a small count for several countries/continents\"),\n        tags$li(\"Interactive Graphs: Users can hover over visualizations in real time, helping them answer specific questions or dive deeper into the data for more information.\")\n      ),\n      p(\"Users can start by selecting one or more variables and adjusting filters to view trends in the dataset. They can then explore the correlations between salaries and factors such as experience level, location, and job title through the visual representations provided.\"),\n      \n      h3(\"Conclusions\"),\n      p(\"From analyzing the dataset, several trends emerge:\"),\n      tags$ul(\n        tags$li(\"Geographical Location: Although the United States and North America have some of the best mean/median pay, conclusions can not be drawn since the counts for other countries and continents are so low.\"),\n        tags$li(\"Experience Level: Higher experience levels generally correlate with higher salaries. Senior and executive roles have a significant salary increase compared to entry-level positions.\"),\n        tags$li(\"Company Size: Larger and medium companies tend to offer higher salaries, likely due to greater financial resources and the scale of operations. Due to a low count for small countries, it may not be entirely accurate\"),\n        tags$li(\"Work Location: On site jobs have a higher pay than remote and hybrid job. Hybrid jobs have a signifcantly lower count so more data would need to be collected for more accurate conclusions.\"),\n        tags$li(\"Salary Comparison by Residence: Those who work in the country they reside in have a higher salary than those who do not overall. Due to the low count of those who do not reside in the country they work in, more data would need to be collected for more accuratae conclusions\")\n      ),\n      p(\"This app helps users see these patterns and more. By interacting with the visualizations, users can draw their own conclusions about the data job market, and make informed decisions about their career paths or salary expectations.\"),\n      \n      h3(\"Citations\"),\n      p(\"Data Source: The dataset used in this app is sourced from \", a(href = \"https://www.kaggle.com/datasets/yusufdelikkaya/datascience-salaries-2024/data\", \"Kaggle: Data Science Salaries 2024\"), \".\"))\n    \n\n  )\n)\n\n\nserver &lt;- function(input, output) {\n  \n  \n  filtered_data &lt;- reactive({\n    Salaries_Clean %&gt;%\n      filter(\n        ExperienceLevel %in% input$filter_experience,\n        WorkLocation %in% input$filter_location,\n        CompanySize %in% input$filter_company\n        \n      )\n  })\n  \n  filtered_data_2 &lt;- reactive({\n    Salaries_Clean %&gt;%\n      filter(\n        ExperienceLevel %in% input$filter_experience_proportions,\n        CompanySize %in% input$filter_company_proportions,\n        JobTitle %in% input$job_title\n      ) \n  })\n  \n  # Data Table\n  output$salaries_table &lt;- renderDT({\n    datatable(Salaries_Clean)\n  })\n  \n  # Salary Box Plot\n  output$salary_box_plot &lt;- renderPlotly({\n    p &lt;- ggplot(filtered_data(), aes_string(x = input$color_var, y = \"salary_in_usd\", fill = input$color_var)) +\n      geom_boxplot() +\n      scale_y_continuous(labels = scales::comma) +  \n      labs(title = paste(\"Salary Box Plot by\", input$color_var),\n           x = input$color_var,\n           y = \"Salary (USD)\") +\n      scale_fill_viridis(discrete = TRUE, begin = 0.3) +  # Begin later for lighter purple\n      theme_minimal()\n    \n    ggplotly(p)  \n  })\n  \n  # Mean and Median Table\n  output$summary_table &lt;- renderDT({\n    Salaries_Clean %&gt;%\n      group_by(!!sym(input$color_var)) %&gt;%\n      summarise(\n        `Count` = n(),\n        `Mean Salary`= round(mean(salary_in_usd, na.rm = TRUE), 2),\n        `Median Salary` = round(median(salary_in_usd, na.rm = TRUE), 2)\n      ) \n  })\n  \n  output$remote_ratio_plot &lt;- renderPlot({\n    filtered_data_2() %&gt;%\n      group_by(WorkYear, WorkLocation) %&gt;%\n      summarise(count = n(), .groups = 'drop') %&gt;%\n      mutate(percentage = count / sum(count) * 100) %&gt;%\n      ggplot(aes(x = as.factor(WorkYear), y = percentage, fill = WorkLocation)) +\n      geom_bar(stat = \"identity\", position = \"fill\") +\n      labs(title = paste(\"Proportion of Work Locations for Selected Filters by Year\"),\n           x = \"Year\",\n           y = \"Proportion (%)\",\n           fill = \"Work Location\") +\n      scale_fill_viridis(discrete = TRUE, begin = 0.3) +  # Begin later so lighter purple\n      theme_minimal()\n  })\n  \n\n  \n  \n\n  \n  output$ggplot_map &lt;- renderPlot({\n    # Dynamically group data with ifelse\n    grouped_map_data &lt;- map_data %&gt;%\n      st_set_geometry(NULL) %&gt;%  # Drop geometry for aggregation\n      group_by(Group = if (input$group_var == \"Country\") name else continent) %&gt;%\n      summarize(\n        count = sum(count, na.rm = TRUE),\n        mean_salary = round(mean(mean_salary, na.rm = TRUE), 2),\n        median_salary = round(median(median_salary, na.rm = TRUE), 2)\n      ) %&gt;%\n      left_join(world, by = c(\"Group\" = if (input$group_var == \"Country\") \"name\" else \"continent\")) %&gt;%\n      st_as_sf()  # Reattach geometry after grouping\n    \n    # Extract the range of the selected variable\n    selected_var &lt;- grouped_map_data[[input$map_var]]\n    min_val &lt;- min(selected_var, na.rm = TRUE)\n    max_val &lt;- max(selected_var, na.rm = TRUE)\n    \n    # Plot the map\n    ggplot(data = grouped_map_data) +\n      geom_sf(aes_string(fill = input$map_var), color = \"white\") +\n      scale_fill_viridis(\n        option = \"C\",\n        direction = 1,\n        na.value = \"lightgrey\",\n        limits = c(min_val, max_val),  # Ensure the scale fits the range\n        breaks = c(min_val, max_val),  # Only show min and max in the legend\n        labels = scales::comma\n      ) +\n      labs(\n        title = paste(\n          if (input$group_var == \"Country\") \"Employee Residence by Country\" else \"Employee Residence by Continent\",\n          \"-\", if (input$map_var == \"count\") \"Employee Count\" else if (input$map_var == \"mean_salary\") \"Mean Salary\" else \"Median Salary\"\n        ),\n        fill = if (input$map_var == \"count\") \"Employee Count\" else if (input$map_var == \"mean_salary\") \"Mean Salary\" else \"Median Salary\"\n      ) +\n      theme_minimal() +\n      theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\"),\n        legend.text = element_text(size = 10)\n      )\n  })\n  \n\n\n  \n  \n  # Grouped Table\n  output$continent_table &lt;- renderDT({\n    grouped_data &lt;- map_data %&gt;%\n      st_set_geometry(NULL) %&gt;%  # Drop geometry column\n      group_by(group = if (input$group_var == \"Country\") name else continent) %&gt;%\n      summarize(\n        `Employee Count` = sum(count, na.rm = TRUE),\n        `Mean Salary` = round(mean(mean_salary, na.rm = TRUE), 2),\n        `Median Salary` = round(median(median_salary, na.rm = TRUE), 2)\n      ) %&gt;%\n      arrange(desc(`Employee Count`))\n    \n    datatable(grouped_data, options = list(pageLength = 5, searching = FALSE))\n  })\n  \n  output$residence_plot &lt;- renderPlotly({\n    \n    # Filter based on inputs\n    filtered_data3 &lt;- reactive({\n      Salaries %&gt;%\n        filter(\n          company_size %in% input$company_size_input,\n          experience_level %in% input$experience_level_input,\n          remote_ratio %in% input$work_location_input \n          \n        ) %&gt;%\n        mutate(\n          CompanySize = case_when(\n            company_size == \"S\" ~ \"Small\",\n            company_size == \"M\" ~ \"Medium\",\n            company_size == \"L\" ~ \"Large\",\n            TRUE ~ as.character(company_size)\n          ),\n          ExperienceLevel = case_when(\n            experience_level == \"EN\" ~ \"Entry-Level\",\n            experience_level == \"MI\" ~ \"Mid-Level\",\n            experience_level == \"SE\" ~ \"Senior-Level\",\n            experience_level == \"EX\" ~ \"Executive-Level\",\n            TRUE ~ as.character(experience_level)\n          ),\n          ResidesInWorkCountry = ifelse(\n            employee_residence == company_location,\n            \"Resides in Work Country\",\n            \"Does Not Reside in Work Country\"\n          )\n        )\n    })\n    \n    data &lt;- filtered_data3()\n    \n    # Count observations \n    counts &lt;- data %&gt;%\n      group_by(ResidesInWorkCountry) %&gt;%\n      summarise(count = n(), .groups = \"drop\")\n    \n    # Make plot\n    p &lt;- plot_ly(\n      data = data,\n      x = ~ResidesInWorkCountry,\n      y = ~salary_in_usd,\n      type = \"box\",\n      color = ~ResidesInWorkCountry,\n      colors = viridis::viridis(2),\n      boxmean = TRUE\n    ) %&gt;%\n      layout(\n        title = \"Salary Distribution for Residents vs Non-Residents in Work Country\",\n        xaxis = list(title = \"Residency Status\"),\n        yaxis = list(title = \"Salary (USD)\"),\n        showlegend = FALSE\n        \n      )\n    \n    \n    # Add count above the boxplots\n    p &lt;- p %&gt;%\n      add_annotations(\n        x = counts$ResidesInWorkCountry,\n        y = max(data$salary_in_usd, na.rm = TRUE) + 5000, # Adjust position\n        text = paste(\"Count: \", counts$count),\n        showarrow = FALSE,\n        font = list(size = 12, color = \"black\"),\n        xanchor = \"center\",\n        yanchor = \"bottom\"\n      )\n    \n    # Return the plot\n    p\n  })\n  \n\n\n  \n  observeEvent(input$predict, {\n    # Collect inputs \n    new_data &lt;- data.frame(\n      ExperienceLevel = input$experience_level,\n      WorkLocation = input$worklocation,\n      employee_residence = input$employee_residence,\n      CompanySize = input$companysize,\n      JobTitle = input$jobtitle,  \n      stringsAsFactors = TRUE  # IDK why this worked\n    )\n    \n    # Predict salary using stepwise model\n    predicted_salary &lt;- predict(stepwise_model, newdata = new_data)\n    \n    # Output salary\n    output$predicted_salary &lt;- renderText({\n      paste(\"The predicted salary in USD is $\", round(predicted_salary, 2))\n    })\n  })\n  \n  \n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/dash/ShinyDashboard.html",
    "href": "posts/dash/ShinyDashboard.html",
    "title": "Real Estate R-Shiny Dashboard",
    "section": "",
    "text": "Create and host a dashboard using R-Shiny to explore housing prices in Taiwan!\nLink to the dashboard: https://nathanbresette.shinyapps.io/real_estate/\nI‚Äôve built countless dashboards using Tableau, PowerBI, Quicksight, and Qualtrics, but I‚Äôve always wanted to create and host one from scratch. After learning how in my data visualization class, I decided to take the plunge!\nThis dashboard tells the story of real estate prices in New Taipei City, Taiwan, and it‚Äôs fully interactive‚Äînot just a static display of data.\nKey Features:\nPairwise Plots: Using my custom R package, InquisitR, I generated detailed pairwise plots for easy exploration of variable relationships.\nInteractive Scatter Plots & Histograms: Users can choose the third variable‚Äôs color and select regression lines for scatterplots. The histogram allows manipulation of the number of bins.\nLeaflet Map: An interactive map visualizes the geographical distribution of properties, revealing key insights into property prices across the city.\nThis experience opened up new possibilities in data storytelling and visualization. While many companies rely on drag-and-drop tools, using R offers complete control over the graphs‚Äîand best of all, it‚Äôs free! The data was very limited with only 7 variables but this was the foundational learning step!\nDashboard Code:\n\nCodelibrary(shiny)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(GGally)\nlibrary(devtools)\n\n# My package!!!\ninstall_github(\"NathanBresette/InquisitR\")\nlibrary(InquisitR)\n\n\nlibrary(tidyr) \n\n# Read the dataset\ndata &lt;- read_excel(\"Real estate valuation data set 2.xlsx\")\n\n# Round numeric columns to three decimal places\nrounded_data &lt;- data %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3))) %&gt;% \n  select(-`No`)\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Real Estate Valuation Analysis\"),\n  \n  # Remove sidebar layout\n  mainPanel(\n    tabsetPanel(\n      id = \"tab\",\n      tabPanel(\"Overview\", \n               h4(\"Explore the dataset and its summary.\"),\n               DTOutput(\"data_table\")),\n      tabPanel(\"Summary Statistics\", \n               h4(\"View key summary statistics of the data.\"),\n               DTOutput(\"summary_stats\")),  # Changed to DTOutput for better presentation\n      tabPanel(\"Pairwise Plot\", \n               h4(\"Explore pairwise relationships between multiple variables.\"),\n               plotOutput(\"pairwisePlot\")),\n      tabPanel(\"Map of Properties\", \n               h4(\"View the geographical distribution of properties.\"),\n               leafletOutput(\"propertyMap\")),\n      tabPanel(\"Distribution of House Prices\", \n               h4(\"Explore the distribution of house prices per unit area.\"),\n               sliderInput(\"bins\", \"Number of bins:\", \n                           min = 5, max = 50, value = 30),  # Add slider to control bins\n               plotOutput(\"priceDistributionPlot\")),\n      tabPanel(\"House Price vs. House Age\", \n               h4(\"Analyze the relationship between house age and price.\"),\n               radioButtons(\"smoothing_age\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_age\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceAgePlot\")),\n      tabPanel(\"House Price vs. Distance to MRT Station\", \n               h4(\"Investigate the effect of distance to MRT stations on house prices.\"),\n               radioButtons(\"smoothing_distance\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_distance\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceDistancePlot\")),\n      tabPanel(\"House Price vs. Convenience Stores\", \n               h4(\"Examine how the number of convenience stores affects house prices.\"),\n               plotOutput(\"priceConveniencePlot\"))\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Overview Data Table\n  output$data_table &lt;- renderDT({\n    datatable(rounded_data, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # Summary Statistics\n  output$summary_stats &lt;- renderDT({\n    summary_stats &lt;- rounded_data %&gt;%\n      summarise(across(everything(), \n                       list(mean = ~round(mean(.x, na.rm = TRUE), 3),\n                            median = ~round(median(.x, na.rm = TRUE), 3),\n                            sd = ~round(sd(.x, na.rm = TRUE), 3),\n                            min = ~round(min(.x, na.rm = TRUE), 3),\n                            max = ~round(max(.x, na.rm = TRUE), 3)))) %&gt;%\n      pivot_longer(everything(), names_to = c(\".value\", \"variable\"), names_sep = \"_\") %&gt;%\n      rename(Variable = variable)\n    \n    datatable(summary_stats, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # House Price vs. House Age\n  output$priceAgePlot &lt;- renderPlot({\n    if (input$color_age == \"none\") {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`, color = .data[[input$color_age]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Distance to MRT Station\n  output$priceDistancePlot &lt;- renderPlot({\n    if (input$color_distance == \"none\") {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`, color = .data[[input$color_distance]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Number of Convenience Stores\n  output$priceConveniencePlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = factor(`X4 number of convenience stores`), y = `Y house price of unit area`)) +\n      geom_boxplot(fill = \"orange\", color = \"black\") +\n      labs(title = \"House Price vs. Number of Convenience Stores\",\n           x = \"Number of Convenience Stores\", y = \"House Price of Unit Area\") +\n      theme_minimal()\n  })\n  \n  # Distribution of House Prices\n  output$priceDistributionPlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = `Y house price of unit area`)) +\n      geom_histogram(bins = input$bins, fill = \"skyblue\", color = \"black\") +  # Use input$bins\n      labs(title = \"Distribution of House Prices\",\n           x = \"House Price of Unit Area\", y = \"Frequency\") +\n      theme_minimal()\n  })\n  \n  # Leaflet map\n  output$propertyMap &lt;- renderLeaflet({\n    leaflet(rounded_data) %&gt;%\n      addTiles() %&gt;%\n      addCircleMarkers(~`X6 longitude`, ~`X5 latitude`,\n                       radius = 5, color = \"blue\", fillOpacity = 0.5,\n                       popup = ~paste(\"Price per unit area:\", `Y house price of unit area`)) %&gt;%\n      setView(lng = 121.54, lat = 24.98, zoom = 13)\n  })\n  \n  # Pairwise Plot\n  output$pairwisePlot &lt;- renderPlot({\n    correlationR(rounded_data)  \n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\nImages of Dashboard\nMap of Taiwan:\n\nCodeknitr::include_graphics(\"Taiwan_Map.png\")\n\n\n\n\n\n\n\nPairwise Plot (My package!!!)\n\nCodeknitr::include_graphics(\"Pairwise_Shiny.png\")\n\n\n\n\n\n\n\nScatterplot\n\nCodeknitr::include_graphics(\"Scatterplot_Shiny.png\")"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html",
    "href": "posts/Brain Tumor/BrainTumor.html",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "",
    "text": "Brain tumor classification using MobileNetV2 on MRI scans with preprocessing, augmentation, model evaluation, and GradCam images"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#introduction",
    "href": "posts/Brain Tumor/BrainTumor.html#introduction",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Introduction",
    "text": "Introduction\nMedical imaging plays a critical role in diagnosing and managing brain tumors, which vary widely in type and severity. Automating tumor classification using neural networks can support radiologists by providing faster, consistent, and potentially more accurate assessments. This project aims to learn how to classify brain tumor MRI images into four categories‚Äîglioma, meningioma, pituitary tumors, and no tumor by using convolutional neural networks (CNNs). Specifically, I leveraged transfer learning with the lightweight MobileNetV2 architecture, pretrained on ImageNet, to adapt it to the medical imaging domain. Through this project, the goals include gaining practical experience in medical image classification, preprocessing real-world MRI datasets, and evaluating model performance using various metrics and visualization techniques such as Grad-CAM."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#background-on-tumor-types",
    "href": "posts/Brain Tumor/BrainTumor.html#background-on-tumor-types",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Background on Tumor Types",
    "text": "Background on Tumor Types\n\nMeningioma:\nTumors arising from the meninges (protective membranes covering brain/spinal cord). Usually benign and slow-growing but may cause pressure effects depending on size/location.\nPituitary Tumors:\nTumors in the pituitary gland (hormone control center at brain base). Usually benign but can alter hormone production, causing various symptoms.\nGlioma:\nTumors originating from glial cells (which support neurons). Tend to be more aggressive and malignant (e.g., astrocytomas, glioblastomas)."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#set-the-python-environment-explicitly-for-reticulate",
    "href": "posts/Brain Tumor/BrainTumor.html#set-the-python-environment-explicitly-for-reticulate",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Set the Python environment explicitly for reticulate",
    "text": "Set the Python environment explicitly for reticulate\n\nCodereticulate::use_python(\"/Users/nathanbresette/Documents/Portfolio/.venv/bin/python\", required = TRUE)\n\nreticulate::py_config()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#imports",
    "href": "posts/Brain Tumor/BrainTumor.html#imports",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Imports",
    "text": "Imports\n\nCodeimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport pandas as pd\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport visualkeras"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#download-dataset-from-kaggle",
    "href": "posts/Brain Tumor/BrainTumor.html#download-dataset-from-kaggle",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Download Dataset from Kaggle",
    "text": "Download Dataset from Kaggle\n\nCodefrom kaggle.api.kaggle_api_extended import KaggleApi\n\nOUTPUT_DIR = \"brain_tumor_classification/data/raw/kaggle_brain_tumor\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef download():\n    api = KaggleApi()\n    api.authenticate()\n    dataset = \"masoudnickparvar/brain-tumor-mri-dataset\"\n    api.dataset_download_files(dataset, path=OUTPUT_DIR, unzip=True)\n    print(\"Dataset downloaded and extracted.\")\n\nif __name__ == \"__main__\":\n    download()\n\nDataset URL: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset\nDataset downloaded and extracted."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#preprocess-images",
    "href": "posts/Brain Tumor/BrainTumor.html#preprocess-images",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Preprocess Images",
    "text": "Preprocess Images\n\nResize images to 224x224 (MobileNetV2 standard)\nNormalize pixel intensities later in data generator\nSplit into train (80%), validation (20%), test sets\nSave file paths and labels as CSVs for efficient loading\n\n\nCodeRAW_DIR = \"brain_tumor_classification/data/raw/kaggle_brain_tumor\"\nPROC_DIR = \"brain_tumor_classification/data/processed/kaggle_brain_tumor\"\nIMG_SIZE = (224, 224)\nSEED = 42\nrandom.seed(SEED)\n\ndef preprocess():\n    train_dir = os.path.join(RAW_DIR, \"Training\")\n    test_dir = os.path.join(RAW_DIR, \"Testing\")\n\n    classes = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n\n    # Create output dirs\n    for split in [\"train\", \"val\", \"test\"]:\n        for cls in classes:\n            os.makedirs(os.path.join(PROC_DIR, split, cls), exist_ok=True)\n\n    # Load train+val images\n    train_val_images = []\n    for cls in classes:\n        cls_path = os.path.join(train_dir, cls)\n        for img_name in os.listdir(cls_path):\n            train_val_images.append((os.path.join(cls_path, img_name), cls))\n\n    random.shuffle(train_val_images)\n    n = len(train_val_images)\n    train_cutoff = int(0.8 * n)  # 80% train, 20% val\n\n    train_images = train_val_images[:train_cutoff]\n    val_images = train_val_images[train_cutoff:]\n\n    # Load test images\n    test_images = []\n    for cls in classes:\n        cls_path = os.path.join(test_dir, cls)\n        for img_name in os.listdir(cls_path):\n            test_images.append((os.path.join(cls_path, img_name), cls))\n\n    def save_split(images, split_name):\n        records = []\n        for i, (src_path, label) in enumerate(images):\n            img = Image.open(src_path).convert(\"RGB\")\n            img = img.resize(IMG_SIZE)\n            filename = f\"{label}_{i:05d}.png\"\n            out_path = os.path.join(PROC_DIR, split_name, label, filename)\n            img.save(out_path)\n            records.append({\"filepath\": out_path, \"label\": label})\n        df = pd.DataFrame(records)\n        df.to_csv(os.path.join(PROC_DIR, f\"{split_name}_labels.csv\"), index=False)\n\n    save_split(train_images, \"train\")\n    save_split(val_images, \"val\")\n    save_split(test_images, \"test\")\n\n    print(\"Preprocessing complete!\")\n\nif __name__ == \"__main__\":\n    preprocess()\n\nPreprocessing complete!"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#build-model",
    "href": "posts/Brain Tumor/BrainTumor.html#build-model",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Build Model",
    "text": "Build Model\n\nLoad CSV label files\nMap labels to numeric classes\nCreate image data generators with pixel rescaling\n\n\nCodePROC_DIR = \"brain_tumor_classification/data/processed/kaggle_brain_tumor\"\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\ntrain_df = pd.read_csv(os.path.join(PROC_DIR, \"train_labels.csv\"))\nval_df = pd.read_csv(os.path.join(PROC_DIR, \"val_labels.csv\"))\n\nlabel_map = {label: idx for idx, label in enumerate(sorted(train_df['label'].unique()))}\ntrain_df[\"class\"] = train_df[\"label\"].map(label_map)\nval_df[\"class\"] = val_df[\"label\"].map(label_map)\n\ndatagen = ImageDataGenerator(rescale=1./255)\n\ntrain_gen = datagen.flow_from_dataframe(\n    train_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42\n)\n\nFound 4569 validated image filenames belonging to 4 classes.\n\nCodeval_gen = datagen.flow_from_dataframe(\n    val_df,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    target_size=IMG_SIZE,\n    class_mode=\"categorical\",\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nFound 1143 validated image filenames belonging to 4 classes."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#visualize-dataset-distribution",
    "href": "posts/Brain Tumor/BrainTumor.html#visualize-dataset-distribution",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Visualize Dataset Distribution",
    "text": "Visualize Dataset Distribution\nDistribution of our four classes which are relatively very balanced\n\nCodetrain_counts = train_df['label'].value_counts().reset_index()\ntrain_counts.columns = ['Class', 'Count']\ntrain_counts['Dataset'] = 'Train'\n\nval_counts = val_df['label'].value_counts().reset_index()\nval_counts.columns = ['Class', 'Count']\nval_counts['Dataset'] = 'Validation'\n\ncounts_df = pd.concat([train_counts, val_counts])\n\nprint(\"Image counts per class:\")\n\nImage counts per class:\n\nCodeprint(counts_df.pivot(index='Class', columns='Dataset', values='Count').fillna(0))\n\nDataset     Train  Validation\nClass                        \nglioma       1056         265\nmeningioma   1052         287\nnotumor      1287         308\npituitary    1174         283\n\nCodeplt.figure(figsize=(8, 5))\nsns.barplot(data=counts_df, x='Class', y='Count', hue='Dataset')\nplt.title('Number of Images per Class in Train and Validation Sets')\nplt.ylabel('Number of Images')\nplt.xlabel('Class')\nplt.show()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#build-model-1",
    "href": "posts/Brain Tumor/BrainTumor.html#build-model-1",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Build Model",
    "text": "Build Model\nI chose to go with MobileNetV2 since it‚Äôs a lightweight, efficient CNN pretrained on ImageNet. This makes it ideal for transfer learning on smaller medical image datasets.\n\nInput Shape (224, 224, 3): This matches MobileNetV2‚Äôs expected input dimensions, enabling reuse of its pretrained weights without modification.\nLearning Rate (0.0001): A low learning rate ensures stable fine-tuning and prevents large gradient updates, which is important when using a pretrained base.\n\n\nCodebase_model = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\nbase_model.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\noutput = Dense(len(label_map), activation=\"softmax\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\")]\n)"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#train-model",
    "href": "posts/Brain Tumor/BrainTumor.html#train-model",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Train Model",
    "text": "Train Model\n\nEpochs (5): A small number of epochs was chosen to avoid overfitting and speed up training during initial experiments, especially since the base model is frozen.\n\n\nCodehistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=5\n)\nCodemodel.save(\"models/mobilenetv2_model.h5\")"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#visualize-model-architecture",
    "href": "posts/Brain Tumor/BrainTumor.html#visualize-model-architecture",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Visualize Model Architecture",
    "text": "Visualize Model Architecture\nThis is the MobileNetV2 architecture!\n\nCodeimg = visualkeras.layered_view(model, legend=True)\nplt.figure(figsize=(20, 8))\nplt.imshow(img)\nplt.axis('off')\n\n(np.float64(-0.5), np.float64(7822.5), np.float64(1232.5), np.float64(-0.5))\n\nCodeplt.show()"
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#plot-training-accuracy-over-epochs",
    "href": "posts/Brain Tumor/BrainTumor.html#plot-training-accuracy-over-epochs",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Plot Training Accuracy Over Epochs",
    "text": "Plot Training Accuracy Over Epochs\n\nCodetr_acc = history.history['accuracy']\ntr_loss = history.history['loss']\ntr_per = history.history['precision']\ntr_recall = history.history['recall']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nval_per = history.history['val_precision']\nval_recall = history.history['val_recall']\n\nindex_loss = np.argmin(val_loss)\nval_lowest = val_loss[index_loss]\nindex_acc = np.argmax(val_acc)\nacc_highest = val_acc[index_acc]\nindex_precision = np.argmax(val_per)\nper_highest = val_per[index_precision]\nindex_recall = np.argmax(val_recall)\nrecall_highest = val_recall[index_recall]\n\nEpochs = [i + 1 for i in range(len(tr_acc))]\nloss_label = f'Best epoch = {str(index_loss + 1)}'\nacc_label = f'Best epoch = {str(index_acc + 1)}'\nper_label = f'Best epoch = {str(index_precision + 1)}'\nrecall_label = f'Best epoch = {str(index_recall + 1)}'\n\nplt.figure(figsize=(20, 6))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_loss, 'r', label='Training loss')\nplt.plot(Epochs, val_loss, 'g', label='Validation loss')\nplt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\nplt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\nplt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.suptitle('Model Training Metrics - Loss and Accuracy', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nLoss consistently decreases over the 5 epochs for both training and validation.the model is learning effectively and not overfitting in the short term.\nValidation accuracy lags slightly behind training accuracy but follows a similar trend, with the best performance reached at epoch 5.\n\n\nCodeplt.figure(figsize=(20, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_per, 'r', label='Precision')\nplt.plot(Epochs, val_per, 'g', label='Validation Precision')\nplt.scatter(index_precision + 1, per_highest, s=150, c='blue', label=per_label)\nplt.title('Precision and Validation Precision')\nplt.xlabel('Epochs')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_recall, 'r', label='Recall')\nplt.plot(Epochs, val_recall, 'g', label='Validation Recall')\nplt.scatter(index_recall + 1, recall_highest, s=150, c='blue', label=recall_label)\nplt.title('Recall and Validation Recall')\nplt.xlabel('Epochs')\nplt.ylabel('Recall')\nplt.legend()\nplt.grid(True)\n\nplt.suptitle('Model Training Metrics - Precision and Recall', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nPrecision steadily improves for both training and validation, with training precision slightly higher than validation, so the model generalizes well but still performs better on the training data.\nRecall also increases consistently, with validation recall slightly higher than training recall in early epochs. This suggests the model became more sensitive to true positives over time, and maintained good generalization.\nEpoch 5 is marked as the best based on all metrics. More epochs could potentially further improve performance, but my Mac is quite slow already."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#evaluate-model-with-classification-report-and-confusion-matrix",
    "href": "posts/Brain Tumor/BrainTumor.html#evaluate-model-with-classification-report-and-confusion-matrix",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Evaluate Model with Classification Report and Confusion Matrix",
    "text": "Evaluate Model with Classification Report and Confusion Matrix\n\nCodefrom sklearn.metrics import classification_report, confusion_matrix\n\ny_true = val_gen.classes\ny_pred = model.predict(val_gen)\n\n\n[1m 1/36[0m [37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m37s[0m 1s/step\n[1m 2/36[0m [32m‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m11s[0m 332ms/step\n[1m 3/36[0m [32m‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m11s[0m 342ms/step\n[1m 4/36[0m [32m‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m10s[0m 326ms/step\n[1m 5/36[0m [32m‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m9s[0m 319ms/step \n[1m 6/36[0m [32m‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m9s[0m 322ms/step\n[1m 7/36[0m [32m‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m9s[0m 324ms/step\n[1m 8/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m9s[0m 324ms/step\n[1m 9/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m8s[0m 324ms/step\n[1m10/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m8s[0m 326ms/step\n[1m11/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m8s[0m 328ms/step\n[1m12/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m7s[0m 330ms/step\n[1m13/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m7s[0m 331ms/step\n[1m14/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m7s[0m 331ms/step\n[1m15/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m7s[0m 334ms/step\n[1m16/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m6s[0m 335ms/step\n[1m17/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m6s[0m 335ms/step\n[1m18/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m5s[0m 333ms/step\n[1m19/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m5s[0m 332ms/step\n[1m20/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m5s[0m 331ms/step\n[1m21/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m4s[0m 329ms/step\n[1m22/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m4s[0m 328ms/step\n[1m23/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m4s[0m 328ms/step\n[1m24/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m3s[0m 328ms/step\n[1m25/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m3s[0m 329ms/step\n[1m26/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m3s[0m 328ms/step\n[1m27/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m2s[0m 326ms/step\n[1m28/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m2s[0m 325ms/step\n[1m29/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m2s[0m 324ms/step\n[1m30/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ‚îÅ[0m [1m1s[0m 323ms/step\n[1m31/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ[0m [1m1s[0m 321ms/step\n[1m32/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ‚îÅ[0m [1m1s[0m 322ms/step\n[1m33/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ[0m [1m0s[0m 322ms/step\n[1m34/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ‚îÅ[0m [1m0s[0m 322ms/step\n[1m35/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m‚îÅ[0m [1m0s[0m 323ms/step\n[1m36/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 335ms/step\n[1m36/36[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 335ms/step\n\nCodey_pred_classes = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_true, y_pred_classes, target_names=label_map.keys()))\n\n              precision    recall  f1-score   support\n\n      glioma       0.85      0.77      0.81       265\n  meningioma       0.73      0.66      0.69       287\n     notumor       0.90      0.93      0.91       308\n   pituitary       0.81      0.94      0.87       283\n\n    accuracy                           0.83      1143\n   macro avg       0.82      0.82      0.82      1143\nweighted avg       0.82      0.83      0.82      1143\n\n\n\nCodecm = confusion_matrix(y_true, y_pred_classes)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nBest performance was on ‚Äúnotumor‚Äù and ‚Äúpituitary‚Äù classes, with F1-scores of 0.91 and 0.88 respectively.\n‚ÄúMeningioma‚Äù was the most challenging class, with the lowest recall (0.61), so many meningioma images were misclassified.\nOverall model accuracy is 83%, with balanced precision and recall across most classes. It showed generally strong performance but room for improvement in class-specific sensitivity."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#grad-cam",
    "href": "posts/Brain Tumor/BrainTumor.html#grad-cam",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Grad-CAM",
    "text": "Grad-CAM\nThis function generates Grad-CAM heatmaps for several images in a batch, showing where the model is focusing when making predictions. It displays both the true and predicted class for each image, helping interpret model behavior visually.\nIn Grad-CAM: - Brighter (hotter) colors = more important - Darker (cooler) colors = less important\n\nCodeimport cv2\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        inputs=model.input,\n        outputs=[model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef display_gradcam(img, heatmap, alpha=0.4):\n    # Resize heatmap to match image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n\n    # Apply colormap (you can change COLORMAP_JET to any OpenCV colormap)\n    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Convert image to BGR for OpenCV overlay\n    img_bgr = np.uint8(255 * img)\n    if img_bgr.shape[-1] == 1:\n        img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n\n    # Overlay heatmap on image\n    overlayed_img = cv2.addWeighted(img_bgr, 1 - alpha, heatmap_color, alpha, 0)\n    \n    # Convert back to RGB for matplotlib\n    overlayed_img = cv2.cvtColor(overlayed_img, cv2.COLOR_BGR2RGB)\n\n    # Show\n    plt.figure(figsize=(6, 6))\n    plt.imshow(overlayed_img)\n    plt.axis(\"off\")\n    plt.title(\"Grad-CAM Overlay\")\n    plt.show()\n\n\ndef gradcam_on_batch(generator, model, last_conv_layer_name=\"Conv_1\", num_images=5):\n    images, labels = next(generator)\n    class_names = list(generator.class_indices.keys())\n\n    displayed = 0\n    for i in range(len(images)):\n        if displayed &gt;= num_images:\n            break\n\n        img = images[i]\n        label_idx = np.argmax(labels[i])\n        true_label = class_names[label_idx]\n\n        img_exp = np.expand_dims(img, axis=0)\n        preds = model.predict(img_exp)\n        pred_idx = np.argmax(preds[0])\n        pred_label = class_names[pred_idx]\n\n        if pred_label == true_label:\n            heatmap = make_gradcam_heatmap(img_exp, model, last_conv_layer_name)\n            print(f\"Image {i+1}: True = {true_label}, Predicted = {pred_label} (Correct)\")\n            display_gradcam(img, heatmap)\n            displayed += 1\n\ngradcam_on_batch(val_gen, model, last_conv_layer_name=\"Conv_1\", num_images=3)\n\n\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 475ms/step\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 484ms/step\nImage 1: True = glioma, Predicted = glioma (Correct)\n\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 29ms/step\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 38ms/step\nImage 2: True = pituitary, Predicted = pituitary (Correct)\n\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 25ms/step\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 34ms/step\n\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 27ms/step\n[1m1/1[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 36ms/step\nImage 4: True = notumor, Predicted = notumor (Correct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the area differs in each image: - Model focuses on features it finds most discriminative, which vary by image. - Differences in predictions, confidence, or tumor appearance shift attention. - Grad-CAM uses low-res feature maps, making heatmaps coarse and inconsistent."
  },
  {
    "objectID": "posts/Brain Tumor/BrainTumor.html#conclusions",
    "href": "posts/Brain Tumor/BrainTumor.html#conclusions",
    "title": "Brain Tumor Classification with TensorFlow and MobileNetV2",
    "section": "Conclusions",
    "text": "Conclusions\nThe MobileNetV2-based model achieved an overall classification accuracy of approximately 83% on the validation set. It demonstrated promising performance for this multi-class brain tumor classification task with limited computing power. The model showed strong precision and recall for ‚Äúno tumor‚Äù and ‚Äúpituitary tumor‚Äù classes. However, the ‚Äúmeningioma‚Äù class proved more challenging, with the lowest recall score, suggesting room for improvement in identifying this tumor type.\nVisualization tools such as Grad-CAM provided useful interpretability, highlighting the regions of MRI scans the model focuses on during classification. This aligns with the goal of developing more transparent and explainable AI models in medical imaging.\nOverall, the project validated the feasibility of transfer learning for brain tumor classification and highlighted the importance of dataset balancing, preprocessing, and careful evaluation in medical image analysis. It was a great learning project"
  },
  {
    "objectID": "posts/This Website/ThisWebsite.html",
    "href": "posts/This Website/ThisWebsite.html",
    "title": "This Website!",
    "section": "",
    "text": "This was my first ‚Äòfrontend‚Äô project that was done through lots of trial and erorr, tutorials, and reading. Overall, I am very pleased with the end result and would recommend Quarto for a portfolio. Once the website was created, it has been very easy to upload new files."
  },
  {
    "objectID": "datafest_file/CourseKata.html",
    "href": "datafest_file/CourseKata.html",
    "title": "Actual DataFest - CourseKata",
    "section": "",
    "text": "24 hour analysis on CourseKata, an online book on statistics. Data cleaning, exploratory data analysis, XGBoost, and suggestions for improvement ## Overview of Project Our initial area of interest upon reception of the data was the subjective student responses in the checkpoints_pulse table, as the developers of CourseKata, Jim Stigler and Ji Son, were primarily interested in the students‚Äô opinions of the textbook. Unfortunately, this data proved unfruitful, as there was no variation in response for any factor we could find. Since subjective measures had no analytical value, we pivoted to looking at objective measures, starting with the EOC variable in the checkpoints_eoc table, which is the final percentage of questions each student answered correctly on the end of chapter (EOC) quiz. This data was much more diverse and had some interesting potential factors that might influence it. We ended up modifying the EOC data into a binary pass/fail variable with a division at 0.6 (for a 60% pass rate) that focused on book College(ABC).\nOur group used a gradient-boosted classification tree to chunk down the variables due to the high cardinality so we could use it as an exploratory model. The model started with 20 gradient classification models and picked the best one utilizing racing anova. The final model had 1649 trees. The importance of variables was calculated by how often they were utilized in the final fitted model to make a decision. The important variables were sum of engagement, average attempt, institution, and chapter. The model produced an AUC of 0.847 with an accuracy of 78.3%.\nOf the top 4 variables our model found to be important, two were student-determined variables and two were environment-determined variables. The total engagement time as well as the average attempts per question, the latter of which we engineered ourselves based on n_possible and n_attempted, were the two most influential variables regarding student pass/fail rate, and were the two student-determined variables. Students who obtained over a 60% on the EOC quizzes spent more time utilizing the textbook than students who obtained less than a 60%. Additionally, students with an average attempts per question over 3 were more likely to have an EOC score below 60%, with less students being over that threshold the higher the average attempts were. The book version for College (ABC) is important to the pass/fail rate of the students with more students improving with newer book versions. There is a large variance in pass/fail rates for institutions with some passing at 75% and others failing at 75%.\nOur next steps, if there was more time, would be to remove or rework the subjective ‚Äúpulse‚Äù questions, looking into different book versions, and investigate discrepancies amongst institutions."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "RgentAI: AI-Powered R Data Science Assistant\n\n\n\nR Programming\n\nAI/ML\n\nWeb Development\n\nData Science\n\nFull-Stack\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nBrain Tumor Classification with TensorFlow and MobileNetV2\n\n\n\nMedical Imaging\n\nDeep Learning\n\nTensorFlow\n\nMobileNetV2\n\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nMRI Super-Resolution with Diffusion Models\n\n\n\nMedical Imaging\n\nDeep Learning\n\nPyTorch\n\nDiffusion Models\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nCollege Football Playoffs Ticket Price Analysis\n\n\n\nAWS Lambda\n\nAWS Eventbridge\n\nAWS S3\n\nPython\n\nWebscraping\n\nR\n\n\n\n\n\n\n\n\n\nJan 3, 2025\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nData Salary Analysis Dashboard\n\n\n\nR-Shiny\n\nTidyverse\n\nPlotly\n\nRegression\n\nPython\n\nWebscraping\n\nR\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nNathan Bresette, Bek Usmonov, Riccardo Crapanzano\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Nate Silver‚Äôs Polling Outcomes:\n\n\n\nBinomial Distributions\n\nVisualizations\n\nWeights\n\nR\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nReal Estate R-Shiny Dashboard\n\n\n\nDashboard\n\nData Visualization\n\nMaps\n\nInteractive\n\nR\n\nR-Shiny\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nPlayfair‚Äôs Recreation & Improvement\n\n\n\nGraphical Critique\n\nData Visualization\n\nR\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nInquisitR\n\n\n\nPackage Creation\n\nFunctions\n\nR\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nThis Website!\n\n\n\nQuarto\n\nHTML\n\nSCSS\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nNCAA Basketball Analysis\n\n\n\nWebscraping\n\nPCA\n\nNeural Network\n\nXGBoost\n\nR\n\nPlotly\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nNathan Bresette\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "",
    "text": "RgentAI is an intelligent AI assistant for RStudio that provides conversational chat, plot analysis, error debugging, and specialized agents.\nOfficial Website\nGitHub Repository"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#key-features",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#key-features",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Key Features",
    "text": "Key Features\n\n5 Specialized Data Science Agents: Statistical analysis, machine learning modeling, data transformation, data cleaning, and visualization\nReal-time Plot Analysis: Automatically captures and analyzes plots from RStudio viewer pane\nEnvironment Awareness: Inspects live R environment including dataframes, variables, and objects\nDirect Code Execution: Executes generated R code directly in RStudio console\nConversation Memory: Maintains context across sessions with intelligent indexing\nProduction-Ready: User management, billing, security, and cloud infrastructure"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#system-components",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#system-components",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "System Components",
    "text": "System Components\ngraph TB\n    A[RStudio Addin] --&gt; B[WebSocket Client]\n    B --&gt; C[Python FastAPI Backend]\n    C --&gt; D[PostgreSQL Database]\n    C --&gt; E[Claude AI API]\n    C --&gt; F[Response Cache]\n    G[React Frontend] --&gt; C\n    H[Stripe Billing] --&gt; C"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#fastapi-server-backendmain.py",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#fastapi-server-backendmain.py",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "FastAPI Server (backend/main.py)",
    "text": "FastAPI Server (backend/main.py)\nThe backend is built with Python FastAPI and handles all core functionality:\n\nCore Endpoints\n\n/chat - Main chat endpoint with conversation memory\n/chat/stream - Real-time streaming responses\n/context/capture - R environment context capture\n/plot/analyze - Plot analysis and interpretation\n/agent/execute - Specialized agent execution\n\n\n\nKey Features\n# Conversation memory with PostgreSQL indexing\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat_with_ai(request: ChatRequest):\n    conversation_id = request.conversation_id\n    if request.new_conversation or not conversation_id:\n        conversation_id = user_manager.start_conversation(request.access_code)\n    \n    # Enhanced prompt with context\n    enhanced_prompt = build_enhanced_prompt(\n        request.prompt, \n        request.context_data, \n        conversation_history\n    )\n\n\nSmart Response Caching\nclass SmartResponseCache:\n    def __init__(self, max_cache_size: int = 500, cache_ttl_hours: int = 6):\n        self.cache = {}\n        self.context_summarizer = ContextSummarizer()\n        \n    def is_cacheable_question(self, prompt: str) -&gt; bool:\n        # Intelligent caching based on question type\n        cacheable_patterns = [\n            r\"how\\s+to\\s+\", r\"what\\s+is\\s+\", r\"explain\\s+\",\n            r\"create\\s+a\\s+\", r\"plot\\s+\", r\"visualize\\s+\"\n        ]"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#database-schema-backenduser_management_postgres.py",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#database-schema-backenduser_management_postgres.py",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Database Schema (backend/user_management_postgres.py)",
    "text": "Database Schema (backend/user_management_postgres.py)\n\nCore Tables\n-- Users and authentication\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    access_code VARCHAR(50) UNIQUE NOT NULL,\n    stripe_customer_id VARCHAR(100),\n    subscription_status VARCHAR(50) DEFAULT 'inactive',\n    daily_limit INTEGER DEFAULT 1000,\n    monthly_budget DECIMAL(10,2) DEFAULT 50.0\n);\n\n-- Conversation memory\nCREATE TABLE contexts (\n    id SERIAL PRIMARY KEY,\n    access_code VARCHAR(50) NOT NULL,\n    session_id VARCHAR(64) NOT NULL,\n    context_data JSONB,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Usage tracking\nCREATE TABLE usage_records (\n    id SERIAL PRIMARY KEY,\n    access_code VARCHAR(50) NOT NULL,\n    request_type VARCHAR(50),\n    tokens_used INTEGER,\n    cost DECIMAL(10,4),\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#rstudio-addin-clean_packagerwebsocket_addin.r",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#rstudio-addin-clean_packagerwebsocket_addin.r",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "RStudio Addin (clean_package/R/websocket_addin.R)",
    "text": "RStudio Addin (clean_package/R/websocket_addin.R)\nThe R package provides seamless integration with RStudio:\n\nWebSocket Integration\n# Real-time communication with backend\nws &lt;- websocket::WebSocket$new(\"wss://rgent.onrender.com/ws\")\nws$onMessage(function(event) {\n  response &lt;- jsonlite::fromJSON(event$data)\n  if (response$type == \"code\") {\n    rstudioapi::sendToConsole(response$content)\n  }\n})\n\n\nContext Capture\n# Automatic environment capture\ncapture_context &lt;- function() {\n  list(\n    dataframes = get_available_dataframes(),\n    plots = get_active_plots(),\n    variables = ls(globalenv()),\n    working_directory = getwd(),\n    loaded_packages = .packages()\n  )\n}"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#react-web-interface-payment-frontend",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#react-web-interface-payment-frontend",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "React Web Interface (payment-frontend/)",
    "text": "React Web Interface (payment-frontend/)\n\nKey Components\n\nChat Interface - Real-time messaging with streaming responses\nAgent Selection - Choose from 5 specialized data science agents\nPlot Viewer - Display and analyze R plots\nCode Execution - Send code directly to RStudio\nUser Dashboard - Usage tracking and subscription management\n\n\n\nInstallation Page\n&lt;!-- Streamlined installation process --&gt;\n&lt;div class=\"code-block\"&gt;\n  &lt;code&gt;devtools::install_github(\"NathanBresette/Rgent-AI\", force = TRUE, upgrade = \"never\")&lt;/code&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#real-time-data-flow",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#real-time-data-flow",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Real-time Data Flow",
    "text": "Real-time Data Flow\nsequenceDiagram\n    participant R as RStudio Addin\n    participant WS as WebSocket\n    participant B as Backend\n    participant AI as Claude API\n    \n    R-&gt;&gt;WS: Send chat message + context\n    WS-&gt;&gt;B: Process request\n    B-&gt;&gt;AI: Enhanced prompt with context\n    AI--&gt;&gt;B: Streaming response\n    B--&gt;&gt;WS: Stream chunks\n    WS--&gt;&gt;R: Real-time updates\n    R-&gt;&gt;R: Execute generated code"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#message-types",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#message-types",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Message Types",
    "text": "Message Types\n\nchat - Standard chat messages\ncontext - R environment data\nplot - Plot analysis requests\nagent - Specialized agent execution\ncode - R code execution"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#statistical-analysis-agent-clean_packagerstatistical_agent.r",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#statistical-analysis-agent-clean_packagerstatistical_agent.r",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "1. Statistical Analysis Agent (clean_package/R/statistical_agent.R)",
    "text": "1. Statistical Analysis Agent (clean_package/R/statistical_agent.R)\nComprehensive statistical testing with intelligent test selection:\nstart_statistical_analysis &lt;- function(dataframe_name, analysis_options, variables, method_options) {\n  # Automatic test selection based on data characteristics\n  if (group_count == 2) {\n    test_result &lt;- perform_two_group_test(df, continuous_var, grouping_var)\n  } else {\n    test_result &lt;- perform_anova_test(df, continuous_var, grouping_var)\n  }\n  \n  # Effect size calculation\n  effect_size &lt;- calculate_cohens_d(group1_data, group2_data)\n}\nCapabilities: - T-tests, ANOVA, Chi-squared tests - Effect size calculations (Cohen‚Äôs d, eta-squared) - Power analysis and sample size calculations - Multiple testing corrections - Assumption checking and non-parametric alternatives"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#machine-learning-agent-clean_packagermodeling_agent.r",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#machine-learning-agent-clean_packagermodeling_agent.r",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "2. Machine Learning Agent (clean_package/R/modeling_agent.R)",
    "text": "2. Machine Learning Agent (clean_package/R/modeling_agent.R)\nComprehensive ML workflows with model interpretability:\nexecute_linear_regression &lt;- function(df, target_variable, options, selected_variables) {\n  # Feature importance calculation\n  feature_importance &lt;- data.frame(\n    feature = names(coefficients),\n    coefficient = coefficients,\n    abs_coefficient = abs(coefficients)\n  )\n  \n  # Residuals analysis\n  residuals_analysis &lt;- list(\n    mean_residual = mean(resid(model)),\n    normality_test = shapiro.test(resid(model))$p.value\n  )\n}\nSupported Algorithms: - Linear/Logistic/Multinomial Regression - Random Forest with feature importance - XGBoost with interpretability - Dimensionality reduction (PCA, t-SNE, UMAP) - Feature engineering and selection"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#data-transformation-agent-clean_packagertransformation_agent.r",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#data-transformation-agent-clean_packagertransformation_agent.r",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "3. Data Transformation Agent (clean_package/R/transformation_agent.R)",
    "text": "3. Data Transformation Agent (clean_package/R/transformation_agent.R)\nIntelligent data preprocessing and transformation:\ngenerate_transformation_step_code &lt;- function(step_info, dataframe, method_options) {\n  switch(step_info$operation,\n    \"mathematical_transformations\" = {\n      # Log, sqrt, power transformations based on skewness\n      if (abs(skewness) &gt; 1.5) {\n        cat(\"SUGGESTION: Highly skewed - consider log/sqrt transformation\\n\")\n      }\n    },\n    \"categorical_transformations\" = {\n      # Dummy encoding, recoding, combining categories\n    }\n  )\n}\nTransformation Types: - Mathematical (log, sqrt, power, polynomial) - Categorical (dummy encoding, recoding, combining) - DateTime (extracting components, time differences) - Statistical (z-score, normalization, ranking) - Text (case changes, pattern extraction) - Spatial (coordinate transformations, distance calculations)"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#data-cleaning-agent-clean_packageragent.r",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#data-cleaning-agent-clean_packageragent.r",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "4. Data Cleaning Agent (clean_package/R/agent.R)",
    "text": "4. Data Cleaning Agent (clean_package/R/agent.R)\nAutomated data quality assessment and cleaning:\nexecute_cleaning_agent &lt;- function(dataframe, na_handling = \"median\", iteration = 1) {\n  cleaning_steps &lt;- list(\n    list(operation = \"examine_structure\", code = paste0(\"str(\", dataframe, \")\")),\n    list(operation = \"analyze_summary\", code = paste0(\"summary(\", dataframe, \")\")),\n    list(operation = \"handle_nas\", code = generate_na_handling_code(dataframe, na_handling)),\n    list(operation = \"fix_data_types\", code = generate_type_conversion_code(dataframe)),\n    list(operation = \"remove_outliers\", code = generate_outlier_removal_code(dataframe))\n  )\n}\nCleaning Operations: - Missing value analysis and imputation - Data type detection and conversion - Outlier detection (IQR, Z-score, Modified Z-score) - Duplicate identification and removal - Column naming standardization - Custom cleaning operations"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#visualization-agent",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#visualization-agent",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "5. Visualization Agent",
    "text": "5. Visualization Agent\nPlot analysis and intelligent visualization suggestions:\n# Plot analysis capabilities\nanalyze_plot &lt;- function(plot_data) {\n  # Extract plot characteristics\n  plot_type &lt;- detect_plot_type(plot_data)\n  aesthetics &lt;- extract_aesthetics(plot_data)\n  \n  # Provide intelligent suggestions\n  suggestions &lt;- generate_plot_suggestions(plot_type, aesthetics)\n}"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#intelligent-context-capture-backendcontext_summarizer.py",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#intelligent-context-capture-backendcontext_summarizer.py",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Intelligent Context Capture (backend/context_summarizer.py)",
    "text": "Intelligent Context Capture (backend/context_summarizer.py)\nclass ContextSummarizer:\n    def summarize_context(self, context_data: dict) -&gt; str:\n        # Extract key information from R environment\n        dataframes = context_data.get('dataframes', [])\n        plots = context_data.get('plots', [])\n        variables = context_data.get('variables', [])\n        \n        # Create concise summary for AI prompt\n        summary = f\"Dataframes: {', '.join(dataframes)}\\n\"\n        summary += f\"Active plots: {len(plots)}\\n\"\n        summary += f\"Variables: {len(variables)}\"\n        \n        return summary"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#conversation-memory",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#conversation-memory",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Conversation Memory",
    "text": "Conversation Memory\n\nPostgreSQL indexing for fast context retrieval\nSimilarity matching for relevant conversation history\nContext summarization to maintain prompt efficiency\nSession management with automatic cleanup"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#security-features",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#security-features",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Security Features",
    "text": "Security Features\n# Content Security Policy\ncsp_policy = (\n    \"default-src 'self'; \"\n    \"script-src 'self' 'unsafe-inline' https://js.stripe.com; \"\n    \"connect-src 'self' https://api.stripe.com https://rgent.onrender.com; \"\n    \"upgrade-insecure-requests;\"\n)"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#user-management",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#user-management",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "User Management",
    "text": "User Management\n\nAccess code authentication (16-character alphanumeric)\nUsage tracking with daily/monthly limits\nSubscription tiers (Free Trial, Pro Haiku, Pro Sonnet)\nRate limiting and cost controls\nStripe integration for billing"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#cloud-infrastructure",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#cloud-infrastructure",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Cloud Infrastructure",
    "text": "Cloud Infrastructure\n\nRender.com deployment with auto-scaling\nPostgreSQL database with connection pooling\nHTTPS with Let‚Äôs Encrypt certificates\nCDN integration for static assets\nMonitoring and logging with error tracking"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#response-caching",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#response-caching",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Response Caching",
    "text": "Response Caching\n# Smart caching based on question type\ndef is_cacheable_question(self, prompt: str) -&gt; bool:\n    cacheable_patterns = [\n        r\"how\\s+to\\s+\", r\"what\\s+is\\s+\", r\"explain\\s+\",\n        r\"create\\s+a\\s+\", r\"plot\\s+\", r\"visualize\\s+\"\n    ]\n    return any(re.search(pattern, prompt.lower()) for pattern in cacheable_patterns)"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#memory-management",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#memory-management",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Memory Management",
    "text": "Memory Management\n\nStreaming responses to reduce memory usage\nContext summarization to limit prompt size\nAutomatic cleanup of old conversations\nEfficient database queries with proper indexing"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#quick-start",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#quick-start",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Quick Start",
    "text": "Quick Start\n# Install from GitHub\ndevtools::install_github(\"NathanBresette/Rgent-AI\", force = TRUE, upgrade = \"never\")\n\n# Load the package\nlibrary(rstudioai)\n\n# Launch the AI assistant\nrun_rgent()"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#web-interface",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#web-interface",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Web Interface",
    "text": "Web Interface\nVisit rgentai.com for: - User dashboard and usage tracking - Subscription management - Installation instructions - Documentation and examples"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#planned-features",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#planned-features",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Planned Features",
    "text": "Planned Features\n\nCustom agent creation - Users can define their own specialized agents\nCollaborative features - Share agents and workflows with teams\nAdvanced visualization - Interactive plot editing and manipulation\nModel deployment - Deploy trained models directly from RStudio\nIntegration expansion - Support for Python, Julia, and other languages"
  },
  {
    "objectID": "posts/portfolio_rgentai/portfolio_rgentai.html#technical-roadmap",
    "href": "posts/portfolio_rgentai/portfolio_rgentai.html#technical-roadmap",
    "title": "RgentAI: AI-Powered R Data Science Assistant",
    "section": "Technical Roadmap",
    "text": "Technical Roadmap\n\nPerformance improvements - Faster response times and better caching\nEnhanced security - Additional authentication methods and audit logging\nScalability - Support for enterprise deployments and team management\nAPI expansion - Public API for third-party integrations"
  },
  {
    "objectID": "datafest_file/DataFest_Final.html",
    "href": "datafest_file/DataFest_Final.html",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "",
    "text": "Teams were asked to explore real, proprietary data provided from the commercial real estate advising firm Savills, looking for property categories that might be systemically overpriced or underpriced for their particular market. Students were given multiple tables of data, totaling more than six million rows of data across more than 100 total variables. The completed report and presentation were reviewed by a panel of real-world experts who selected winners in several categories."
  },
  {
    "objectID": "datafest_file/DataFest_Final.html#join-tables",
    "href": "datafest_file/DataFest_Final.html#join-tables",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "Join Tables",
    "text": "Join Tables\n\nCode# Join the datasets by common columns\nLeases &lt;- merge(Leases, Unemployment, by = c(\"year\", \"quarter\", \"state\"), all.x = TRUE)\nLeases &lt;- merge(Leases, MajorMarket, by = c(\"year\", \"quarter\", \"market\"), all.x = TRUE)"
  },
  {
    "objectID": "datafest_file/DataFest_Final.html#group-by-to-remove-duplicates",
    "href": "datafest_file/DataFest_Final.html#group-by-to-remove-duplicates",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "Group by to remove duplicates",
    "text": "Group by to remove duplicates\n\nCodeget_mode &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Get grouped data since there are so many duplicate rows\ngrouped_data &lt;- Leases %&gt;%\n  group_by(overall_rent) %&gt;%\n  summarize(\n    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),\n    # ending_occupancy_proportion = mean(ending_occupancy_proportion, na.rm = TRUE), - had too many NA's to be utilized for our model\n    # starting_occupancy_proportion = mean(starting_occupancy_proportion, na.rm = TRUE), - had too many NA's to be utilized for our model\n    avg_occupancy_proportion = mean(avg_occupancy_proportion, na.rm = TRUE),\n    leasedSF = mean(leasedSF, na.rm = TRUE),\n    RBA = mean(RBA, na.rm = TRUE),\n    availability_proportion = mean(availability_proportion, na.rm = TRUE),\n    market = get_mode(market),\n    zip = get_mode(zip),\n    internal_class = get_mode(internal_class),\n    transaction_type = get_mode(transaction_type),\n    space_type = get_mode(space_type),\n    CBD_suburban = get_mode(CBD_suburban),\n\n    year = get_mode(year),\n    monthsigned = get_mode(monthsigned),\n    .groups = 'drop'  \n  )\n\nhead(grouped_data)"
  },
  {
    "objectID": "datafest_file/DataFest_Final.html#impute-na-values",
    "href": "datafest_file/DataFest_Final.html#impute-na-values",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "Impute NA values",
    "text": "Impute NA values\n\nCodeimpute_mode &lt;- function(x) {\n  mode_value &lt;- as.character(names(sort(table(x), decreasing = TRUE))[1])\n  x[is.na(x)] &lt;- mode_value\n  return(x)\n}\n\nimpute_mean &lt;- function(df) {\n  numeric_cols &lt;- sapply(df, is.numeric)\n  df[, numeric_cols] &lt;- lapply(df[, numeric_cols], function(x) {\n    x[is.na(x)] &lt;- mean(x, na.rm = TRUE)  # Replace NA with the mean of the column\n    return(x)\n  })\n  return(df)\n}\n\n\n\ngrouped_data$space_type &lt;- impute_mode(grouped_data$space_type)\ngrouped_data &lt;- impute_mean(grouped_data)\n\nna_values &lt;- cbind(lapply(lapply(grouped_data, is.na), sum))\nna_values"
  },
  {
    "objectID": "datafest_file/DataFest_Final.html#vip-plot",
    "href": "datafest_file/DataFest_Final.html#vip-plot",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "VIP Plot",
    "text": "VIP Plot\n\nCodeimportance_data &lt;- importance(rf_model)\n\n\nimportance_df &lt;- data.frame(Variable = rownames(importance_data), \n                            Importance = importance_data[, 1])  \n\nimportance_df &lt;- importance_df %&gt;% arrange(desc(Importance))\n\nggplot(importance_df, aes(x = reorder(Variable, abs(Importance)), y = abs(Importance))) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") + \n  coord_flip() + \n  labs(title = \"Variable Importance for Predicting Overall Rent\",\n       x = \"\",\n       y = \"Importance\",\n       caption = \"DataFest 2025 Analysis\") +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray90\", fill = NA),\n    plot.caption = element_text(color = \"gray50\", face = \"italic\")\n  )"
  },
  {
    "objectID": "datafest_file/DataFest_Final.html#create-price-variable",
    "href": "datafest_file/DataFest_Final.html#create-price-variable",
    "title": "Actual DataFest - Real Estate Analysis",
    "section": "Create Price variable",
    "text": "Create Price variable\n\nCodepredictions &lt;- predict(rf_model, grouped_data)\n\ngrouped_data$predicted_rent &lt;- predictions\n\ngrouped_data$price_category &lt;- ifelse(grouped_data$overall_rent &gt; grouped_data$predicted_rent, 1, 0)"
  },
  {
    "objectID": "datafest_file/Practice_Datafest2025.html",
    "href": "datafest_file/Practice_Datafest2025.html",
    "title": "Practice DataFest - Royals Analysis",
    "section": "",
    "text": "Research Question: What factors contributed to the pitchers‚Äô overall success for the Kansas City Royals 2024 season?"
  },
  {
    "objectID": "datafest_file/Practice_Datafest2025.html#introduction",
    "href": "datafest_file/Practice_Datafest2025.html#introduction",
    "title": "Practice DataFest - Royals Analysis",
    "section": "Introduction",
    "text": "Introduction\nThis report presents an in-depth analysis of Kansas City Royals baseball 2024 season data, focusing on pitching performance using statistical and machine learning techniques. The analysis leverages dplyr for data cleaning, ggplot2 for visualization, and tidymodels for predictive modeling.\nThis project was completed in six hours from coming up with a research questions, creating visualizations, predictice models, and key takeaways. We did this to practice for DataFest, a regional data competition, the following week.\n\nCodelibrary(tidyverse)\nlibrary(scales)\nlibrary(randomForest)\nlibrary(vip)\nlibrary(caret)\nlibrary(knitr)\nlibrary(kableExtra)"
  },
  {
    "objectID": "datafest_file/Practice_Datafest2025.html#data-cleaning",
    "href": "datafest_file/Practice_Datafest2025.html#data-cleaning",
    "title": "Practice DataFest - Royals Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nData Source\nThe data is collected from https://statds.org/events/csas2025/challenge.html that provides over 700,000 rows and 113 columns on the 2024 MLB season. A vector of KC Royals hitters is created for future use in our data cleaning.\n\nCode# Vector of Kansas City Royals Hitters\nkc_batters &lt;- c(\n  \"Witt Jr., Bobby\", \"Perez, Salvador\", \"Garcia, Maikel\", \n  \"Pasquantino, Vinnie\", \"Melendez, MJ\", \"Renfroe, Hunter\",\n  \"Isbel, Kyle\", \"Fermin, Freddy\", \"Massey, Michael\",\n  \"Frazier, Adam\", \"Hampson, Garrett\", \"Velazquez, Nelson\",\n  \"Loftin, Nick\", \"Blanco, Dairon\", \"DeJong, Paul\",\n  \"Pham, Tommy\", \"Gurriel, Yuli\", \"Grossman, Robbie\",\n  \"Waters, Drew\", \"Alexander, CJ\", \"Gentry, Tyler\"\n)\n\n\nFiltering and Feauture Engineering\nSince our research question is for the Kansas City Royals, we select only games that the KC Royals played in. The vector of royals players is now used to filter any rows where they were at-bat. This ensures that we now only have rows where the opposing team only has hitters since we want to look at KC Royals pitching. Acceleration and velocity each have an x,y, and z variable so to condense the number of variables, we combine the x,y, and z into a single normalized variable for both accelaration and velocity. Our team selected the rest of the columns after research and choosing all columns that could be related to pitching data. Lastly, a new variable was created that determines whether a pitcher was succesful. If the pitcher got a strikeout, fieldout, double_play, strikeout into a double play, force out, fielders choice out, field error, catcher interference (not the pitcher‚Äôs fault), or a truncated plate appearance, it was recorded as a success. All other outcomes such as singles, doubles, triples, homeruns, sac-bunts, or sac-flys, are recorded as a pitcher‚Äôs failure.\n\nCodedata_baseball &lt;- read.csv(\"~/Downloads/statcast_pitch_swing_data_20240402_20241030_with_arm_angle.csv\", stringsAsFactors=TRUE)\n\n\nkc_data &lt;- data_baseball %&gt;%\n  filter(home_team == \"KC\" | away_team == \"KC\") %&gt;%\n  filter(!(batter %in% kc_batters)) %&gt;%\n  mutate(\n    vx0_normalized = scale(vx0),\n    vy0_normalized = scale(vy0),\n    vz0_normalized = scale(vz0),\n    ax_normalized = scale(ax),\n    ay_normalized = scale(ay),\n    az_normalized = scale(az)\n  ) %&gt;%\n  mutate(\n    combined_normalized_velocity = rowMeans(select(., vx0_normalized, vy0_normalized, vz0_normalized)),\n    combined_normalized_acceleration = rowMeans(select(., ax_normalized, ay_normalized, az_normalized))\n  ) %&gt;% \n  # Exclude rows where the batter is on the KC 2024 roster\n  select(\n    events, pitcher, batter, release_speed, release_spin_rate, effective_speed, \n    pfx_x, pfx_z, zone, plate_x, plate_z, events, hit_distance_sc, \n    woba_value, delta_pitcher_run_exp, game_date, outs_when_up, \n    home_team, away_team, hc_x, hc_y, description, combined_normalized_velocity, combined_normalized_acceleration, pitch_name,      pitch_type, spin_axis\n  ) %&gt;% \n  mutate(pitcher_outcome = case_when(\n    events %in% c(\"strikeout\", \"field_out\", \"double_play\", \"strikeout_double_play\", \"force_out\", \"fielders_choice_out\",             \"field_error\", \"catcher_interf\", \"truncated_pa\") ~ \"success\", \n    events %in% c(\"single\", \"double\", \"triple\", \"home_run\", \"walk\", \"hit_by_pitch\", \n                  \"fielders_choice\", \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\") ~ \"fail\"  )) %&gt;% \n  filter(!is.na(pitcher_outcome))"
  },
  {
    "objectID": "datafest_file/Practice_Datafest2025.html#exploratory-data-analysis",
    "href": "datafest_file/Practice_Datafest2025.html#exploratory-data-analysis",
    "title": "Practice DataFest - Royals Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThe distributions, correlations, and boxplots of our variables were analyzed using InquisitR package. Since so many graphs are produced during the EDA stage, the output will be excluded.\n\nCodedistributionR(kc_data)\nboxplotR(kc_data)\n\ncorrelationR(kc_data)"
  },
  {
    "objectID": "datafest_file/Practice_Datafest2025.html#pitching-analysis",
    "href": "datafest_file/Practice_Datafest2025.html#pitching-analysis",
    "title": "Practice DataFest - Royals Analysis",
    "section": "Pitching Analysis",
    "text": "Pitching Analysis\nThe model shows that the location of where the ball is thrown is important to the pitchers success. This can be visualized in two main ways with the first way being zones compared to hit distances. The 1-9 zones are a strike while 11-14 are balls. Zones 7-9 are lower in the strikezone which shows that the pitchers allow a smaller hit distance when the ball is thrown here. Zones 11-14 are not over the plate (not a strike) which also have a lower hit distance. As our ALE plot shows, a lower hit distance gives a better chance of pitcher success.\n\nCodeggplot(kc_data, aes(x = factor(zone), y = hit_distance_sc, fill = pitcher_outcome)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    title = \"Hit Distance by Zone and Outcome\",\n    x = \"Pitch Zone\",\n    y = \"Hit Distance\",\n    fill = \"Pitcher Outcome\"\n  ) +\n  scale_fill_manual(values = c(\"success\" = \"#004687\", \"fail\" = \"#FFA500\")) +\n  theme_minimal()\n\n\nThe second way is shown with a heat chart a location of all pitches thrown from the KC Pitchers. A majority of the pitches were thrown down with a wide variety.\n\nCode# Create strike zone coordinates\nstrike_zone &lt;- data.frame(\n  x = c(-0.708, 0.708, 0.708, -0.708, -0.708),  # Horizontal edges (ft from center)\n  y = c(1.5, 1.5, 3.5, 3.5, 1.5)               # Vertical edges (ft from ground)\n)\n\n# Create the heatmap with proper strike zone overlay\nggplot(kc_data, aes(x = plate_x, y = plate_z)) + \n  geom_bin2d() +  \n  scale_fill_viridis_c(option = \"plasma\") + \n  geom_path(\n    data = strike_zone, \n    aes(x = x, y = y), \n    color = \"black\", \n    linewidth = 1) +  \n  coord_fixed() +  \n  labs(\n    title = \"Pitch Density Heatmap (KC Pitchers)\", \n    x = \"Horizontal Location (feet from center)\", \n    y = \"Vertical Location (feet from ground)\"\n  ) + \n  theme_minimal()\n\n\nThis scatterplot shows where hits are located based on hc_x and hx_y. Where there are several gaps in the data is where the defenders stand. The majority of doubles and triples also have a much further hit distance than singles.\n\nCode# Clean version without NA points\nggplot(kc_data, aes(x = hc_x, y = -hc_y)) + \n  geom_point(aes(color = events), alpha = 0.7) + \n  theme_minimal() + \n  labs(\n    title = \"Hit Ball Locations on the Field (KC Games)\", \n    x = \"Horizontal Location (hc_x)\", \n    y = \"Vertical Location (hc_y)\", \n    color = \"Event Type\"\n  ) + \n  scale_color_manual(\n    values = c(\n      \"single\" = \"#004687\",\n      \"double\" = \"#FF5F1F\",\n      \"triple\" = \"#00B5B8\",\n      \"home_run\" = \"#F2C800\"\n    ),\n    limits = c(\"single\", \"double\", \"triple\", \"home_run\"),  # Reorder legend\n    labels = c(\"single\" = \"Single\", \"double\" = \"Double\", \"triple\" = \"Triple\", \"home_run\" = \"Home Run\"),  # Rename legend labels\n    na.value = NA  # Explicitly remove NA values\n  )\n\n\nRelease speed was also found as an important\n\nCodespeed_bins_clean &lt;- kc_data %&gt;%\n  filter(!is.na(release_speed), !is.na(pitcher_outcome), \n         release_speed &gt;= 70, release_speed &lt;= 100) %&gt;%\n  mutate(\n    speed_bin = cut(\n      release_speed,\n      breaks = seq(70, 100, by = 5),\n      labels = paste0(seq(70, 95, by = 5), \"-\", seq(75, 100, by = 5)),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(speed_bin) %&gt;%\n  summarise(success_rate = mean(pitcher_outcome == \"success\"), .groups = \"drop\")\n\nggplot(speed_bins_clean, aes(x = speed_bin, y = success_rate)) +\n  geom_col(fill = \"blue\", width = 0.7, alpha = 0.8) +\n  geom_text(\n    aes(label = percent(success_rate, accuracy = 1)),\n    vjust = -0.5, \n    size = 3.5,\n    fontface = \"bold\"\n  ) +\n  scale_y_continuous(\n    labels = percent_format(),\n    limits = c(0, max(speed_bins_clean$success_rate) * 1.1)) +\n  labs(\n    title = \"Pitcher Success Rate by Release Speed (KC Games\",\n    x = \"Release Speed (mph)\",\n    y = \"Success Rate\"\n  ) +\n  theme_minimal()\n\n\n\nCodelibrary(tidymodels)\nset.seed(123)\n\n# Prepare data\nrf_data &lt;- kc_data %&gt;%\n  mutate(pitcher_outcome = factor(pitcher_outcome, levels = c(\"fail\", \"success\"))) %&gt;% \n  select(-c(\"pitcher\", \"batter\", \"game_date\", \"events\", \"woba_value\", \n           \"delta_pitcher_run_exp\", \"description\", \"hc_x\", \"hc_y\"))\n\ntrain_index &lt;- createDataPartition(rf_data$pitcher_outcome, p = 0.8, list = FALSE)\ntrain_data &lt;- rf_data[train_index, ]\ntest_data &lt;- rf_data[-train_index, ]\n\n# Recipe for feature preprocessing\nrecipe_spec &lt;- recipe(pitcher_outcome ~ ., data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors(), -all_outcomes()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;%  # Replace NAs in numeric columns with mean\n  step_impute_mode(all_nominal_predictors())      # Replace NAs in categorical columns with mode\n\n\n\n# Define models\nrf_model &lt;- rand_forest(\n  mtry = tune(),\n  trees = 500,\n  min_n = tune()\n) %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_model &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n# Create workflows\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(rf_model)\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(xgb_model)\n\n# Cross-validation\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Define tuning grids\nrf_grid &lt;- grid_random(\n  mtry(range = c(2, ncol(train_data) - 1)), \n  min_n(range = c(2, 10)), \n  size = 20\n)\n\nxgb_grid &lt;- grid_random(\n  trees(range = c(100, 1000)), \n  tree_depth(range = c(3, 10)), \n  learn_rate(range = c(0.01, 0.3)), \n  size = 20\n)\n\n# Tune models\nrf_tune &lt;- tune_grid(\n  rf_workflow, \n  resamples = cv_folds, \n  grid = rf_grid\n)\n\nxgb_tune &lt;- tune_grid(\n  xgb_workflow, \n  resamples = cv_folds, \n  grid = xgb_grid\n)\n\n\n\nCode# Collect tuning results\nrf_results &lt;- collect_metrics(rf_tune)\nxgb_results &lt;- collect_metrics(xgb_tune)\n\n# Combine the results for comparison\ntuning_results &lt;- bind_rows(\n  rf_results %&gt;% mutate(model = \"Random Forest\"),\n  xgb_results %&gt;% mutate(model = \"XGBoost\")\n)\n\n# Print the comparison of metrics\nprint(tuning_results)\n\n\n\nCode# Select best hyperparameters\nbest_rf &lt;- select_best(rf_tune, metric = \"accuracy\")\nbest_xgb &lt;- select_best(xgb_tune, metric = \"accuracy\")\n\n# Finalize models\nfinal_rf &lt;- finalize_workflow(rf_workflow, best_rf)\nfinal_xgb &lt;- finalize_workflow(xgb_workflow, best_xgb)\n\n# Fit final models\nrf_fit &lt;- fit(final_rf, data = train_data)\nxgb_fit &lt;- fit(final_xgb, data = train_data)\n\n# Evaluate on test data\nrf_preds &lt;- predict(rf_fit, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data %&gt;% select(pitcher_outcome))\n\nxgb_preds &lt;- predict(xgb_fit, test_data, type = \"class\") %&gt;%\n  bind_cols(test_data %&gt;% select(pitcher_outcome))\n\nshow_best(rf_tune, metric = \"accuracy\", n = 5)  # Show top 5 results\n\n# Variable Importance Plot (for Random Forest)\nvip::vip(rf_fit$fit$fit) + \n  geom_bar(stat = \"identity\", fill = \"#0073e6\") + \n  labs(\n    title = \"Variable Importance for Pitcher Success Outcome (RF)\",\n    x = \"Importance\", \n    y = \"Variables\"\n  ) +  \n  theme_minimal()\n\n\n\nCode# Extract the top 5 variables from the variable importance plot\nlibrary(randomForest)\nlibrary(pdp)\nlibrary(ALEPlot)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(dplyr)\n\n# Extract the model and preprocessed data\n# We need to ensure we're using the processed data that has the same structure as what the model was trained on\nextract_model_info &lt;- function(workflow_fit) {\n  # Extract the random forest model\n  model &lt;- workflow_fit$fit$fit\n  \n  # Get the recipe from the workflow\n  recipe_obj &lt;- workflow_fit$pre$actions$recipe$recipe\n  \n  # Apply the recipe to get processed training data\n  processed_data &lt;- prep(recipe_obj) %&gt;% \n    bake(new_data = NULL)\n  \n  return(list(model = model, processed_data = processed_data))\n}\n\n# Extract model and processed data\nmodel_info &lt;- extract_model_info(rf_fit)\nrf_model &lt;- model_info$model\nprocessed_train_data &lt;- model_info$processed_data\n\n# Get variable importance\nvar_imp &lt;- vip::vi(rf_model)\ntop_5_vars &lt;- var_imp %&gt;% \n  arrange(desc(Importance)) %&gt;% \n  slice_head(n = 5) %&gt;% \n  pull(Variable)\n\nprint(\"Top 5 Variables:\")\nprint(top_5_vars)\n\n# Function to create both PDP and ALE plots for a variable\ncreate_plots &lt;- function(var_name, model, processed_data) {\n  # For PDP\n  pdp_result &lt;- NULL\n  pdp_plot &lt;- NULL\n  \n  tryCatch({\n    pdp_result &lt;- partial(model, pred.var = var_name, \n                         train = processed_data,\n                         prob = TRUE, which.class = 2)\n    \n    pdp_plot &lt;- autoplot(pdp_result) +\n      theme_minimal() +\n      labs(title = paste(\"PDP for\", var_name),\n           y = \"Probability of Success\",\n           x = var_name)\n  }, error = function(e) {\n    message(\"Error in PDP: \", e$message)\n    pdp_plot &lt;- ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, \n               label = paste(\"Error in PDP plot:\", e$message)) +\n      theme_void() +\n      labs(title = paste(\"PDP Error for\", var_name))\n  })\n  \n  # For ALE\n  ale_plot &lt;- NULL\n  \n  tryCatch({\n    # Create a numeric predictor function for randomForest\n    pred_fun &lt;- function(X) {\n      preds &lt;- predict(model, X, type = \"prob\")\n      if(is.vector(preds)) {\n        return(preds)  # For binary classification returning a vector\n      } else {\n        return(preds[,2])  # For returning the second column (success probability)\n      }\n    }\n    \n    # Set up a temporary plotting device\n    ale_file &lt;- tempfile(fileext = \".png\")\n    png(ale_file, width = 800, height = 600)\n    \n    J_index &lt;- which(names(processed_data) == var_name)\n    \n    ALEPlot(processed_data[, names(processed_data) != \"pitcher_outcome\"], \n            model, \n            J = J_index,\n            pred.fun = pred_fun)\n    title(paste(\"ALE for\", var_name))\n    \n    dev.off()\n    \n    # Read back the ALE plot\n    if(file.exists(ale_file)) {\n      img &lt;- tryCatch({\n        jpeg::readJPEG(ale_file)\n      }, error = function(e) {\n        message(\"Error reading ALE plot file: \", e$message)\n        NULL\n      })\n      \n      if(!is.null(img)) {\n        ale_plot &lt;- ggplot() + \n          annotation_custom(grid::rasterGrob(img), \n                          xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf)\n      }\n    }\n  }, error = function(e) {\n    message(\"Error in ALE: \", e$message)\n    ale_plot &lt;- ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, \n               label = paste(\"Error in ALE plot:\", e$message)) +\n      theme_void() +\n      labs(title = paste(\"ALE Error for\", var_name))\n  })\n  \n  # If either plot failed, provide a fallback\n  if(is.null(pdp_plot)) {\n    pdp_plot &lt;- ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, \n               label = \"Failed to generate PDP plot\") +\n      theme_void()\n  }\n  \n  if(is.null(ale_plot)) {\n    ale_plot &lt;- ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, \n               label = \"Failed to generate ALE plot\") +\n      theme_void()\n  }\n  \n  # Combine plots\n  combined_plot &lt;- gridExtra::grid.arrange(pdp_plot, ale_plot, ncol = 2)\n  \n  return(combined_plot)\n}\n\n# Alternative approach using vip package for partial dependence plots\ncreate_plots_vip &lt;- function(var_name, model, processed_data) {\n  pdp_plot &lt;- NULL\n  \n  tryCatch({\n    pdp_plot &lt;- vip::partial(model, pred.var = var_name, \n                           train = processed_data,\n                           prob = TRUE, which.class = 2,\n                           plot = TRUE, plot.engine = \"ggplot2\") +\n      theme_minimal() +\n      labs(title = paste(\"PDP for\", var_name),\n           y = \"Probability of Success\",\n           x = var_name)\n  }, error = function(e) {\n    message(\"Error in VIP partial: \", e$message)\n    pdp_plot &lt;- ggplot() + \n      annotate(\"text\", x = 0.5, y = 0.5, \n               label = paste(\"Error in PDP plot:\", e$message)) +\n      theme_void() +\n      labs(title = paste(\"PDP Error for\", var_name))\n  })\n  \n  return(pdp_plot)\n}\n\n# Try the plots one by one\nfor(var in top_5_vars) {\n  cat(\"\\nGenerating plots for:\", var, \"\\n\")\n  \n  # Try with the vip package first (simpler approach)\n  tryCatch({\n    pdp_plot &lt;- create_plots_vip(var, rf_model, processed_train_data)\n    print(pdp_plot)\n    cat(\"Successfully generated PDP plot for\", var, \"\\n\")\n  }, error = function(e) {\n    cat(\"Failed to generate PDP plot for\", var, \":\", e$message, \"\\n\")\n  })\n  \n  # Attempt the combined plots if needed\n  tryCatch({\n    plot &lt;- create_plots(var, rf_model, processed_train_data)\n    plots_list[[var]] &lt;- plot\n    print(plot)\n    cat(\"Successfully generated combined plots for\", var, \"\\n\")\n  }, error = function(e) {\n    cat(\"Failed to generate combined plots for\", var, \":\", e$message, \"\\n\")\n  })\n}\n\n# Create a simple variable importance plot\nimportance_plot &lt;- var_imp %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"#0073e6\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top 10 Variables by Importance\",\n    x = \"Variables\",\n    y = \"Importance\"\n  )\n\nprint(importance_plot)\n\n# Summary table of the top variables\nsummary_table &lt;- var_imp %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%  # Show top 10 for context\n  mutate(\n    Importance = round(Importance, 4),\n    Percentage = round(Importance / sum(var_imp$Importance) * 100, 2)\n  ) %&gt;%\n  select(Variable, Importance, Percentage)\n\nprint(\"Variable Importance Summary:\")\nprint(summary_table)\n\n\n\nCode# Load required libraries\nlibrary(tidymodels)\nlibrary(pdp)\nlibrary(vip)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# First, let's print out the top variable importance values\nvar_imp_df &lt;- vi(rf_fit)\nprint(\"Variable Importance:\")\nprint(var_imp_df %&gt;% arrange(desc(Importance)) %&gt;% head(10))\n\n# Get top 5 variables\ntop_5_vars &lt;- var_imp_df %&gt;% \n  arrange(desc(Importance)) %&gt;% \n  slice_head(n = 5) %&gt;% \n  pull(Variable)\n\nprint(\"Top 5 Variables:\")\nprint(top_5_vars)\n\n# Create a variable importance plot\nimportance_plot &lt;- var_imp_df %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"#0073e6\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top 10 Variables by Importance\",\n    x = \"Variables\",\n    y = \"Variable Importance\"\n  )\n\nprint(importance_plot)\n\n# Calculate summary statistics for variable importance\nsummary_table &lt;- var_imp_df %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(\n    Importance = round(Importance, 4),\n    Percentage = round(Importance / sum(var_imp_df$Importance) * 100, 2)\n  ) %&gt;%\n  select(Variable, Importance, Percentage)\n\nprint(\"Variable Importance Summary:\")\nprint(summary_table)\n\n# Extract the actual fitted model from the workflow\nextract_model &lt;- function(workflow_fit) {\n  # Get the underlying model\n  model &lt;- workflow_fit$fit$fit\n  return(model)\n}\n\n# Extract the preprocessed training data\nextract_processed_data &lt;- function(workflow_fit, original_data) {\n  # Get the recipe\n  recipe_obj &lt;- workflow_fit$pre$actions$recipe$recipe\n  \n  # Prepare and apply the recipe to the original data\n  prepped_recipe &lt;- prep(recipe_obj)\n  processed_data &lt;- bake(prepped_recipe, new_data = original_data)\n  return(processed_data)\n}\n\n# Extract the model and processed data\nrf_model &lt;- extract_model(rf_fit)\nprocessed_train &lt;- extract_processed_data(rf_fit, train_data)\n\n# Function to create PDP plots using the pdp package\ncreate_pdp_plots &lt;- function(model, var_name, data) {\n  cat(\"\\nGenerating PDP plot for:\", var_name, \"\\n\")\n  \n  # Create a prediction function for the model\n  pred_func &lt;- function(object, newdata) {\n    # For RF classification, we want probability of \"success\"\n    # This adapts based on the actual model type\n    preds &lt;- predict(object, newdata, type = \"prob\")\n    if (is.data.frame(preds) || is.matrix(preds)) {\n      return(preds[, \"success\"])  # Return probability of success\n    } else {\n      return(preds)  # Return as is\n    }\n  }\n  \n  tryCatch({\n    # Check if variable exists in the data\n    if (var_name %in% names(data)) {\n      # Create partial dependence data\n      pdp_result &lt;- pdp::partial(\n        object = model,\n        pred.var = var_name,\n        pred.fun = pred_func,\n        train = data,\n        grid.resolution = 50  # Higher resolution for smoother curves\n      )\n      \n      # Create PDP plot\n      p &lt;- ggplot(pdp_result, aes_string(x = var_name, y = \"yhat\")) +\n        geom_smooth(method = \"loess\", color = \"#0073e6\", size = 1) +  # Loess line instead of points\n        theme_minimal() +\n        labs(\n          title = paste(\"Partial Dependence Plot for\", var_name),\n          subtitle = \"Effect on probability of pitcher success\",\n          y = \"Predicted Probability\",\n          x = var_name\n        )\n      \n      print(p)\n      cat(\"Successfully created PDP plot for\", var_name, \"\\n\")\n      return(p)\n    } else {\n      cat(\"Variable\", var_name, \"not found in data. Available variables:\\n\")\n      cat(paste(names(data)[1:min(10, length(names(data)))], collapse = \", \"), \"...\\n\")\n      return(NULL)\n    }\n  }, error = function(e) {\n    cat(\"Error creating PDP plot for\", var_name, \":\", e$message, \"\\n\")\n    return(NULL)\n  })\n}\n\n# Create PDP plots for each top variable\npdp_plots &lt;- list()\nfor (var in top_5_vars) {\n  plot &lt;- create_pdp_plots(rf_model, var, processed_train)\n  if (!is.null(plot)) {\n    pdp_plots[[var]] &lt;- plot\n  }\n}\n\n\nprint(\"check\")\nprint(pdp_plots)\n\n# Display summary of successful plots\ncat(\"\\nSuccessfully created\", length(pdp_plots), \"out of\", length(top_5_vars), \"PDP plots\\n\")\n\n# Interpretation helper\ncat(\"\\nInterpretation of PDP Plots:\\n\")\ncat(\"1. The Y-axis shows the predicted probability of pitcher success\\n\")\ncat(\"2. The X-axis shows the value of the predictor variable\\n\")\ncat(\"3. The line shows how the predicted probability changes as the variable changes\\n\")\ncat(\"4. Steeper slopes indicate stronger influence on the prediction\\n\")\ncat(\"5. Flat regions suggest the variable has little effect in that range\\n\")\n\n\n\nCode# Load necessary libraries\nlibrary(randomForest)\nlibrary(pdp)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(vip)\n\n# Function to create line PDP plots for the top 5 variable importance predictors\ncreate_pdp_for_top_vars &lt;- function(rf_model, train_data) {\n  # Get variable importance from the random forest model\n  var_imp_df &lt;- vi(rf_model)\n  \n  # Get top 5 variables based on importance\n  top_5_vars &lt;- var_imp_df %&gt;%\n    arrange(desc(Importance)) %&gt;%\n    slice_head(n = 5) %&gt;%\n    pull(Variable)\n  \n  # Ensure the variables are numeric\n  numeric_vars &lt;- top_5_vars[sapply(train_data[top_5_vars], is.numeric)]\n  \n  # Create a list to store the PDP plots\n  pdp_plots &lt;- list()\n  \n  # Create PDP plots for each numeric variable\n  for (var in numeric_vars) {\n    cat(\"\\nGenerating PDP plot for:\", var, \"\\n\")\n    \n    # Create partial dependence data\n    pdp_result &lt;- partial(rf_model, pred.var = var, grid.resolution = 50)\n    \n    # Plot the PDP as a line plot\n    pdp_plot &lt;- ggplot(pdp_result, aes_string(x = var, y = \"yhat\")) +\n      geom_line(color = \"#0073e6\", size = 1) +\n      theme_minimal() +\n      ggtitle(paste(\"Partial Dependence Plot for\", var)) +\n      labs(x = var, y = \"Predicted Probability\")\n    \n    # Store the plot in the list\n    pdp_plots[[var]] &lt;- pdp_plot\n  }\n  \n  # Return the list of PDP plots\n  return(pdp_plots)\n}\n\n# Test on the iris dataset\nrf_fit &lt;- randomForest(Species ~ ., data = iris)\n\n# Call the function to create PDP plots for the top 5 variables\npdp_plots &lt;- create_pdp_for_top_vars(rf_fit, iris)\n\n# Display the PDP plots\nfor (plot in pdp_plots) {\n  print(plot)\n}\n\n\n\nCodelibrary(pdp)\nlibrary(ggplot2)\n\n# Fit a random forest model (for example)\nrf_fit &lt;- randomForest(Species ~ ., data = iris)\n\n# Create a PDP for a specific variable\npdp_result &lt;- partial(rf_fit, pred.var = \"Sepal.Length\", grid.resolution = 50)\n\n# Plot the PDP as a line plot\npdp_plot &lt;- ggplot(pdp_result, aes(x = Sepal.Length, y = yhat)) +\n  geom_line(color = \"#0073e6\", size = 1) +  # Line instead of points\n  theme_minimal() +\n  ggtitle(\"Partial Dependence Plot for Sepal.Length\") +\n  labs(x = \"Sepal Length\", y = \"Predicted Probability\")\n\n# Display the plot\nprint(pdp_plot)\n\n\n\nCode# Load required libraries\nlibrary(tidymodels)\nlibrary(pdp)\nlibrary(vip)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# First, let's print out the top variable importance values\nvar_imp_df &lt;- vi(rf_fit)\nprint(\"Variable Importance:\")\nprint(var_imp_df %&gt;% arrange(desc(Importance)) %&gt;% head(10))\n\n# Get top 5 variables\ntop_5_vars &lt;- var_imp_df %&gt;% \n  arrange(desc(Importance)) %&gt;% \n  slice_head(n = 5) %&gt;% \n  pull(Variable)\n\nprint(\"Top 5 Variables:\")\nprint(top_5_vars)\n\n# Create a variable importance plot\nimportance_plot &lt;- var_imp_df %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"#0073e6\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top 10 Variables by Importance\",\n    x = \"Variables\",\n    y = \"Variable Importance\"\n  )\n\nprint(importance_plot)\n\n# Calculate summary statistics for variable importance\nsummary_table &lt;- var_imp_df %&gt;%\n  arrange(desc(Importance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(\n    Importance = round(Importance, 4),\n    Percentage = round(Importance / sum(var_imp_df$Importance) * 100, 2)\n  ) %&gt;%\n  select(Variable, Importance, Percentage)\n\nprint(\"Variable Importance Summary:\")\nprint(summary_table)\n\n# Extract the fitted model and preprocessed data\nrf_model &lt;- extract_model(rf_fit)\nprocessed_train &lt;- extract_processed_data(rf_fit, train_data)\n\n# Create and plot PDP for each top variable\nfor (var in top_5_vars) {\n  # Generate Partial Dependence data\n  pdp_result &lt;- partial(rf_model, pred.var = var, train = processed_train, grid.resolution = 50)\n  \n  # Plot the PDP directly\n  p &lt;- plot(pdp_result) +\n    ggtitle(paste(\"Partial Dependence Plot for\", var)) +\n    theme_minimal()\n  \n  print(p)\n}\n\n# Display the summary of successful plots\ncat(\"\\nSuccessfully created PDP plots for the top 5 variables\\n\")\n\n# Interpretation helper\ncat(\"\\nInterpretation of PDP Plots:\\n\")\ncat(\"1. The Y-axis shows the predicted probability of pitcher success\\n\")\ncat(\"2. The X-axis shows the value of the predictor variable\\n\")\ncat(\"3. The line shows how the predicted probability changes as the variable changes\\n\")\ncat(\"4. Steeper slopes indicate stronger influence on the prediction\\n\")\ncat(\"5. Flat regions suggest the variable has little effect in that range\\n\")\n\n\n\nCode# Generate Accumulated Local Effects (ALE) Plots\nale_plots &lt;- list()\nfor (var in top_5_vars) {\n  ale_data &lt;- ALEPlot(as.matrix(train_data[, -1]), train_data$pitcher_outcome, \n                      pred.fun = function(X.model, newdata) predict(rf_fit, newdata, type = \"prob\")[,2], \n                      J = which(names(train_data) == var))\n  \n  ale_plot &lt;- ggplot(data.frame(x = ale_data$x.values, y = ale_data$f.values), aes(x, y)) +\n    geom_line(color = \"blue\") +\n    labs(title = paste(\"ALE for\", var), x = var, y = \"Effect\") +\n    theme_minimal()\n  \n  ale_plots[[var]] &lt;- ale_plot\n}\n\n# Display all ALE plots\nprint(ale_plots)\n\n\n\nCodestrikeouts_per_pitcher &lt;- data_baseball %&gt;%\n  filter(home_team == \"KC\" | away_team == \"KC\") %&gt;%\n  filter(events == \"strikeout\") %&gt;%\n  group_by(pitcher) %&gt;%\n  summarise(strikeouts = n()) %&gt;%\n  arrange(desc(strikeouts))\n\nstrikeouts_per_pitcher\n\n\n\nCode# Prepare the data with normalized metrics\npp_data &lt;- data_baseball %&gt;%\n  filter(home_team == \"KC\" | away_team == \"KC\") %&gt;%\n  mutate(\n    # Convert scaled values to numeric vectors\n    vx0_normalized = as.numeric(scale(vx0)),\n    vy0_normalized = as.numeric(scale(vy0)),\n    vz0_normalized = as.numeric(scale(vz0)),\n    ax_normalized = as.numeric(scale(ax)),\n    ay_normalized = as.numeric(scale(ay)),\n    az_normalized = as.numeric(scale(az))\n  ) %&gt;%\n  mutate(\n    combined_normalized_velocity = rowMeans(cbind(vx0_normalized, vy0_normalized, vz0_normalized), na.rm = TRUE),\n    combined_normalized_acceleration = rowMeans(cbind(ax_normalized, ay_normalized, az_normalized), na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    Pitcher = case_when(\n      pitcher == 666142 ~ \"Cole Ragans\",\n      pitcher == 607625 ~ \"Seth Lugo\",\n      pitcher == 663903 ~ \"Brady Singer\",\n      pitcher == 608379 ~ \"Michael Wacha\",\n      pitcher == 679525 ~ \"Alec Marsh\",\n      TRUE ~ as.character(pitcher)\n    )\n  ) %&gt;%\n  filter(Pitcher %in% c(\"Cole Ragans\", \"Seth Lugo\", \"Brady Singer\", \"Michael Wacha\", \"Alec Marsh\")) %&gt;%\n  mutate(\n    pitcher_outcome = case_when(\n      events %in% c(\"strikeout\", \"field_out\", \"double_play\", \"strikeout_double_play\", \n                   \"force_out\", \"fielders_choice_out\", \"field_error\", \n                   \"catcher_interf\", \"truncated_pa\") ~ \"success\", \n      events %in% c(\"single\", \"double\", \"triple\", \"home_run\", \"walk\", \"hit_by_pitch\", \n                   \"fielders_choice\", \"sac_bunt\", \"sac_fly\", \"sac_fly_double_play\") ~ \"fail\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  filter(!is.na(pitcher_outcome))\n\n# Calculate summary statistics\nsummary_stats &lt;- pp_data %&gt;%\n  group_by(Pitcher) %&gt;%\n  summarise(\n    Total_Pitches = n(),\n    Total_Success = sum(pitcher_outcome == \"success\"),\n    Total_Fail = sum(pitcher_outcome == \"fail\"),\n    Avg_Hit_Distance = round(mean(hit_distance_sc, na.rm = TRUE), 2),\n    Avg_Release_Speed = round(mean(release_speed, na.rm = TRUE), 2),\n    Avg_Effective_Speed = round(mean(effective_speed, na.rm = TRUE), 2),\n    Avg_Normalized_Accel = round(mean(combined_normalized_acceleration, na.rm = TRUE), 2),\n    Avg_Normalized_Velocity = round(mean(combined_normalized_velocity, na.rm = TRUE), 2),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(Success_Percentage = round((Total_Success / Total_Pitches) * 100, 2)) %&gt;%\n  arrange(desc(Success_Percentage))\n\n# Display the table\nsummary_stats %&gt;%\n  kable(format = \"latex\", booktabs = TRUE, caption = \"Pitcher Performance Summary\") %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"hold_position\", \"scale_down\")) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#007bff\") %&gt;%\n  add_header_above(c(\" \" = 1, \"Basic Stats\" = 2, \"Performance Metrics\" = 5, \"Normalized Metrics\" = 2)) %&gt;%\n  scroll_box(width = \"100%\", height = \"300px\")\n\nsummary_stats\n\n\n\nCodeggplot(kc_data, aes(x = pfx_x, y = pfx_z, color = pitcher_outcome)) +\n  geom_point(alpha = 0.4) +\n  facet_wrap(~pitch_name) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"fail\" = \"#FF5252\", \"success\" = \"#448AFF\")) +\n  labs(\n    title = \"Horizontal vs. Vertical Movement by Pitch Type\",\n    x = \"Horizontal Movement (pfx_x, inches)\",\n    y = \"Vertical Movement (pfx_z, inches)\",\n    color = \"Outcome\"\n  ) +\n  theme_minimal()\n\n\n\nCodefastball_data &lt;- kc_data %&gt;%\n  filter(pitch_name == \"4-Seam Fastball\", \n         !is.na(spin_axis),\n         !is.na(pitcher_outcome)) %&gt;%\n  mutate(\n    spin_axis = as.numeric(spin_axis),\n    pitcher_outcome = factor(pitcher_outcome, \n                           levels = c(\"success\", \"fail\"),\n                           labels = c(\"Out Recorded\", \"Hit Allowed\"))\n  )\n\nggplot(fastball_data, aes(x = spin_axis, fill = pitcher_outcome)) +\n  geom_histogram(\n    binwidth = 15,\n    boundary = 0,\n    color = \"white\",\n    linewidth = 0.3,\n    position = \"stack\",\n    alpha = 0.85\n  ) +\n  coord_polar(start = -pi/2, direction = -1) +\n  scale_x_continuous(\n    limits = c(0, 360),\n    breaks = seq(0, 315, by = 45),\n    labels = c(\"0¬∞\", \"45¬∞\", \"90¬∞\", \"135¬∞\", \"180¬∞\", \"225¬∞\", \"270¬∞\", \"315¬∞\")\n  ) +\n  scale_fill_manual(\n    values = c(\"Out Recorded\" = \"#00A087\", \"Hit Allowed\" = \"#E64B35\"),\n    guide = guide_legend(title = \"Pitch Outcome\")\n  ) +\n  labs(\n    title = \"4-Seam Fastball Spin Axis Distribution\",\n    subtitle = \"Successful outs vs. hits allowed | 0¬∞ = Topspin, 180¬∞ = Pure Backspin\",\n    x = \"\",\n    y = \"\"\n  ) +\n  theme_minimal()\n\n\n\nCodeggplot(kc_data, aes(x = release_speed, y = effective_speed, color = pitcher_outcome)) +\n  geom_point(alpha = 0.4) +\n  facet_wrap(~pitch_name) +\n  scale_color_manual(values = c(\"fail\" = \"#FF5252\", \"success\" = \"#448AFF\")) +\n  labs(\n    title = \"Release Speed vs. Effective Speed by Pitch Type\",\n    x = \"Release Speed (mph)\",\n    y = \"Effective Speed (mph)\",\n    color = \"Outcome\"\n  ) +\n  theme_minimal()"
  }
]