[
  {
    "objectID": "aboutme_index.html",
    "href": "aboutme_index.html",
    "title": "Nathan Bresette",
    "section": "",
    "text": "I am a senior at Truman State University, majoring in Statistics with a concentration in Data Science and a minor in Mathematics. Passionate about uncovering insights from data, I am dedicated to leveraging statistical techniques and data analysis tools to solve real-world problems. With a strong foundation in mathematics and a keen interest in data science, I am enthusiastic about contributing to projects that drive innovation and make a positive impact.\n \nIn my portfolio, you’ll find a diverse range of projects, including analyses, visualizations, and applications that showcase my skills and experiences. From developing R packages to conducting consulting projects and participating in DataFest competitions, I’ve gained practical knowledge and hands-on experience in various aspects of data science. I invite you to explore my portfolio to learn more about my work and how I can contribute to your projects and initiatives."
  },
  {
    "objectID": "datafest.html",
    "href": "datafest.html",
    "title": "DataFest",
    "section": "",
    "text": "DataFest is a competition of data in which teams of undergraduates work “around the clock” to discover and share meaning in a large, rich, and complex data set. It is a nationally coordinated weekend-long data analysis competition and challenges students to find their own story to tell with the data that is meaningful to the data donor.\nAt Truman, we had a group of four that worked on several smaller projects before the actual competition. The week before we did a practice DataFest where our teacher gave us an unclean dataset with millions of rows on car crashes across the United States. We worked from 9 am to 5 pm and our completed results are in Traffic Impact. The actual datafest competition project is CourseKata. Due to 4 people writing code, it is not in the cleanest format. I would recommend looking at the slideshow for our finished project\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraffic Impact\n\n\n\n\n\n\nData Cleaning\n\n\nCART\n\n\nR\n\n\nPresentation\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\n\n\n\n\n\n\n\nCourseKata\n\n\n\n\n\n\nData Cleaning\n\n\nXGBoost\n\n\nPresentation\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\nNathan Bresette, Dane Winterboer, Evan AuBuchon, Severin Hussey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datafest_file/Traffic Impact.html",
    "href": "datafest_file/Traffic Impact.html",
    "title": "Traffic Impact",
    "section": "",
    "text": "Analysis on Missouri crashes from 2021 to 2023 with a two-stage predictive model—comprising a decision tree and a CART regression tree was developed."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#overview-of-project",
    "href": "datafest_file/Traffic Impact.html#overview-of-project",
    "title": "Traffic Impact",
    "section": "Overview of Project",
    "text": "Overview of Project\nThe traffic dataset contains 7.7 million cases of crashes across the United States, from 2016 to 2024, as well as the locations of the crash, weather conditions, features of surrounding road, and the severity of the impact of the crash on traffic conditions. For purposes of our analysis, we focused on crashes in Missouri during the years 2021, 2022, and 2023. To enhance analysis, our group engineered new features within the dataset, highlighted by a new feature we called Traffic Impact. This variable was based on how long traffic was inhibited, as well as the total distance of road over which traffic was impacted. Our other main enhancement was refactoring and separating the weather categories into two separate features: one for the type of weather, and another for the severity of the weather.\nExploratory analysis of the new feature Traffic Impact to the weather conditions during which crashes occurred, we found that crashes that occurred during conditions involving snow and ice had a significantly higher Traffic Impact score than crashes in any other conditions. Additionally, when controlling for conditions with snow and ice, we found that temperature had no significant effect on the Traffic Impact Score.\nTo predict the severity of traffic impact, we utilized a two-stage model: one which categorizes if the crash has an impact, and another that predicts its Traffic Impact score. Models were trained on crashes from 2021 and tested on the crashes from 2022. The first stage model utilized is a categorical decision tree which resulted in an accuracy of 81.51%, a sensitivity of 91.41%, and specificity of 21.92%. The second stage model is a CART regression tree that resulted in a RMSEtrain of 1.32 and RMSEtest of 1.578. The second stage model predicted values had a correlation of 0.468 with the actual values."
  },
  {
    "objectID": "datafest_file/Traffic Impact.html#slideshow",
    "href": "datafest_file/Traffic Impact.html#slideshow",
    "title": "Traffic Impact",
    "section": "Slideshow",
    "text": "Slideshow"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html",
    "href": "posts/Text Analysis/text_analysis.html",
    "title": "Text Classification & Sentiment Analysis",
    "section": "",
    "text": "Created text classification and sentiment analysis model to automate the process of classifying free response questions in a category then whether they were positive, neutral, negative, or a question."
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#abstract",
    "href": "posts/Text Analysis/text_analysis.html#abstract",
    "title": "Text Classification & Sentiment Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project was done during my intership at Lifeline Pregnancy. During the school year, they go to schools across Missouri and give a ‘Pure Freedom’ talk to the students. At the end of the talk, the students fill out a survey which has a free response question. The free response question is categorized into one of the four categories: Educator, Program, Content, or Other. The sentiment is then calculated as positive, negative, neutrall, or question. Before this model was created, all classification and sentiment was performed individually for every free response. Because the categories and sentiment are different than any pre-existing model, I had to make my own. To preserve the data and information of the clinic, some information will not be available"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#data-cleaning",
    "href": "posts/Text Analysis/text_analysis.html#data-cleaning",
    "title": "Text Classification & Sentiment Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data cleaning process involves anticipating and correcting student typos, standardizing all words to lowercase, and eliminating pluralizations, with the function addressing some of these tasks.\n\nCodelibrary(tidyverse)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(caret)\n\ncom &lt;- read.csv(\"~/Desktop/comments - Sheet1.csv\", header=FALSE)\n\n#Different data set\n# com &lt;- read.csv(\"~/Desktop/PF Comment Coding  - Fall 2022-Spring 2023.csv\")\n# com &lt;- com %&gt;%\n#   rename(comments = Comment, labels = Aspect, sentiments = Quality)\n\n# Load necessary libraries\nlibrary(dplyr)\n\n# Filter and preprocess the data\npfcomments &lt;- com %&gt;%\n  filter(V2 != \"\") %&gt;%\n  rename(comments = V1, labels = V2, sentiments = V6) %&gt;% \n  mutate(comments = ifelse(comments == \"jakson\", \"Jackson\", comments)) %&gt;% \n  mutate(comments = ifelse(comments == \"under stode\", \"understood\", comments)) %&gt;% \n  mutate(comments = ifelse(comments == \"relation ships\", \"relationships\", comments)) %&gt;%\n  mutate(words_count = str_count(comments, \"\\\\S+\"))\n\n\nThe function is now created and preprocesses text data by converting it to lowercase, removing possessive apostrophes (’s), and then categorizes the comments based on the presence of certain keywords. If a comment contains only a smiley face “:)”, it is labeled as “Other”. Comments containing the word “sex” are labeled as “Remove”. Additionally, comments containing words like “like”, “liked”, “love”, or “loved” are also labeled as “Remove”. For other comments, it calculates the frequency of specific words related to educational programs, educators, and content, then assigns a label (“Educator”, “Program”, “Content”, or “Other”) based on the word frequencies and the length of the comment. The words that were removed are often split between categories so an individual still reviews them.\nSpecific related words will not be shown for educator, program, or content to preserve some information\n\nCode# Define preprocess_and_extract_features function\npreprocess_and_extract_features &lt;- function(comment, label, words_count) {\n  # Convert text to lowercase\n  comment &lt;- tolower(comment)\n  comment &lt;- gsub(\"'s\", \"\", comment) \n\nprogram_count &lt;- sum(sapply(program_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\neducator_count &lt;- sum(sapply(educator_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\ncontent_count &lt;- sum(sapply(content_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n  \n  # If comment contains only \":)\", label as \"Other\"\n  if (grepl(\"^:\\\\)$\", comment)) {\n    pred_label &lt;- \"Other\"\n  }\n  # If comment contains \"sex\", label as \"Remove\"\n  else if (grepl(\"\\\\bsex\\\\b\", comment, ignore.case = TRUE)) {\n        pred_label &lt;- \"Remove\"\n  } \n  else if(grepl(\"\\\\blike\\\\b|\\\\bliked\\\\b|\\\\blove\\\\b|\\\\bloved\\\\b\", comment, ignore.case = TRUE))\n  {\n    pred_label &lt;- \"Remove\"\n  }\n  else {\n    program_count &lt;- sum(sapply(program_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n    educator_count &lt;- sum(sapply(educator_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n    content_count &lt;- sum(sapply(content_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n  \n    # If content_count is greater than or equal to 1, label as content\n    if (content_count &gt;= 1) {\n      pred_label &lt;- \"Content\"\n    } else {\n      # If educator_count is greater than program_count, label as educator\n      if (educator_count &gt; program_count) {\n        pred_label &lt;- \"Educator\"\n      } \n      # If program_count is greater than educator_count, label as program\n      else if (program_count &gt; educator_count) {\n        pred_label &lt;- \"Program\"\n      } \n      else if (program_count &gt;= 1 & educator_count &gt;= 1 & program_count == educator_count) {\n        pred_label &lt;- \"Remove\"\n      }\n      else if(words_count &lt; 35) {\n        pred_label &lt;- \"Remove\"\n      }\n      # If content_count is 0, educator_count or program_count is 1, do not label as other\n      else {\n        pred_label &lt;- \"Other\"\n      }\n    }\n  }\n  \n  return(data.frame(comments = comment, pred_label = pred_label, program_count, educator_count, content_count, labels = label))\n}\n\n\nConfusion Matrix\nThe function is now called and we split the data into correct and incorrect data sets. This allows us to see what content words did well and which ones did not. Additionally, a confusion matrix is used to show model accuracy where we specifically look at sensitivity and specificity\n\nCodeprocessed_data &lt;- Map(preprocess_and_extract_features, pfcomments$comments, pfcomments$labels, pfcomments$words_count) %&gt;%\n  bind_rows()\n\nprocessed &lt;-processed_data %&gt;% \n  filter(pred_label != \"Remove\")\n\nbad_processed_data &lt;- processed %&gt;% \n  filter(pred_label != labels) \n\n\nconfusionMatrix(factor(processed$labels), factor(processed$pred_label))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Content Educator Other Program\n  Content       55        2     0       8\n  Educator       4      410     0      29\n  Other          5        7    12       5\n  Program        8       27     1     301\n\nOverall Statistics\n                                          \n               Accuracy : 0.8902          \n                 95% CI : (0.8675, 0.9101)\n    No Information Rate : 0.5103          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8117          \n                                          \n Mcnemar's Test P-Value : 0.01733         \n\nStatistics by Class:\n\n                     Class: Content Class: Educator Class: Other Class: Program\nSensitivity                 0.76389          0.9193      0.92308         0.8776\nSpecificity                 0.98753          0.9229      0.98026         0.9322\nPos Pred Value              0.84615          0.9255      0.41379         0.8932\nNeg Pred Value              0.97899          0.9165      0.99882         0.9218\nPrevalence                  0.08238          0.5103      0.01487         0.3924\nDetection Rate              0.06293          0.4691      0.01373         0.3444\nDetection Prevalence        0.07437          0.5069      0.03318         0.3856\nBalanced Accuracy           0.87571          0.9211      0.95167         0.9049\n\n\nJustification for removing words over 35 based on the distributions of our correct and incorrect data sets. The means are much different too\n\nCodewordcount &lt;- pfcomments %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text, token = \"regex\") %&gt;% \n  group_by(comments) %&gt;%\n  summarise(words_count = n())  # Count words per comment\n\n#View(wordcloud)\n\nbad_wordcount &lt;- bad_processed_data %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text, token = \"regex\") %&gt;% \n  group_by(comments) %&gt;%\n  summarise(words_count = n())  # Count words per comment\n\nggplot(wordcount, aes(x = words_count)) +\n  geom_bar()\n\n\n\n\n\n\nCodeggplot(bad_wordcount, aes(x = words_count)) +\n  geom_bar()\n\n\n\n\n\n\nCodemean(wordcount$words_count)\n\n[1] 10.27065\n\nCodemean(bad_wordcount$words_count)\n\n[1] 12.5"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#most-common-words",
    "href": "posts/Text Analysis/text_analysis.html#most-common-words",
    "title": "Text Classification & Sentiment Analysis",
    "section": "Most Common Words",
    "text": "Most Common Words\n\nCodereplace_reg &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\"\nunnest_reg &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\ntidycom &lt;- pfcomments %&gt;% \n  mutate(text = str_replace_all(comments, replace_reg, \"\")) %&gt;%\n  unnest_tokens(word, comments, token = \"regex\", pattern = unnest_reg) %&gt;% anti_join(stop_words)\n\n# tidycom %&gt;% count(word, sort = TRUE)\n\ntidycom %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &lt; 120) %&gt;%\n  filter(n &gt; 50) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#sentiment-analysis",
    "href": "posts/Text Analysis/text_analysis.html#sentiment-analysis",
    "title": "Text Classification & Sentiment Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nCode# Apply sentiment analysis using the AFINN lexicon\nsentiment_scores &lt;- pfcomments %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  \n  left_join(get_sentiments(\"afinn\"), by = \"word\") %&gt;%  # Use left_join to preserve all rows in pfcomments\n  group_by(comments) %&gt;%\n  summarise(sentiment_score = sum(value, na.rm = TRUE)) %&gt;%  # Handle NA values in sentiment scores\n  ungroup()\n\n# Classify sentiments\nsentiment_scores &lt;- sentiment_scores %&gt;%\n  mutate(sentiment = case_when(\n    sentiment_score &gt; 0 ~ \"Positive\",\n    sentiment_score &lt; 0 ~ \"Negative\",\n    TRUE ~ \"Neutral\"\n  ))\n\nsentiment_scores %&gt;%\n  ggplot(aes(sentiment, fill = sentiment)) +\n  geom_bar() +\n  labs(title = \"Sentiment Analysis of Comments with Question Classification\")\n\n\n\n\n\n\n\nBar Chart of Sentiments\n\nCodepfcomments %&gt;%\n  filter(sentiments != \"Question\") %&gt;%  # Exclude 'question' sentiments\n  ggplot(aes(sentiments, fill = sentiments)) +\n  geom_bar() +\n  labs(title = \"Sentiment Analysis of Comments\")\n\n\n\n\n\n\n\n\nCodelibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(caret) # For confusionMatrix\n\n# Add an identifier to each comment\npfcomments &lt;- pfcomments %&gt;%\n  mutate(comment_id = row_number())\n\n# Apply sentiment analysis using the AFINN lexicon\nsentiment_scores &lt;- pfcomments %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  left_join(get_sentiments(\"afinn\"), by = \"word\") %&gt;%\n  group_by(comment_id, comments) %&gt;%\n  summarise(sentiment_score = sum(value, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Classify sentiments\nsentiment_scores &lt;- sentiment_scores %&gt;%\n  mutate(sentiment = case_when(\n    sentiment_score &gt; 0 ~ \"Positive\",\n    sentiment_score &lt; 0 ~ \"Negative\",\n    TRUE ~ \"Neutral\"\n  ))\n\n\n# Merge sentiment scores back with original comments\npfcomments &lt;- pfcomments %&gt;%\n  left_join(sentiment_scores %&gt;% select(comment_id, sentiment_score, sentiment), by = \"comment_id\")\n\n# Plot sentiment distribution\npfcomments %&gt;%\n  filter(sentiments != \"Question\") %&gt;%\n  ggplot(aes(sentiments, fill = sentiments)) +\n  geom_bar() +\n  labs(title = \"Sentiment Analysis of Comments\")\n\n\n\n\n\n\nCode# Ensure same levels for confusion matrix\npfcomments &lt;- pfcomments %&gt;%\n  mutate(sentiments = factor(sentiments, levels = c(\"Positive\", \"Negative\", \"Neutral\")),\n         sentiment = factor(sentiment, levels = c(\"Positive\", \"Negative\", \"Neutral\")))\n\n# Generate confusion matrix\nconfusionMatrix(pfcomments$sentiment, pfcomments$sentiments)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Positive Negative Neutral\n  Positive      993       22      29\n  Negative       23       14      24\n  Neutral       184       12      76\n\nOverall Statistics\n                                          \n               Accuracy : 0.7865          \n                 95% CI : (0.7639, 0.8079)\n    No Information Rate : 0.8715          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.3312          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n\nStatistics by Class:\n\n                     Class: Positive Class: Negative Class: Neutral\nSensitivity                   0.8275         0.29167        0.58915\nSpecificity                   0.7119         0.96464        0.84295\nPos Pred Value                0.9511         0.22951        0.27941\nNeg Pred Value                0.3784         0.97416        0.95204\nPrevalence                    0.8715         0.03486        0.09368\nDetection Rate                0.7211         0.01017        0.05519\nDetection Prevalence          0.7582         0.04430        0.19753\nBalanced Accuracy             0.7697         0.62815        0.71605"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#wordcloud",
    "href": "posts/Text Analysis/text_analysis.html#wordcloud",
    "title": "Text Classification & Sentiment Analysis",
    "section": "Wordcloud",
    "text": "Wordcloud\nA nice visualization for the company to use\n\nCode#install.packages(\"wordcloud\")\nlibrary(wordcloud)\n\n#install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n#install.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n#install.packages(\"tm\")\nlibrary(tm)\n#Create a vector containing only the text\ntext &lt;- pfcomments$comments\n# Create a corpus  \ndocs &lt;- Corpus(VectorSource(text))\n\ndtm &lt;- TermDocumentMatrix(docs) \nmatrix &lt;- as.matrix(dtm) \nwords &lt;- sort(rowSums(matrix),decreasing=TRUE) \ndf &lt;- data.frame(word = names(words),freq=words)\n\n\n\nCode#fc5f1b1b-2aeb-4e09-93fc-06fdac0d8030\n# Making DF for word clouds\n\n# Pre word cloud\ncorpus = Corpus(VectorSource(pfcomments$comments))\n\ncorpus &lt;- corpus %&gt;% \n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace) %&gt;%\n  tm_map(content_transformer(tolower)) %&gt;%\n  tm_map(removeWords, stopwords(\"english\")) %&gt;%\n  tm_map(removeWords, stopwords(\"SMART\"))\n\ntdm = TermDocumentMatrix(corpus) %&gt;% \n  as.matrix()\n\nwords = sort(rowSums(tdm), decreasing = TRUE)\n\npre_WCdf = data.frame(words = names(words), freq = words)\n\n\n# Color Palettes\npre_WCcolors = c(\"#8bc13f\", \"#396430\", \"#6e6e6e\")\npre_WCbkgd = \"#FFFFFF\"\npost_WCcolors = c(\"#FFFFFF\", \"#510C76\", \"#87714D\")\npost_WCbkgd = \"#00A8E2\"\n\n#rm unneeded vars\nrm(corpus, tdm, words)\n\nWC_Pre &lt;- wordcloud2(pre_WCdf,\n           color = rep_len(pre_WCcolors, nrow(pre_WCdf)),\n           backgroundColor = pre_WCbkgd,\n           fontFamily = \"AppleMyungjo\",\n           size = .62,\n           rotateRatio = 0)\n\nWC_Pre"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html",
    "href": "posts/InquisitR/InquisitR.html",
    "title": "InquisitR",
    "section": "",
    "text": "Designed to simplify the initial phases of data exploration while enhancing the visual appeal of graphs. Currently, it offers three essential functions listed below. I am also actively developing a fourth to streamline data type conversions."
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#boxplotr",
    "href": "posts/InquisitR/InquisitR.html#boxplotr",
    "title": "InquisitR",
    "section": "boxplotR",
    "text": "boxplotR\nThe function generates detailed boxplots for various combinations of factor and numeric variables within your dataset\n\nCodelibrary(ggplot2)\n\nboxplotR &lt;- function(data) {\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric and one factor variable.\")\n  }\n\n  factor_vars &lt;- names(data)[sapply(data, is.factor)]  # Get factor variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]  # Get numeric variable names\n\n  plots &lt;- list()\n\n  # Loop through each combination of factor and numeric variable\n  for (x in factor_vars) {\n    for (y in numeric_vars) {\n      plot_title &lt;- paste(\"Boxplot of\", y, \"by\", x)\n      plot &lt;- ggplot(data, aes_string(x = x, y = y)) +\n        geom_boxplot(fill = \"skyblue\", color = \"black\") +\n        labs(title = plot_title) +\n        theme_minimal()\n      plots[[paste(x, y, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\nboxplotR(iris)\n\n$Species_Sepal.Length\n\n\n\n\n\n\n\n\n\n$Species_Sepal.Width\n\n\n\n\n\n\n\n\n\n$Species_Petal.Length\n\n\n\n\n\n\n\n\n\n$Species_Petal.Width"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#correlationr",
    "href": "posts/InquisitR/InquisitR.html#correlationr",
    "title": "InquisitR",
    "section": "correlationR",
    "text": "correlationR\nThe function creates customizable pairwise plot matrices using GGally, allowing you to tailor plots for upper triangles, lower triangles, and diagonal views.\n\nCodelibrary(GGally)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nupperFn &lt;- function(data, mapping, ...) {\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n\n  cor_value &lt;- round(cor(x, y), 2)\n  cor_label &lt;- format(cor_value, nsmall = 2)\n\n  data.frame(x = 1, y = 1, cor = cor_value) %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_tile(aes(fill = cor), color = \"white\", width = 1, height = 1) +\n    geom_text(aes(label = cor_label), color = \"black\", size = 5, vjust = 0.5) +\n    scale_fill_gradient2(low = \"blue2\", high = \"red2\", mid = \"white\", midpoint = 0, limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n    theme_void() +\n    theme(legend.position = \"none\",\n          plot.margin = unit(c(0, 0, 0, 0), \"cm\")) +\n    theme_minimal()\n}\n\nlowerFn &lt;- function(data, mapping, method = \"lm\", ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(colour = \"skyblue3\") +\n    geom_smooth(method = method, color = \"black\", ...) +\n    theme_minimal()\n}\n\ndiagFn &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_density(aes(y = ..density..), colour = \"red\", fill = \"red\", alpha = 0.2) +\n    geom_histogram(aes(y = ..density..), colour = \"blue\", fill = \"skyblue3\", alpha = 0.5) +\n    theme_minimal()\n}\n\ncorrelationR &lt;- function(df) {\n  numeric_cols &lt;- df %&gt;%\n    dplyr::select(where(is.numeric)) %&gt;%\n    colnames()\n\n  if (length(numeric_cols) &lt; 2) {\n    stop(\"The dataframe must contain at least two numeric columns.\")\n  }\n\n  ggpairs(\n    df[, numeric_cols],\n    lower = list(continuous = wrap(lowerFn, method = \"lm\")),\n    diag = list(continuous = wrap(diagFn)),\n    upper = list(continuous = wrap(upperFn))\n  ) + theme_minimal()\n}\n\ndata(iris)\ncorrelationR(iris)"
  },
  {
    "objectID": "posts/InquisitR/InquisitR.html#distributionr",
    "href": "posts/InquisitR/InquisitR.html#distributionr",
    "title": "InquisitR",
    "section": "distributionR",
    "text": "distributionR\nThis function easily visualize distributions of both numeric and categorical variables, with flexible options to suit your analysis needs.\n\nCodeutils::globalVariables(c(\"Category\", \"Frequency\"))\n\n[1] \"Category\"  \"Frequency\"\n\nCodelibrary(ggplot2)\n\ndistributionR &lt;- function(data, plot_bars = TRUE, plot_histograms = TRUE) {\n  # Check if input is a data frame\n  if (!is.data.frame(data)) {\n    stop(\"Input must be a data frame.\")\n  }\n\n  # Check if input contains only one column\n  if (length(data) == 1 && is.vector(data)) {\n    stop(\"Input must be a data frame, not a single vector or column.\")\n  }\n\n  # Check if data frame contains at least one numeric or categorical variable\n  contains_valid_vars &lt;- any(sapply(data, function(x) is.numeric(x) || is.character(x) || is.factor(x)))\n  if (!contains_valid_vars) {\n    stop(\"Data frame must contain at least one numeric or categorical (character/factor) variable.\")\n  }\n\n  plots &lt;- list()\n\n  # Create bar plots for categorical variables\n  if (plot_bars) {\n    char_factor_cols &lt;- names(data)[sapply(data, function(x) is.character(x) || is.factor(x))]\n    for (col in char_factor_cols) {\n      freq_table &lt;- table(data[[col]])\n      plot_data &lt;- as.data.frame(freq_table)\n      names(plot_data) &lt;- c(\"Category\", \"Frequency\")\n      plot &lt;- ggplot(plot_data, aes(x = Category, y = Frequency)) +\n        geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Bar Plot of\", col), x = col, y = \"Frequency\") +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n        theme_minimal()\n      plots[[paste(\"barplot\", col, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  # Create histograms for numeric variables\n  if (plot_histograms) {\n    numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n    for (var in numeric_vars) {\n      plot &lt;- ggplot(data, aes(x = .data[[var]])) +\n        geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\") +\n        labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n        theme_minimal()\n      plots[[paste(\"histogram\", var, sep = \"_\")]] &lt;- plot\n    }\n  }\n\n  return(plots)\n}\n\ndata(iris)\ndistributionR(iris, plot_bars = TRUE, plot_histograms = TRUE)\n\n$barplot_Species\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Sepal.Width\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Length\n\n\n\n\n\n\n\n\n\n$histogram_Petal.Width\n\n\n\n\n\n\n\n\nGithub link to use my package https://lnkd.in/gR2h7gKB\nor just use: library(devtools) install_github(“NathanBresette/InquisitR”)"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "During my time at Truman, I’ve engaged in three rewarding consulting experiences through STAT 310 - Data Collection & Statistical Communication and STAT 392 - Center for Applied Statistics and Evaluation. These consulting endeavors involved extensive client interaction, survey design, data analysis, and the delivery of comprehensive presentations."
  },
  {
    "objectID": "consulting.html#international-admissions-office",
    "href": "consulting.html#international-admissions-office",
    "title": "Consulting",
    "section": "International Admissions Office",
    "text": "International Admissions Office\nSTAT 310 afforded me the opportunity to collaborate with two peers on a semester-long consulting project for the International Admissions Office. Our primary objective was to assist our client in understanding the factors driving international students to choose Truman. This involved delving into various aspects such as the admissions process, initial awareness of the university, decision-making factors, and current perceptions. Our findings were compiled into a 22-page report supplemented with insightful data visualizations, which we presented to our client in a concise and impactful 15-minute presentation."
  },
  {
    "objectID": "consulting.html#compliance-office",
    "href": "consulting.html#compliance-office",
    "title": "Consulting",
    "section": "Compliance Office",
    "text": "Compliance Office\nSTAT 392 is a semester long class where we managed multiple client projects simultaneously, ensured clear communication and meet all deadlines. The first client was for Truman State University’s Institutional Compliance Office. The objective of the survey was to evaluate the office’s effectiveness in meeting the needs of the university community and to investigate areas that could benefit from further development. This was done through creating a survey, analyzing the results, and compiling a report for the office. I joined this project after the survey had been created so my main task was analyzing the survey and communicate results. After, a paper was compiled which focused on the respondents based on whether they have or have not interacted with the office."
  },
  {
    "objectID": "consulting.html#voice-diagnostics-survey",
    "href": "consulting.html#voice-diagnostics-survey",
    "title": "Consulting",
    "section": "Voice Diagnostics Survey",
    "text": "Voice Diagnostics Survey\nAnother project in STAT 392 was creating a nationwide survey about the protocols used for instrumental assessment of voice. I had the lead in this project and my clients were a professor and a graduate student. Unlike the past surveys, I had no background in protocols used for instrumental assessment of voice. This was a new challenge for me to become familiar enough in the field to help my clients get the best results. Although this project is still on going, with the help of my clients, we created a nationwide survey after several meetings of tweaking the survey, adding sections, and getting the best ordering of questions. This survey will be sent out soon and will be analzed after"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html",
    "href": "posts/NCAA Basketball/basketball.html",
    "title": "NCAA Basketball Analysis",
    "section": "",
    "text": "Performed web scraping using Selenium and BeautifulSoup, followed by an in-depth analysis in that included Principal Component Analysis, XGBoost, and neural networks."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#abstract",
    "href": "posts/NCAA Basketball/basketball.html#abstract",
    "title": "NCAA Basketball Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project presents an analysis of college basketball team performance based on data from men’s NCAA Basketball. After merging the datasets from haslametrics and teamrankings along with data cleaning and feature engineering in R, the dataset consisted of 16 columns with 361 rows.\nExploratory data analysis includes correlation analysis, visualization of distributions, and principal component analysis (PCA) to address collinearity among variables. Although PCA had good insights, it was not utilized due to its limited account for variance.\nThe feature engineered variable, ‘Rank_Category’, classifies teams into three categories based on their ‘Rank’ column: Rank (0-25), Top 50% (excluding Rank), and Bottom 50%. Modeling efforts focused on predicting ‘Rank_Category’ using XGBoost with racing ANOVA tuning which resulted in an accuracy of 79.12% and an AUC of 0.918. Variable importance analysis showed key predictors including defensive efficiency, win rate, defensive field goal percentage, offensive 2-point percentage, and offensive turnovers. Additionally, a neural network model achieved a higher accuracy of 97.80%."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "href": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "title": "NCAA Basketball Analysis",
    "section": "Webscraping Data in Python",
    "text": "Webscraping Data in Python\nScraping the first website Halsametrics.com with selenium\n\nCodefrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\n\n\n# Set up the WebDriver with ChromeOptions\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('/usr/local/bin/chromedriver')  # Add the path to chromedriver executable\n\n# Initialize the WebDriver\ndriver = webdriver.Chrome(options=chrome_options)\n\n# Navigate to the webpage\ndriver.get('https://haslametrics.com/')\n\n# Wait for the page to load and for the 'Defense' button to be clickable\nwait = WebDriverWait(driver, 20)\ndefense_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cboRatings\"]/option[@value=\"Defense\"]')))\n\n# Click the 'Defense' button to load the defensive ratings\ndefense_button.click()\n\n# Wait for the table to load\nwait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"myTable\"]')))\n\n# Scrape the table\ntable = driver.find_element(By.XPATH, '//*[@id=\"myTable\"]')\nhasla = pd.read_html(table.get_attribute('outerHTML'))[0]\n\n# Flatten the MultiIndex columns\nhasla.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in hasla.columns.values]\n\n# Rename 'Unnamed: 1_level_0 Team' to 'Team'\nhasla.rename(columns={'Unnamed: 1_level_0 Team': 'Team'}, inplace=True)\n\n# Extracting win/loss information and creating new columns\nhasla['Win'] = hasla['Team'].str.extract(r'\\((\\d+)-\\d+\\)')\nhasla['Loss'] = hasla['Team'].str.extract(r'\\(\\d+-(\\d+)\\)')\n\n# Remove parentheses and numbers from 'Team' column\nhasla['Team'] = hasla['Team'].replace(regex={'\\([^)]*\\)': '', '\\d+': ''})\n\nhasla['Team'] = hasla['Team'].str.strip()\n\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nhasla.to_excel(f'{desktop_path}/findhasla.xlsx', index=False)\n\n# Close the browser\ndriver.quit()\n\n\nScraping the second website teamrankings.com with BeautifulSoup\n\nCodedef scrape_and_merge(urls, new_column_names):\n    dfs = []\n\n    for url in urls:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        table = soup.find('table')\n        df = pd.read_html(StringIO(str(table)))[0]\n\n        if url in new_column_names:\n            df.columns = new_column_names[url]\n\n        for col in df.columns:\n            if col not in ['Rank', 'Team']:\n                df[col] = pd.to_numeric(df[col].replace('%', '', regex=True), errors='coerce')\n\n        dfs.append(df)\n\n    # Merge all DataFrames dynamically\n    combined_df = dfs[0]\n    for i, df in enumerate(dfs[1:], start=2):\n        combined_df = pd.merge(combined_df, df, on='Team', how='outer', suffixes=('', f'_{i}'))\n\n    # Drop duplicate 'Team' columns\n    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n\n    return combined_df\n\n# Define the URLs\nurls = [\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate',\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game',\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession'\n]\n\n# Create a dictionary with new column names for certain URLs\nnew_column_names = {\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate': ['Rank', 'Team', 'FTR_2023', 'FTR_L3', 'FTR_L1', 'FTR_Home', 'FTR_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct': ['Rank', 'Team', 'ORB_2023', 'ORB_L3', 'ORB_L1', 'ORB_Home', 'ORB_Away', 'ORB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct': ['Rank', 'Team', 'DRB_2023', 'DRP_L3', 'DRB_L1', 'DRB_Home', 'DRB_Away', 'DRB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct': ['Rank', 'Team', 'opp3_2023', 'opp3_L3', 'opp3_L1', 'opp3_Home', 'opp3_Away', 'opp3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct': ['Rank', 'Team', 'p3_2023', 'p3_L3', 'p3_L1', 'p3_Home', 'p3_Away', 'p3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct': ['Rank', 'Team', 'o2p_2023', 'o2p_L3', 'op2_L1', 'op2_Home', 'op2_Away', 'op2 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct': ['Rank', 'Team', '2p_2023', '2p_L3', '2p_L1', '2p_Home', '2p_Away', '2p 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game': ['Rank', 'Team', 'Pace_2023', 'Pace_L3', 'Pace_L1', 'Pace_Home', 'Pace_Away', 'Pace 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession': ['Rank', 'Team', 'TO_2023', 'TO_L3', 'TO_L1', 'TO_Home', 'TO_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession': ['Rank', 'Team', 'oppTO_2023', 'oppTO_L3', 'oppTO_L1', 'oppTO_Home', 'oppTO_Away', 'oppTO 2022']\n}\n\n\nCombining the data frames and saving to desktop\n\nCode# Call the function to scrape and merge data\ncombined_df = scrape_and_merge(urls, new_column_names)\n\ncombined_df['Team'] = combined_df['Team'].replace({\n'Miami (OH)' : 'Miami'\n\n})\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\ncombined_df.to_excel(f'{desktop_path}/findme.xlsx', index=False)\n\n\n\nneutral_input = input(\"Is it a neutral site game (Yes/No): \")\n\n\n# Drop duplicate team names in hasla\nhasla = hasla.drop_duplicates(subset=['Team'])\n\n# Drop duplicate team names in combined_df\ncombined_df = combined_df.drop_duplicates(subset=['Team'])\n\n# Merge the DataFrames based on 'Team'\nmerged_df = pd.merge(hasla, combined_df, on='Team', how='inner')\n\n# Save the merged DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nmerged_df.to_excel(f'{desktop_path}/merged_data.xlsx', index=False)"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "href": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "title": "NCAA Basketball Analysis",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nAll further code performed in R:\nOnce the data has been combined into one data frame, there are over 100 variables. Using dplyr, 16 columns are selected, renamed for easier readability, mutated to correct variable type (numeric, factor, etc), and a new variable is feauture engineered to split the ranks into three categories of Ranked, Top 50%, and Bottom 50%.\n\nCodelibrary(readxl)\nlibrary(tidyverse)\nmerged_data &lt;- read_excel(\"~/Desktop/merged_data.xlsx\")\n\nclean_data &lt;- merged_data %&gt;%\n  select(`Unnamed: 0_level_0 Rk`, `Win`, `Loss`, `DEFENSIVE SUMMARY Eff`, `DEFENSIVE SUMMARY 3P%`, `DEFENSIVE SUMMARY FG%`, `DEFENSIVE SUMMARY MR%`, `DEFENSIVE SUMMARY NP%`, FTR_2023, TO_2023, ORB_2023, DRB_2023, p3_2023, `2p_2023`, Pace_2023, TO_2023) %&gt;%\n  rename(\n    Rank = `Unnamed: 0_level_0 Rk`,\n    `Def_Eff` = `DEFENSIVE SUMMARY Eff`,\n    `Def_3P` = `DEFENSIVE SUMMARY 3P%`,\n    `Def_FG` = `DEFENSIVE SUMMARY FG%`,\n    `Def_MR` = `DEFENSIVE SUMMARY MR%`,\n    `Def_NP` = `DEFENSIVE SUMMARY NP%`,\n    Off_FTR = FTR_2023,\n    Off_TO = TO_2023,\n    Off_ORB = ORB_2023,\n    Def_DRB = DRB_2023,\n    Off_3P = p3_2023,\n    Off_2P = `2p_2023`,\n    Pace = Pace_2023\n  ) %&gt;% \n  mutate(Win = as.numeric(Win),\n         Loss = as.numeric(Loss)) \n\nclean_data$Rank_Category &lt;- ifelse(clean_data$Rank &gt;= 0 & clean_data$Rank &lt;= 25, \"Ranked\",\n                                   ifelse(clean_data$Rank &gt; 25 & clean_data$Rank &lt;= 181, \"Top 50%\", \"Bottom 50%\"))\nclean_data &lt;- clean_data %&gt;%\n  mutate(Rank_Category = as.factor(Rank_Category))\n\n\nThe final data cleaning step is checking total NA values for each variable which there are none\n\nCodecbind(lapply(lapply(clean_data, is.na), sum))\n\n              [,1]\nRank          0   \nWin           0   \nLoss          0   \nDef_Eff       0   \nDef_3P        0   \nDef_FG        0   \nDef_MR        0   \nDef_NP        0   \nOff_FTR       0   \nOff_TO        0   \nOff_ORB       0   \nDef_DRB       0   \nOff_3P        0   \nOff_2P        0   \nPace          0   \nRank_Category 0"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "href": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "title": "NCAA Basketball Analysis",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCorrelation and Scatter Plots\nThe data exploration begins by looking at the correlation between variables. I created a function to make a correlation plot then if the correlation is above the absolute value of 0.6, it will plot the scatter plot of the two correlated variables. Due to the high correlation in this data, I have limited the output to only two of the scatterplots.\n\nCodelibrary(corrplot)\n\ncompute_and_plot_correlation &lt;- function(data, threshold = 0.6) {\n  # Select numeric columns\n  numeric_data &lt;- data[, sapply(data, is.numeric)]\n  \n  # Remove rows with missing values\n  numeric_data &lt;- numeric_data[complete.cases(numeric_data), ]\n  \n  # Compute correlation matrix\n  correlation_matrix &lt;- cor(numeric_data)\n  \n  # Find pairs of variables with correlation above or below the threshold\n  high_correlation_pairs &lt;- which(abs(correlation_matrix) &gt; threshold & upper.tri(correlation_matrix), arr.ind = TRUE)\n  \n  # Create scatter plots for high correlation pairs\n  plots &lt;- list()\n  for (i in 1:nrow(high_correlation_pairs)) {\n    var_x &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 1]]\n    var_y &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 2]]\n    \n    plot &lt;- ggplot(data = numeric_data, aes_string(x = var_x, y = var_y)) +\n      geom_point() +\n      labs(title = paste(\"Scatter Plot of\", var_y, \"vs\", var_x), x = var_x, y = var_y) + \n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    plots[[paste(var_x, var_y, sep = \"_\")]] &lt;- plot\n  }\n  \n  # Plot correlation matrix\n  corrplot(correlation_matrix, method = \"shade\", type = \"lower\", diag = FALSE, addCoef.col = \"black\", number.cex = 0.5)\n  \n  return(plots)\n}\n\n#Example call to function\nscatter_plots &lt;- compute_and_plot_correlation(clean_data)\n\n\n\n\n\n\nCodefor (i in seq_along(scatter_plots)) {\n  if (i &gt; 2) break\n  print(scatter_plots[[i]])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions - Histograms\nI also made a function to make histograms for all numeric variables to view the distributions. Because all of our variables are numeric, no bar charts were made to view the distribution of categorical variables.\n\nCodecreate_histograms_ggplot &lt;- function(data) {\n  # Get numeric variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n  \n  # Initialize an empty list to store ggplot objects\n  plots &lt;- list()\n  \n  # Loop through each numeric variable and create a histogram using ggplot\n  for (var in numeric_vars) {\n    # Create ggplot object for histogram\n    plot &lt;- ggplot(data, aes_string(x = var)) +\n      geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n      labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    # Append ggplot object to the list\n    plots[[var]] &lt;- plot\n  }\n  \n  return(plots)\n}\n\n# Example call to function\nhist_plots &lt;- create_histograms_ggplot(clean_data)\n\n\n  print(hist_plots[[2]])\n\n\n\n\n\n\nCode  print(hist_plots[[3]])"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "href": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "title": "NCAA Basketball Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nIn our data exploration, the correlation plot showed the high correlation between our variables. Because of this, a principal component analysis was performed to reduce collinearity. A scree plot was used to determine the number of components. Figure 5 shows the scree plot.\n\nCodelibrary(htmlwidgets)\nlibrary(plotly)\n\nX &lt;- subset(clean_data, select = -c(Rank_Category, Win, Loss))\n\nprin_comp &lt;- prcomp(X, center = TRUE, scale. = TRUE)\n\n\nScree Plot\nThe scree plot shows there should be around 3 components to account for the most variance while also reducing the dimensions.\n\nCodeplot(prin_comp, type = \"l\", main = \"Scree Plot\")\n\n\n\n\n\n\n\n3D PCA\nA 3d plot with the three axes of the plot representing the first three principal components (PC1, PC2, and PC3). It also clusters the variable Ranked_Category very accurately. Although it clusters Ranked_Category well, it only accounts for 61.67% of the variance so we will not use it.\n\nCodesumm &lt;- summary(prin_comp)\nsumm$importance[2,]\n\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8     PC9    PC10 \n0.41190 0.13812 0.08788 0.06669 0.05603 0.05414 0.04966 0.04583 0.03675 0.03225 \n   PC11    PC12    PC13 \n0.01271 0.00427 0.00378 \n\nCodecomponents &lt;- prin_comp[[\"x\"]]\ncomponents &lt;- data.frame(components)\ncomponents$PC2 &lt;- -components$PC2\ncomponents$PC3 &lt;- -components$PC3\ncomponents = cbind(components, clean_data$Rank_Category)\n\n# Combine components with Ranked labels\ncomponents &lt;- cbind(components, Rank_Category = clean_data$Rank_Category)\n\n# Create Plotly figure\nfig &lt;- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Rank_Category,\n               colors = c('#636EFA','#EF553B','#00CC96'), type = \"scatter3d\", mode = \"markers\",\n               marker = list(size = 4))\n\n\n# Customize layout\nfig &lt;- fig %&gt;% layout(\n  title = \"61.67% Variance Explained\",\n  scene = list(bgcolor = \"#e5ecf6\")\n)\n\n# Show the plot\nfig\n\n\n\n\nCodesaveWidget(fig, \"interactive_plot.html\")"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "href": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "title": "NCAA Basketball Analysis",
    "section": "XGBoost Classification Model",
    "text": "XGBoost Classification Model\nDue to the high colinearity between our variables, our model must be able to take it into account. This model will be for exploration use rather than predictive so that we can see what variables are important to be ranked higher at the end of the season.\n\nCode#libs\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(pROC)\nlibrary(data.table)\nlibrary(kableExtra)\n\n\nSplitting into Training/Testing\n\nCodeDATA &lt;- clean_data %&gt;% \n  select(-Rank)\n\nset.seed(123)\nDATA_SPLIT &lt;- DATA %&gt;%\n  initial_split(strata = Rank_Category)\n\nDATA_TRAIN &lt;- training(DATA_SPLIT)\nDATA_TEST &lt;- testing(DATA_SPLIT)\n\nset.seed(234)\nDATA_folds &lt;- vfold_cv(DATA_TRAIN, strata = Rank_Category)\nDATA_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [242/28]&gt; Fold01\n 2 &lt;split [242/28]&gt; Fold02\n 3 &lt;split [242/28]&gt; Fold03\n 4 &lt;split [242/28]&gt; Fold04\n 5 &lt;split [243/27]&gt; Fold05\n 6 &lt;split [243/27]&gt; Fold06\n 7 &lt;split [244/26]&gt; Fold07\n 8 &lt;split [244/26]&gt; Fold08\n 9 &lt;split [244/26]&gt; Fold09\n10 &lt;split [244/26]&gt; Fold10\n\n\nRecipe\n\nCodeDATA_rec &lt;-\n  recipe(Rank_Category ~ ., data = DATA_TRAIN) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nprep(DATA_rec) # checking prep\n\n\nTuning Model\n\nCodexgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n#workflow\nxgb_workfl &lt;- workflow(DATA_rec, xgb_spec)\n\n\nRace Anova\nOur model will be an XGBoost that utilizes a racing ANOVA. Tidymodels will also be used so that we can tune trees, min_n, mtry, tree_depth, learn_rate, and loss_reduction.\n\nCodelibrary(finetune)\ndoParallel::registerDoParallel()\n\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_workfl,\n  resamples = DATA_folds,\n  grid = 20,\n  metrics = metric_set(accuracy),\n  control = control_race(verbose_elim = TRUE)\n)\n\n\nComparing Models\nThe plot below shows the racing ANOVA as it picks out the best model\n\nCodeanova &lt;- plot_race(xgb_rs)\n\nanova +\n  labs(title = \"Model Race ANOVA\",\n       y = \"Model Accuracy\") +\n  theme_minimal() +\n  theme(plot.title = (element_text(hjust = 0.5)))\n\n\n\n\n\n\n\nBest Model\nThe following code is used to extract the best model\n\nCodeshow_best(xgb_rs)\n\n# A tibble: 4 × 12\n   mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     \n1    12  1597    11          1     0.0191       2.36e- 1 accuracy multiclass\n2    14   908     8          6     0.0292       8.29e- 1 accuracy multiclass\n3     3   724    24         10     0.155        2.41e- 2 accuracy multiclass\n4     7   502     8          5     0.310        3.53e-10 accuracy multiclass\n# ℹ 4 more variables: mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;\n\n\nMetrics\n\nCodexgb_last &lt;- xgb_workfl %&gt;%\n  finalize_workflow(select_best(xgb_rs, metric = \"accuracy\")) %&gt;%\n  last_fit(DATA_SPLIT)\n\nxgb_last$.metrics\n\n[[1]]\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass     0.835 Preprocessor1_Model1\n2 roc_auc     hand_till      0.915 Preprocessor1_Model1\n3 brier_class multiclass     0.124 Preprocessor1_Model1\n\n\nConfusion Matrix\nThe final model had an accuracy of 83.52 % for predicting ‘Ranked_Category’ and an AUC of 0.914. Although this is not the highest accuracy, the more important part is the importance of each variable for the model are Def_Eff, Win, Def_FG, Off_2p, and Off_TO as seen in the plot below.\n\nCodeDATA_pred &lt;- collect_predictions(xgb_last)$.pred_class\n\nDATA_act &lt;- DATA_TEST$Rank_Category\n\nconfusionMatrix(DATA_pred, DATA_act)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Bottom 50% Ranked Top 50%\n  Bottom 50%         37      0       4\n  Ranked              0      2       0\n  Top 50%             7      4      37\n\nOverall Statistics\n                                          \n               Accuracy : 0.8352          \n                 95% CI : (0.7427, 0.9047)\n    No Information Rate : 0.4835          \n    P-Value [Acc &gt; NIR] : 3.435e-12       \n                                          \n                  Kappa : 0.6965          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Bottom 50% Class: Ranked Class: Top 50%\nSensitivity                     0.8409       0.33333         0.9024\nSpecificity                     0.9149       1.00000         0.7800\nPos Pred Value                  0.9024       1.00000         0.7708\nNeg Pred Value                  0.8600       0.95506         0.9070\nPrevalence                      0.4835       0.06593         0.4505\nDetection Rate                  0.4066       0.02198         0.4066\nDetection Prevalence            0.4505       0.02198         0.5275\nBalanced Accuracy               0.8779       0.66667         0.8412\n\n\nVIP\n\nCodelibrary(vip)\nvip &lt;- extract_workflow(xgb_last) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\", num_features = 10, mapping = aes(fill = Variable))\nvip"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#neural-network",
    "href": "posts/NCAA Basketball/basketball.html#neural-network",
    "title": "NCAA Basketball Analysis",
    "section": "Neural Network",
    "text": "Neural Network\nNow that we know the important variables for ‘Ranked Category’, a neural network was performed to see how well we could predict it. The neural network was made in R using the command neuralnet(). The best neural network is seen in Figure 9 which had a 97.80% accuracy.\n\nCodelibrary(neuralnet)\nlibrary(caret)\nlibrary(tidymodels)\n\nnndata &lt;- clean_data \nset.seed(123)\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(nndata, prop = 3/4, strata = Rank_Category)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nNN &lt;- neuralnet(Rank_Category ~ ., train_data, hidden = c(5,3), linear.output = TRUE)\nplot(NN, rep = \"best\")\n\n\n\n\n\n\n\nConfusion Matrix\n\nCodepredicted_classes &lt;- predict(NN, test_data)\n# Extract predicted class labels\npredicted_classes &lt;- max.col(predicted_classes)\n\n# Convert the indices to class labels\npredicted_classes &lt;- levels(test_data$Rank_Category)[predicted_classes]\nactual_classes &lt;- test_data$Rank_Category\n\npredicted_classes &lt;- factor(predicted_classes, levels = levels(actual_classes))\n\n# length(predicted_classes)\n# print(predicted_classes)\n\n# Extract actual class labels from the test data\n# length(actual_classes)\n# print(actual_classes)\n\n# Create a confusion matrix\nconfusionMatrix(predicted_classes, test_data$Rank_Category)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Bottom 50% Ranked Top 50%\n  Bottom 50%         42      0       0\n  Ranked              0      6       0\n  Top 50%             2      0      41\n\nOverall Statistics\n                                          \n               Accuracy : 0.978           \n                 95% CI : (0.9229, 0.9973)\n    No Information Rate : 0.4835          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9607          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Bottom 50% Class: Ranked Class: Top 50%\nSensitivity                     0.9545       1.00000         1.0000\nSpecificity                     1.0000       1.00000         0.9600\nPos Pred Value                  1.0000       1.00000         0.9535\nNeg Pred Value                  0.9592       1.00000         1.0000\nPrevalence                      0.4835       0.06593         0.4505\nDetection Rate                  0.4615       0.06593         0.4505\nDetection Prevalence            0.4615       0.06593         0.4725\nBalanced Accuracy               0.9773       1.00000         0.9800"
  },
  {
    "objectID": "posts/This Website/ThisWebsite.html",
    "href": "posts/This Website/ThisWebsite.html",
    "title": "This Website!",
    "section": "",
    "text": "This was my first ‘frontend’ project that was done through lots of trial and erorr, tutorials, and reading. Overall, I am very pleased with the end result and would recommend Quarto for a portfolio. Once the website was created, it has been very easy to upload new files."
  },
  {
    "objectID": "datafest_file/CourseKata.html",
    "href": "datafest_file/CourseKata.html",
    "title": "CourseKata",
    "section": "",
    "text": "24 hour analysis on CourseKata, an online book on statistics. Data cleaning, exploratory data analysis, XGBoost, and suggestions for improvement ## Overview of Project Our initial area of interest upon reception of the data was the subjective student responses in the checkpoints_pulse table, as the developers of CourseKata, Jim Stigler and Ji Son, were primarily interested in the students’ opinions of the textbook. Unfortunately, this data proved unfruitful, as there was no variation in response for any factor we could find. Since subjective measures had no analytical value, we pivoted to looking at objective measures, starting with the EOC variable in the checkpoints_eoc table, which is the final percentage of questions each student answered correctly on the end of chapter (EOC) quiz. This data was much more diverse and had some interesting potential factors that might influence it. We ended up modifying the EOC data into a binary pass/fail variable with a division at 0.6 (for a 60% pass rate) that focused on book College(ABC).\nOur group used a gradient-boosted classification tree to chunk down the variables due to the high cardinality so we could use it as an exploratory model. The model started with 20 gradient classification models and picked the best one utilizing racing anova. The final model had 1649 trees. The importance of variables was calculated by how often they were utilized in the final fitted model to make a decision. The important variables were sum of engagement, average attempt, institution, and chapter. The model produced an AUC of 0.847 with an accuracy of 78.3%.\nOf the top 4 variables our model found to be important, two were student-determined variables and two were environment-determined variables. The total engagement time as well as the average attempts per question, the latter of which we engineered ourselves based on n_possible and n_attempted, were the two most influential variables regarding student pass/fail rate, and were the two student-determined variables. Students who obtained over a 60% on the EOC quizzes spent more time utilizing the textbook than students who obtained less than a 60%. Additionally, students with an average attempts per question over 3 were more likely to have an EOC score below 60%, with less students being over that threshold the higher the average attempts were. The book version for College (ABC) is important to the pass/fail rate of the students with more students improving with newer book versions. There is a large variance in pass/fail rates for institutions with some passing at 75% and others failing at 75%.\nOur next steps, if there was more time, would be to remove or rework the subjective “pulse” questions, looking into different book versions, and investigate discrepancies amongst institutions.\n\n\n\nWe Won!\nOut of 20 teams, we were able to take the prize of best in show!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real Estate R-Shiny Dashboard\n\n\n\n\n\n\nDashboard\n\n\nData Visualization\n\n\nMaps\n\n\nInteractive\n\n\nR\n\n\nR-Shiny\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nPlayfair’s Recreation & Improvement\n\n\n\n\n\n\nGraphical Critique\n\n\nData Visualization\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nInquisitR\n\n\n\n\n\n\nPackage Creation\n\n\nFunctions\n\n\nR\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Nate Silver’s Polling Outcomes:\n\n\n\n\n\n\nBinomial Distributions\n\n\nVisualizations\n\n\nWeights\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification & Sentiment Analysis\n\n\n\n\n\n\nText Classification\n\n\nSentiment Analysis\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nThis Website!\n\n\n\n\n\n\nQuarto\n\n\nHTML\n\n\nSCSS\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nNCAA Basketball Analysis\n\n\n\n\n\n\nWebscraping\n\n\nPCA\n\n\nNeural Network\n\n\nXGBoost\n\n\nR\n\n\nPlotly\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Playfair Project/Playfair.html",
    "href": "posts/Playfair Project/Playfair.html",
    "title": "Playfair’s Recreation & Improvement",
    "section": "",
    "text": "Critiquing, recreating, and improving playfair’s famous graph\n\nCodeknitr::include_graphics(\"Playfairimage.png\")\n\n\n\n\n\n\n\nPart 1: Critique\nPlayfair’s chart works well in several ways. First, it uses bars as a simple and effective visual representation to compare taxation and revenue across nations. The use of height to represent quantitative values allows viewers to easily see differences between nations. Additionally, the connecting line between the two bars allow for an easier comparison when looking at one specific county. The categorical arrangement of the nations by population size also helps in maintaining clarity and comparison. The use of circle size to represent population size is simple yet effective. The chart combines several forms of data in one visualization. Playfair’s introduction an effective method of showing patterns and trends visually.\nDespite its innovation, the chart has limitations that detract from its effectiveness. One of the main issues is the overloading of visual elements—having multiple variables in a single visual without clear legends. Without reading more about the graph, it is impossible to tell what is taxation and revenue. The pie charts inside the countries circles are too small to see. The Venn diagram is also extremely small and I had to zoom in to see it.\nAll things considered, Playfair’s chart effectively communicates its purpose—to compare the economic and geographical standings of European nations. While there are some things that are not ideal in its ability to precisely communicate all the data elements, the chart’s innovative use of bars for comparison was groundbreaking for its time. It set the foundation for modern data visualization, even though it lacks some refinements that we would expect today especially in DATA324!\nPart 2 - Recreation\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\nlibrary(stringr) # str_wrap\n\neurope &lt;- read.csv(\"~/Downloads/playfair_european_nations.csv\")\n\n# Calculate radius\nRadius &lt;- (sqrt(europe$Area / 3.1459)) / 140  # Scale\n\n# Dynamic center calculation\nnum_countries &lt;- nrow(europe)\nspacing &lt;- 3  \ncenter &lt;- numeric(num_countries)  # Initialize\n\n# Position of the first circle\ncenter[1] &lt;- 10 \n\n# Calculate the center positions based on the radius of the circles\nfor (i in 2:num_countries) {\n  center[i] &lt;- center[i - 1] + Radius[i - 1] + Radius[i] + spacing\n}\n\n# Axisbreak based on the center\naxisbreak &lt;- unique(center)\n\n# Wrapping x-axis labels\nwrapped_labels &lt;- str_wrap(europe$Country, width = 15)\n\nPlayfair &lt;- ggplot(europe) +\n  geom_circle(aes(x0 = center, y0 = 0, r = Radius)) +\n  \n  # Yellow line for Taxation\n  geom_segment(aes(x = center + Radius, xend = center + Radius, y = 0, yend = Taxation),\n               size = 1, color = \"yellow\") +\n  \n  # Red line for Population\n  geom_segment(aes(x = center - Radius, xend = center - Radius, y = 0, yend = Population),\n               size = 1, color = \"red\") +\n  \n  # Dashed line to connect red and yellow\n  geom_segment(aes(x = center - Radius, xend = center + Radius, \n                   y = Population, yend = Taxation), linetype = \"longdash\") +\n  \n  # Solid black lines at y = 0, 10, and 20\n  geom_hline(yintercept = c(0, 10, 20), color = \"black\", size = 0.5) +\n  \n  # Apply wrapped labels and adjust x-axis text\n  scale_x_continuous(breaks = axisbreak, labels = wrapped_labels) +\n  \n  # Set y-axis to display 0 to 30 with every integer as a tick mark\n  scale_y_continuous(\n    limits = c(-10, 30), \n    breaks = seq(0, 30, by = 1),\n    sec.axis = sec_axis(trans = ~ ., \n                        breaks = seq(0, 30, by = 1))  # Add name for secondary axis\n  ) +\n  \n  # Custom old paper-like background\n  theme(\n    # Old paper color\n    panel.background = element_rect(fill = \"#d3b58f\"),  \n    plot.background = element_rect(fill = \"#d3b58f\"),   \n    panel.grid.major = element_line(color = \"darkgrey\", size = 0.5), \n    panel.grid.minor = element_blank(), \n    \n    # Axis stuff\n    axis.text.x = element_text(angle = 90, face = \"bold\", hjust = 1),\n    axis.text.y = element_text(size = 5),\n    axis.text.y.right = element_text(size = 5),\n    plot.title = element_text(hjust = 0.5, size = 7, face = \"bold\"),\n    panel.border = element_rect(color = \"black\", fill = NA) # black border\n  ) +\n  labs(title = \"Chart Representing the Extent, Population & Revenues, of the Principal Nations in Europe, after the division of Poland & Treaty of Luneville\", x = \"\", y =\"\")\n\nPlayfair\n\n\n\n\n\n\n\nPart 3: New Data Visualization\n\nCodeRecreation &lt;- ggplot(europe, aes(x = Population, y = Taxation)) + \n  geom_point(aes(col = Country, size = Area)) + \n  ylim(c(0, 30)) + \n  labs(title = \"New Playfair Data Visualization\",\n       y = \"Taxation (Millions)\", \n       x = \"Population (Millions)\",\n       size = \"Land Area\",    \n       color = \"Country\") +         \n  scale_size_continuous(breaks = c(62000, 200000, 500000, 1000000, 2000000, 4720000),  \n                        labels = c(\"62k\", \"200k\", \"500k\", \"1M\", \"2M\", \"4.7M\"),\n                        range = c(2, 12)) +                # Better visibility\n  geom_text(aes(label = Country),  # Data labels\n            vjust = -0.5,           # Adjust vertical position\n            size = 3,               # Size of the text\n            check_overlap = TRUE) +   \n  guides(color = \"none\") +           # Remove legend for Country\n  theme(legend.key = element_blank()) + # Remove legend background\n  theme_minimal()\nRecreation\n\n\n\n\n\n\n\nPart 4: Concluding Explanation\nIn creating this data visualization, I chose to map Population to the x-axis and Taxation to the y-axis. This allows viewers to easily observe the relationship between a country’s population size and its tax revenue, which is critical for understanding how population might influence taxation. Each point represents a different country that allows for a direct visual comparison across multiple nations. The choice to use size for the Area of each country is also significant. Larger countries are represented by larger points which enables viewers to quickly assess the relative land size of each nation with its population and taxation.\nThe use of color to differentiate countries adds another layer of information, helping viewers quickly identify which point corresponds to which country. However, I opted to remove the color legend for Country to streamline the visualization and avoid clutter. Instead, I added data labels directly to the points, ensuring that viewers can easily discern which country each point represents. This decision supports clarity and allows for immediate identification without needing to look at a legend.\nIn terms of design principles, I used a minimalist theme (using theme_minimal()), which eliminates unnecessary gridlines and distractions and helps the viewer’s attention on the data The vertical adjustment of text labels helps to ensure that country names do not overlap with the points as much. I also carefully selected the size breaks for the Area scale and used significant thresholds that correspond to the actual data values to avoid misleading interpretations. The breaks provide a clear representation of how land area varies among the countries and allow for a more intuitive understanding of size differences.\nThe choice of colors could be more intentional by using a color palette that is colorblind friendly which could enhance accessibility. There are a few data labels that overlap and does not show in the graph. I would also add a footnote of where I got the data. My graph effectively illustrates that larger populations do not always correlate with higher taxation levels. This visualization aligns with Playfair’s assertion that visual data can reveal complex relationships that might not be immediately apparent through numerical data alone. Overall, the design choices made in this visualization work to highlight these relationships"
  },
  {
    "objectID": "posts/Real Estate Dashboard/test.html",
    "href": "posts/Real Estate Dashboard/test.html",
    "title": "Text Classification & Sentiment Analysis",
    "section": "",
    "text": "Code# Load necessary libraries\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(GGally)\nlibrary(tidyr)  # Added for pivot_longer\n\n# Read the dataset\ndata &lt;- read_excel(\"~/Desktop/Real estate valuation data set 2.xlsx\", sheet = \"工作表1\")\n\n# Round numeric columns to three decimal places\nrounded_data &lt;- data %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3))) %&gt;% \n  select(-`No`)\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Real Estate Valuation Analysis\"),\n  \n  # Remove sidebar layout\n  mainPanel(\n    tabsetPanel(\n      id = \"tab\",\n      tabPanel(\"Overview\", \n               h4(\"Explore the dataset and its summary.\"),\n               DTOutput(\"data_table\")),\n      tabPanel(\"Summary Statistics\", \n               h4(\"View key summary statistics of the data.\"),\n               DTOutput(\"summary_stats\")),  # Changed to DTOutput for better presentation\n      tabPanel(\"Pairwise Plot\", \n               h4(\"Explore pairwise relationships between multiple variables.\"),\n               plotOutput(\"pairwisePlot\")),\n      tabPanel(\"Map of Properties\", \n               h4(\"View the geographical distribution of properties.\"),\n               leafletOutput(\"propertyMap\")),\n      tabPanel(\"Distribution of House Prices\", \n               h4(\"Explore the distribution of house prices per unit area.\"),\n               plotOutput(\"priceDistributionPlot\")),\n      tabPanel(\"House Price vs. House Age\", \n               h4(\"Analyze the relationship between house age and price.\"),\n               radioButtons(\"smoothing_age\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_age\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceAgePlot\")),\n      tabPanel(\"House Price vs. Distance to MRT Station\", \n               h4(\"Investigate the effect of distance to MRT stations on house prices.\"),\n               radioButtons(\"smoothing_distance\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_distance\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceDistancePlot\")),\n      tabPanel(\"House Price vs. Convenience Stores\", \n               h4(\"Examine how the number of convenience stores affects house prices.\"),\n               plotOutput(\"priceConveniencePlot\"))\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  # Overview Data Table\n  output$data_table &lt;- renderDT({\n    datatable(rounded_data, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # Summary Statistics\n  output$summary_stats &lt;- renderDT({\n    summary_stats &lt;- rounded_data %&gt;%\n      summarise(across(everything(), \n                       list(mean = ~round(mean(.x, na.rm = TRUE), 3),\n                            median = ~round(median(.x, na.rm = TRUE), 3),\n                            sd = ~round(sd(.x, na.rm = TRUE), 3),\n                            min = ~round(min(.x, na.rm = TRUE), 3),\n                            max = ~round(max(.x, na.rm = TRUE), 3)))) %&gt;%\n      pivot_longer(everything(), names_to = c(\".value\", \"variable\"), names_sep = \"_\") %&gt;%\n      rename(Variable = variable)\n    \n    datatable(summary_stats, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # House Price vs. House Age\n  output$priceAgePlot &lt;- renderPlot({\n    if (input$color_age == \"none\") {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`, color = .data[[input$color_age]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Distance to MRT Station\n  output$priceDistancePlot &lt;- renderPlot({\n    if (input$color_distance == \"none\") {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`, color = .data[[input$color_distance]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Number of Convenience Stores\n  output$priceConveniencePlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = factor(`X4 number of convenience stores`), y = `Y house price of unit area`)) +\n      geom_boxplot(fill = \"orange\", color = \"black\") +\n      labs(title = \"House Price vs. Number of Convenience Stores\",\n           x = \"Number of Convenience Stores\", y = \"House Price of Unit Area\") +\n      theme_minimal()\n  })\n  \n  # Distribution of House Prices\n  output$priceDistributionPlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = `Y house price of unit area`)) +\n      geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n      labs(title = \"Distribution of House Prices\",\n           x = \"House Price of Unit Area\", y = \"Frequency\") +\n      theme_minimal()\n  })\n  \n  # Leaflet map\n  output$propertyMap &lt;- renderLeaflet({\n    leaflet(rounded_data) %&gt;%\n      addTiles() %&gt;%\n      addCircleMarkers(~`X6 longitude`, ~`X5 latitude`,\n                       radius = 5, color = \"blue\", fillOpacity = 0.5,\n                       popup = ~paste(\"Price per unit area:\", `Y house price of unit area`)) %&gt;%\n      setView(lng = 121.54, lat = 24.98, zoom = 13)\n  })\n  \n  # Pairwise Plot (assuming this is your intended implementation)\n  output$pairwisePlot &lt;- renderPlot({\n    correlationR(rounded_data)  \n    })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/Playfair/Playfair.html",
    "href": "posts/Playfair/Playfair.html",
    "title": "Playfair’s Recreation & Improvement",
    "section": "",
    "text": "Critiquing, recreating, and improving playfair’s famous graph\n\nCodeknitr::include_graphics(\"pf.png\")\n\n\n\n\n\n\n\nPart 1: Critique\nPlayfair’s chart works well in several ways. First, it uses bars as a simple and effective visual representation to compare taxation and revenue across nations. The use of height to represent quantitative values allows viewers to easily see differences between nations. Additionally, the connecting line between the two bars allow for an easier comparison when looking at one specific county. The categorical arrangement of the nations by population size also helps in maintaining clarity and comparison. The use of circle size to represent population size is simple yet effective. The chart combines several forms of data in one visualization. Playfair’s introduction an effective method of showing patterns and trends visually. Despite its innovation, the chart has limitations that detract from its effectiveness. One of the main issues is the overloading of visual elements—having multiple variables in a single visual without clear legends. Without reading more about the graph, it is impossible to tell what is taxation and revenue. The pie charts inside the countries circles are too small to see. The Venn diagram is also extremely small and I had to zoom in to see it. All things considered, Playfair’s chart effectively communicates its purpose—to compare the economic and geographical standings of European nations. While there are some things that are not ideal in its ability to precisely communicate all the data elements, the chart’s innovative use of bars for comparison was groundbreaking for its time. It set the foundation for modern data visualization, even though it lacks some refinements that we would expect today especially in DATA324! # Part 2 - Recreation\n\nCodelibrary(tidyverse)\nlibrary(ggforce)\nlibrary(stringr) # str_wrap\neurope &lt;- read.csv(\"~/Downloads/playfair_european_nations.csv\")\n# Calculate radius\nRadius &lt;- (sqrt(europe$Area / 3.1459)) / 140  # Scale\n# Dynamic center calculation\nnum_countries &lt;- nrow(europe)\nspacing &lt;- 3  \ncenter &lt;- numeric(num_countries)  # Initialize\n# Position of the first circle\ncenter[1] &lt;- 10 \n# Calculate the center positions based on the radius of the circles\nfor (i in 2:num_countries) {\n  center[i] &lt;- center[i - 1] + Radius[i - 1] + Radius[i] + spacing\n}\n# Axisbreak based on the center\naxisbreak &lt;- unique(center)\n# Wrapping x-axis labels\nwrapped_labels &lt;- str_wrap(europe$Country, width = 15)\nPlayfair &lt;- ggplot(europe) +\n  geom_circle(aes(x0 = center, y0 = 0, r = Radius)) +\n  \n  # Yellow line for Taxation\n  geom_segment(aes(x = center + Radius, xend = center + Radius, y = 0, yend = Taxation),\n               size = 1, color = \"yellow\") +\n  \n  # Red line for Population\n  geom_segment(aes(x = center - Radius, xend = center - Radius, y = 0, yend = Population),\n               size = 1, color = \"red\") +\n  \n  # Dashed line to connect red and yellow\n  geom_segment(aes(x = center - Radius, xend = center + Radius, \n                   y = Population, yend = Taxation), linetype = \"longdash\") +\n  \n  # Solid black lines at y = 0, 10, and 20\n  geom_hline(yintercept = c(0, 10, 20), color = \"black\", size = 0.5) +\n  \n  # Apply wrapped labels and adjust x-axis text\n  scale_x_continuous(breaks = axisbreak, labels = wrapped_labels) +\n  \n  # Set y-axis to display 0 to 30 with every integer as a tick mark\n  scale_y_continuous(\n    limits = c(-10, 30), \n    breaks = seq(0, 30, by = 1),\n    sec.axis = sec_axis(trans = ~ ., \n                        breaks = seq(0, 30, by = 1))  # Add name for secondary axis\n  ) +\n  \n  # Custom old paper-like background\n  theme(\n    # Old paper color\n    panel.background = element_rect(fill = \"#d3b58f\"),  \n    plot.background = element_rect(fill = \"#d3b58f\"),   \n    panel.grid.major = element_line(color = \"darkgrey\", size = 0.5), \n    panel.grid.minor = element_blank(), \n    \n    # Axis stuff\n    axis.text.x = element_text(angle = 90, face = \"bold\", hjust = 1),\n    axis.text.y = element_text(size = 5),\n    axis.text.y.right = element_text(size = 5),\n    plot.title = element_text(hjust = 0.5, size = 7, face = \"bold\"),\n    panel.border = element_rect(color = \"black\", fill = NA) # black border\n  ) +\n  labs(title = \"Chart Representing the Extent, Population & Revenues, of the Principal Nations in Europe, after the division of Poland & Treaty of Luneville\", x = \"\", y =\"\")\nPlayfair\n\n\n\n\n\n\n\nPart 3: New Data Visualization\n\nCodeRecreation &lt;- ggplot(europe, aes(x = Population, y = Taxation)) + \n  geom_point(aes(col = Country, size = Area)) + \n  ylim(c(0, 30)) + \n  labs(title = \"New Playfair Data Visualization\",\n       y = \"Taxation (Millions)\", \n       x = \"Population (Millions)\",\n       size = \"Land Area\",    \n       color = \"Country\") +         \n  scale_size_continuous(breaks = c(62000, 200000, 500000, 1000000, 2000000, 4720000),  \n                        labels = c(\"62k\", \"200k\", \"500k\", \"1M\", \"2M\", \"4.7M\"),\n                        range = c(2, 12)) +                # Better visibility\n  geom_text(aes(label = Country),  # Data labels\n            vjust = -0.5,           # Adjust vertical position\n            size = 3,               # Size of the text\n            check_overlap = TRUE) +   \n  guides(color = \"none\") +           # Remove legend for Country\n  theme(legend.key = element_blank()) + # Remove legend background\n  theme_minimal()\nRecreation\n\n\n\n\n\n\n\nPart 4: Concluding Explanation\nIn creating this data visualization, I chose to map Population to the x-axis and Taxation to the y-axis. This allows viewers to easily observe the relationship between a country’s population size and its tax revenue, which is critical for understanding how population might influence taxation. Each point represents a different country that allows for a direct visual comparison across multiple nations. The choice to use size for the Area of each country is also significant. Larger countries are represented by larger points which enables viewers to quickly assess the relative land size of each nation with its population and taxation. The use of color to differentiate countries adds another layer of information, helping viewers quickly identify which point corresponds to which country. However, I opted to remove the color legend for Country to streamline the visualization and avoid clutter. Instead, I added data labels directly to the points, ensuring that viewers can easily discern which country each point represents. This decision supports clarity and allows for immediate identification without needing to look at a legend. In terms of design principles, I used a minimalist theme (using theme_minimal()), which eliminates unnecessary gridlines and distractions and helps the viewer’s attention on the data The vertical adjustment of text labels helps to ensure that country names do not overlap with the points as much. I also carefully selected the size breaks for the Area scale and used significant thresholds that correspond to the actual data values to avoid misleading interpretations. The breaks provide a clear representation of how land area varies among the countries and allow for a more intuitive understanding of size differences. The choice of colors could be more intentional by using a color palette that is colorblind friendly which could enhance accessibility. There are a few data labels that overlap and does not show in the graph. I would also add a footnote of where I got the data. My graph effectively illustrates that larger populations do not always correlate with higher taxation levels. This visualization aligns with Playfair’s assertion that visual data can reveal complex relationships that might not be immediately apparent through numerical data alone. Overall, the design choices made in this visualization work to highlight these relationships"
  },
  {
    "objectID": "posts/dash/ShinyDashboard.html",
    "href": "posts/dash/ShinyDashboard.html",
    "title": "Real Estate R-Shiny Dashboard",
    "section": "",
    "text": "Create and host a dashboard using R/R-Shiny!\nLink to the dashboard: https://nathanbresette.shinyapps.io/real_estate/\nI’ve built countless dashboards using Tableau, PowerBI, Quicksight, and Qualtrics, but I’ve always wanted to create and host one from scratch. After learning how in my data visualization class, I decided to take the plunge!\nThis dashboard tells the story of real estate prices in New Taipei City, Taiwan, and it’s fully interactive—not just a static display of data.\nKey Features:\nPairwise Plots: Using my custom R package, InquisitR, I generated detailed pairwise plots for easy exploration of variable relationships.\nInteractive Scatter Plots & Histograms: Users can choose the third variable’s color and select regression lines for scatterplots. The histogram allows manipulation of the number of bins.\nLeaflet Map: An interactive map visualizes the geographical distribution of properties, revealing key insights into property prices across the city.\nThis experience opened up new possibilities in data storytelling and visualization. While many companies rely on drag-and-drop tools, using R offers complete control over the graphs—and best of all, it’s free! The data was very limited with only 7 variables but this was the foundational learning step!\nDashboard Code:\n\nCodelibrary(shiny)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(GGally)\nlibrary(devtools)\n\n# My package!!!\ninstall_github(\"NathanBresette/InquisitR\")\nlibrary(InquisitR)\n\n\nlibrary(tidyr) \n\n# Read the dataset\ndata &lt;- read_excel(\"Real estate valuation data set 2.xlsx\")\n\n# Round numeric columns to three decimal places\nrounded_data &lt;- data %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 3))) %&gt;% \n  select(-`No`)\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Real Estate Valuation Analysis\"),\n  \n  # Remove sidebar layout\n  mainPanel(\n    tabsetPanel(\n      id = \"tab\",\n      tabPanel(\"Overview\", \n               h4(\"Explore the dataset and its summary.\"),\n               DTOutput(\"data_table\")),\n      tabPanel(\"Summary Statistics\", \n               h4(\"View key summary statistics of the data.\"),\n               DTOutput(\"summary_stats\")),  # Changed to DTOutput for better presentation\n      tabPanel(\"Pairwise Plot\", \n               h4(\"Explore pairwise relationships between multiple variables.\"),\n               plotOutput(\"pairwisePlot\")),\n      tabPanel(\"Map of Properties\", \n               h4(\"View the geographical distribution of properties.\"),\n               leafletOutput(\"propertyMap\")),\n      tabPanel(\"Distribution of House Prices\", \n               h4(\"Explore the distribution of house prices per unit area.\"),\n               sliderInput(\"bins\", \"Number of bins:\", \n                           min = 5, max = 50, value = 30),  # Add slider to control bins\n               plotOutput(\"priceDistributionPlot\")),\n      tabPanel(\"House Price vs. House Age\", \n               h4(\"Analyze the relationship between house age and price.\"),\n               radioButtons(\"smoothing_age\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_age\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceAgePlot\")),\n      tabPanel(\"House Price vs. Distance to MRT Station\", \n               h4(\"Investigate the effect of distance to MRT stations on house prices.\"),\n               radioButtons(\"smoothing_distance\", \"Select Smoothing Method:\",\n                            choices = c(\"None\" = \"none\", \"Linear\" = \"lm\", \"LOESS\" = \"loess\"),\n                            selected = \"loess\"),\n               selectInput(\"color_distance\", \"Color by:\", \n                           choices = c(\"None\" = \"none\", colnames(rounded_data)),  # Include all column names\n                           selected = \"none\"),\n               plotOutput(\"priceDistancePlot\")),\n      tabPanel(\"House Price vs. Convenience Stores\", \n               h4(\"Examine how the number of convenience stores affects house prices.\"),\n               plotOutput(\"priceConveniencePlot\"))\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Overview Data Table\n  output$data_table &lt;- renderDT({\n    datatable(rounded_data, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # Summary Statistics\n  output$summary_stats &lt;- renderDT({\n    summary_stats &lt;- rounded_data %&gt;%\n      summarise(across(everything(), \n                       list(mean = ~round(mean(.x, na.rm = TRUE), 3),\n                            median = ~round(median(.x, na.rm = TRUE), 3),\n                            sd = ~round(sd(.x, na.rm = TRUE), 3),\n                            min = ~round(min(.x, na.rm = TRUE), 3),\n                            max = ~round(max(.x, na.rm = TRUE), 3)))) %&gt;%\n      pivot_longer(everything(), names_to = c(\".value\", \"variable\"), names_sep = \"_\") %&gt;%\n      rename(Variable = variable)\n    \n    datatable(summary_stats, options = list(pageLength = 5, autoWidth = TRUE))\n  })\n  \n  # House Price vs. House Age\n  output$priceAgePlot &lt;- renderPlot({\n    if (input$color_age == \"none\") {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X2 house age`, y = `Y house price of unit area`, color = .data[[input$color_age]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. House Age\",\n             x = \"House Age (years)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_age == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_age, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Distance to MRT Station\n  output$priceDistancePlot &lt;- renderPlot({\n    if (input$color_distance == \"none\") {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`)) +\n        geom_point(alpha = 0.5) +  # No color mapping\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    } else {\n      ggplot(rounded_data, aes(x = `X3 distance to the nearest MRT station`, y = `Y house price of unit area`, color = .data[[input$color_distance]])) +\n        geom_point(alpha = 0.5) +\n        scale_color_gradient(low = \"blue\", high = \"red\", na.value = \"grey\") +\n        labs(title = \"House Price vs. Distance to MRT Station\",\n             x = \"Distance to MRT Station (meters)\", y = \"House Price of Unit Area\") +\n        {\n          if (input$smoothing_distance == \"none\") {\n            NULL\n          } else {\n            geom_smooth(method = input$smoothing_distance, se = FALSE, color = \"red\")\n          }\n        } +\n        theme_minimal()\n    }\n  })\n  \n  # House Price vs. Number of Convenience Stores\n  output$priceConveniencePlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = factor(`X4 number of convenience stores`), y = `Y house price of unit area`)) +\n      geom_boxplot(fill = \"orange\", color = \"black\") +\n      labs(title = \"House Price vs. Number of Convenience Stores\",\n           x = \"Number of Convenience Stores\", y = \"House Price of Unit Area\") +\n      theme_minimal()\n  })\n  \n  # Distribution of House Prices\n  output$priceDistributionPlot &lt;- renderPlot({\n    ggplot(rounded_data, aes(x = `Y house price of unit area`)) +\n      geom_histogram(bins = input$bins, fill = \"skyblue\", color = \"black\") +  # Use input$bins\n      labs(title = \"Distribution of House Prices\",\n           x = \"House Price of Unit Area\", y = \"Frequency\") +\n      theme_minimal()\n  })\n  \n  # Leaflet map\n  output$propertyMap &lt;- renderLeaflet({\n    leaflet(rounded_data) %&gt;%\n      addTiles() %&gt;%\n      addCircleMarkers(~`X6 longitude`, ~`X5 latitude`,\n                       radius = 5, color = \"blue\", fillOpacity = 0.5,\n                       popup = ~paste(\"Price per unit area:\", `Y house price of unit area`)) %&gt;%\n      setView(lng = 121.54, lat = 24.98, zoom = 13)\n  })\n  \n  # Pairwise Plot\n  output$pairwisePlot &lt;- renderPlot({\n    correlationR(rounded_data)  \n  })\n}\n\n# Run the app\nshinyApp(ui = ui, server = server)\n\n\nImages of Dashboard\nMap of Taiwan:\n\nCodeknitr::include_graphics(\"Taiwan_Map.png\")\n\n\n\n\n\n\n\nPairwise Plot (My package!!!)\n\nCodeknitr::include_graphics(\"Pairwise_Shiny.png\")\n\n\n\n\n\n\n\nScatterplot\n\nCodeknitr::include_graphics(\"Scatterplot_Shiny.png\")"
  },
  {
    "objectID": "posts/Polling/Polling.html",
    "href": "posts/Polling/Polling.html",
    "title": "Analysis of Nate Silver’s Polling Outcomes:",
    "section": "",
    "text": "Analysis of Nate Silver’s Polling Outcomes: Observed vs. Expected Results\nGoal: Examine, analyze, and enhance Nate Silver’s paper on presidential polling\nIn examining the polling data, we start with the foundational parameters that govern our analysis. We have a total of 249 polls in the database, with an expected probability of a poll showing a close result under the null hypothesis set at 0.55. This probability is based on Nate Silver’s analysis, which indicates that, in a tied race, we would expect 55% of the polls to show results within ±2.5 points. This expectation arises from the margin of error formula and varies significantly depending on sample size but for a more in depth reasoning, check out his paper. In the data, the actual number of polls that showed a close result is 193 so 193/249 polls or 78% showed a close race.\nBinomial Distribution\nUsing these parameters, we calculate the probability of observing 193 or more close polls under the null hypothesis. This is accomplished through the binomial cumulative distribution function (CDF):\n\nCode# Number of polls in the database\nn &lt;- 249\n\n# Expected probability of a poll showing a close result under the null hypothesis\np &lt;- 0.55\n\n# Number of polls that actually showed a close result\nobserved_successes &lt;- 193\n\n# Calculate the probability using the binomial cumulative distribution function\nprobability &lt;- pbinom(observed_successes - 1, n, p, lower.tail = FALSE)\n\n# Convert the probability into a \"1 in X\" number\none_in_x &lt;- 1 / probability\n\n# Print the result without scientific notation\nformatted_one_in_x &lt;- format(one_in_x, scientific = FALSE)\npaste(\"One in\", formatted_one_in_x, \"or about 8 trillion which is close to Nate Silver's 9 trillion probability\")\n\n[1] \"One in 8316941692884 or about 8 trillion which is close to Nate Silver's 9 trillion probability\"\n\n\nThe calculation indicates the rarity of observing such a result under the stated null hypothesis. This perspective aligns with Nate Silver’s approach to polling, where understanding the significance of results is crucial to interpreting their implications in real-world scenarios.\nVisualizing Simulated Outcomes\nTo further illustrate the significance of our findings, we conduct 10,000 simulations to model the distribution of close poll outcomes. By creating a histogram of these simulated outcomes, we can visually assess where the observed success fits within the broader distribution. In Graph 1, the red dashed line signifies the number of observed close polls. This visual representation allows us to contextualize our findings within the realm of simulated expectations and see how rare it would be if the polls are correct.\n\nCodelibrary(ggplot2)\nset.seed(123)\nsimulations &lt;- 10000\nsimulated_outcomes &lt;- rbinom(simulations, n, p)\n\n# Create a histogram of simulated outcomes\nggplot(data.frame(simulated_outcomes), aes(x = simulated_outcomes)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  # Move the annotation text slightly to the left and bold both parts of the text\n  annotate(\"text\", x = observed_successes - 10, y = max(table(simulated_outcomes)) * 0.9,\n           label = bquote(bold(\"Observed: \") * bold(.(observed_successes))), color = \"red\") +\n  scale_x_continuous(limits = c(min(simulated_outcomes) - 5,200)) +  # Expand x-axis range\n  labs(\n    title = \"Graph 2: Simulated Distribution of Close Poll Outcomes\",\n    x = \"Number of Close Polls\",\n    y = \"Frequency\",\n    subtitle = \"The blue distrubition is the expected number of close polls from the simulation\nand the red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nComparing Expected vs. Observed Outcomes\nNext, we can create a bar chart, Graph 3, to compare the expected number of close polls against the observed number. The expected number is derived from the initial probability (0.55), while the observed number reflects real-world polling results. This comparison highlights the disparity between expected outcomes based on historical data and the actual observed results, providing insight into the polling landscape.\n\nCode# Calculate the expected number of close polls under the null hypothesis\nexpected_successes &lt;- n * p\n\n# Create a data frame for plotting\ncomparison_data &lt;- data.frame(\n  Category = c(\"Expected\", \"Observed\"),\n  Count = c(expected_successes, observed_successes)\n)\n\n# Plot\nggplot(comparison_data, aes(x = Category, y = Count, fill = Category)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  scale_fill_manual(values = c(\"skyblue\", \"red\")) +\n  labs(\n    title = \"Graph 2: Expected vs. Observed Number of Close Polls\",\n    y = \"Number of Close Polls\",\n    x = \"\",\n    caption = \"Expected is based on a probability of 0.55. Observed is the actual count.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nCumulative Distribution Function Analysis\nLastly, we can examine the cumulative distribution function (CDF) of the binomial distribution for further insights. The CDF plot provides a comprehensive view of the probability of observing various numbers of close polls, further emphasizing the significance of our observed results in relation to expectations.\n\nCode# Calculate the CDF for the binomial distribution\nx_vals &lt;- 0:n\ncdf_vals &lt;- pbinom(x_vals, n, p)\n\n# Create a data frame for plotting\ncdf_data &lt;- data.frame(x_vals, cdf_vals)\n\n# Plot the CDF\nggplot(cdf_data, aes(x = x_vals, y = cdf_vals)) +\n  geom_line(color = \"blue\") +\n  geom_vline(xintercept = observed_successes, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Graph 3: Cumulative Distribution Function (CDF) of Binomial Distribution\",\n    x = \"Number of Close Polls\",\n    y = \"Cumulative Probability\",\n    caption = \"The red dashed line shows the actual number of close polls observed.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nConclusion\nThe analysis of polling data reveals a stark contrast between the expected outcomes based on Nate Silver’s framework and the actual results observed in recent polls. With an expected probability of p = 0.55, representing the likelihood that a poll should show a close result within ±2.5 points in a tied race, the finding of 193 close polls out of 249 conducted indicates a significant deviation from this norm. This discrepancy translates into an extraordinarily low probability, quantifying the actual observation as a 1 in 9 trillion scenario. Such results raise critical questions about the accuracy and reliability of the polling methodologies in use.\nA noteworthy aspect of this analysis is the way polling firms often adjust weights in their models to achieve closer race outcomes. By calibrating the weights based on historical voting patterns and demographic data, pollsters can create an artificial tightness in the race. This practice might stem from a desire to generate more competitive narratives or to align with expected electoral dynamics, but it can inadvertently lead to inflated expectations for how close the race actually is."
  }
]