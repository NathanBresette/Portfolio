---
title: "Text Classification & Sentiment Analysis"
date: 05-20-2024 
author: Nathan Bresette 
categories: [Text Classification, Sentiment Analysis, R]
image: "LL_wordcloud.jpg"

output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
format:
  html:
    code-link: true
    code-fold: true
    code-overflow: wrap
    warning: false
---

Created text classification and sentiment analysis model to automate the process of classifying free response questions in a category then whether they were positive, neutral, negative, or a question.


## Abstract
This project was done during my intership at Lifeline Pregnancy. During the school year, they go to schools across Missouri and give a 'Pure Freedom' talk to the students. At the end of the talk, the students fill out a survey which has a free response question. The free response question is categorized into one of the four categories: Educator, Program, Content, or Other. The sentiment is then calculated as positive, negative, neutrall, or question. Before this model was created, all classification and sentiment was performed individually for every free response. Because the categories and sentiment are different than any pre-existing model, I had to make my own. To preserve the data and information of the clinic, some information will not be available


## Data Cleaning
The data cleaning process involves anticipating and correcting student typos, standardizing all words to lowercase, and eliminating pluralizations, with the function addressing some of these tasks. 
```{r}
library(tidyverse)
library(tidytext)
library(stringr)
library(caret)

com <- read.csv("~/Desktop/comments - Sheet1.csv", header=FALSE)

#Different data set
# com <- read.csv("~/Desktop/PF Comment Coding  - Fall 2022-Spring 2023.csv")
# com <- com %>%
#   rename(comments = Comment, labels = Aspect, sentiments = Quality)

# Load necessary libraries
library(dplyr)

# Filter and preprocess the data
pfcomments <- com %>%
  filter(V2 != "") %>%
  rename(comments = V1, labels = V2, sentiments = V6) %>% 
  mutate(comments = ifelse(comments == "jakson", "Jackson", comments)) %>% 
  mutate(comments = ifelse(comments == "under stode", "understood", comments)) %>% 
  mutate(comments = ifelse(comments == "relation ships", "relationships", comments)) %>%
  mutate(words_count = str_count(comments, "\\S+"))
```

The function is now created and preprocesses text data by converting it to lowercase, removing possessive apostrophes ('s), and then categorizes the comments based on the presence of certain keywords. If a comment contains only a smiley face ":)", it is labeled as "Other". Comments containing the word "sex" are labeled as "Remove". Additionally, comments containing words like "like", "liked", "love", or "loved" are also labeled as "Remove". For other comments, it calculates the frequency of specific words related to educational programs, educators, and content, then assigns a label ("Educator", "Program", "Content", or "Other") based on the word frequencies and the length of the comment. The words that were removed are often split between categories so an individual still reviews them.

Specific related words will not be shown for educator, program, or content to preserve some information
```{r, echo=FALSE}
  educator_words <- c("jaclyn","ryan", "ryan", "hanley", "hanley","lynae", "lynae","mcfarland", "mcfarland", "jackson", "jackson", "jacks", "page", "margo", "margo","staiger", "amy", "posterick", "she", "he", "his", "her","they", "guys","man", "guy","girl", "woman", "yâ€™all", "yall",  "yourself","bro","good", "job", "good job", "did great", "great work", "great job","speakers", "teacher","teacher", "teachers", "teaching","educator", "educators", "instructor", "instructor", "instructor", "instructors","speaker", "awesome", "awsome", "fun", "sweet", "friendly","personal", "story", "stories", "talk")
  program_words <- c("learned", "sense", "it", "this", "sex", "sexual", "health", "thank", "thank","thanks", "thanks", "thx", "program", "lesson", "lessons","time", "good", "fun", "presentation", "know", "talk", "course", "educational", "class", "liked")
  content_words <- c("relationship", "relation ships","relationships", "std", "sti", "stds", "stis", "body", "anatomy","porn", "pornography", "baby", "baby belly", "safe","sexual", "butterfly effect", "pregnancy", "pregnancies","pregnant", "boundary", "boundaries", "birth control", "condom", "plan b","toxic", "puberty", "penis", "cock", "rape", "menstri")
```

```{r}
# Define preprocess_and_extract_features function
preprocess_and_extract_features <- function(comment, label, words_count) {
  # Convert text to lowercase
  comment <- tolower(comment)
  comment <- gsub("'s", "", comment) 

program_count <- sum(sapply(program_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
educator_count <- sum(sapply(educator_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
content_count <- sum(sapply(content_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
  
  # If comment contains only ":)", label as "Other"
  if (grepl("^:\\)$", comment)) {
    pred_label <- "Other"
  }
  # If comment contains "sex", label as "Remove"
  else if (grepl("\\bsex\\b", comment, ignore.case = TRUE)) {
        pred_label <- "Remove"
  } 
  else if(grepl("\\blike\\b|\\bliked\\b|\\blove\\b|\\bloved\\b", comment, ignore.case = TRUE))
  {
    pred_label <- "Remove"
  }
  else {
    program_count <- sum(sapply(program_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
    educator_count <- sum(sapply(educator_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
    content_count <- sum(sapply(content_words, function(word) grepl(paste0("\\b", word, "\\b"), comment, ignore.case = TRUE)))
  
    # If content_count is greater than or equal to 1, label as content
    if (content_count >= 1) {
      pred_label <- "Content"
    } else {
      # If educator_count is greater than program_count, label as educator
      if (educator_count > program_count) {
        pred_label <- "Educator"
      } 
      # If program_count is greater than educator_count, label as program
      else if (program_count > educator_count) {
        pred_label <- "Program"
      } 
      else if (program_count >= 1 & educator_count >= 1 & program_count == educator_count) {
        pred_label <- "Remove"
      }
      else if(words_count < 35) {
        pred_label <- "Remove"
      }
      # If content_count is 0, educator_count or program_count is 1, do not label as other
      else {
        pred_label <- "Other"
      }
    }
  }
  
  return(data.frame(comments = comment, pred_label = pred_label, program_count, educator_count, content_count, labels = label))
}
```

### Confusion Matrix
The function is now called and we split the data into correct and incorrect data sets. This allows us to see what content words did well and which ones did not. Additionally, a confusion matrix is used to show model accuracy where we specifically look at sensitivity and specificity
```{r}
processed_data <- Map(preprocess_and_extract_features, pfcomments$comments, pfcomments$labels, pfcomments$words_count) %>%
  bind_rows()

processed <-processed_data %>% 
  filter(pred_label != "Remove")

bad_processed_data <- processed %>% 
  filter(pred_label != labels) 


confusionMatrix(factor(processed$labels), factor(processed$pred_label))
```


Justification for removing words over 35 based on the distributions of our correct and incorrect data sets. The means are much different too
```{r}
wordcount <- pfcomments %>%
  mutate(text = comments) %>%
  unnest_tokens(word, text, token = "regex") %>% 
  group_by(comments) %>%
  summarise(words_count = n())  # Count words per comment

#View(wordcloud)

bad_wordcount <- bad_processed_data %>%
  mutate(text = comments) %>%
  unnest_tokens(word, text, token = "regex") %>% 
  group_by(comments) %>%
  summarise(words_count = n())  # Count words per comment

ggplot(wordcount, aes(x = words_count)) +
  geom_bar()

ggplot(bad_wordcount, aes(x = words_count)) +
  geom_bar()

mean(wordcount$words_count)
mean(bad_wordcount$words_count)
```
## Most Common Words
```{r}
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

tidycom <- pfcomments %>% 
  mutate(text = str_replace_all(comments, replace_reg, "")) %>%
  unnest_tokens(word, comments, token = "regex", pattern = unnest_reg) %>% anti_join(stop_words)

# tidycom %>% count(word, sort = TRUE)

tidycom %>%
  count(word, sort = TRUE) %>%
  filter(n < 120) %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


## Sentiment Analysis
```{r}
# Apply sentiment analysis using the AFINN lexicon
sentiment_scores <- pfcomments %>%
  mutate(text = comments) %>%
  unnest_tokens(word, text) %>%
  
  left_join(get_sentiments("afinn"), by = "word") %>%  # Use left_join to preserve all rows in pfcomments
  group_by(comments) %>%
  summarise(sentiment_score = sum(value, na.rm = TRUE)) %>%  # Handle NA values in sentiment scores
  ungroup()

# Classify sentiments
sentiment_scores <- sentiment_scores %>%
  mutate(sentiment = case_when(
    sentiment_score > 0 ~ "Positive",
    sentiment_score < 0 ~ "Negative",
    TRUE ~ "Neutral"
  ))

sentiment_scores %>%
  ggplot(aes(sentiment, fill = sentiment)) +
  geom_bar() +
  labs(title = "Sentiment Analysis of Comments with Question Classification")

```




### Bar Chart of Sentiments
```{r}
pfcomments %>%
  filter(sentiments != "Question") %>%  # Exclude 'question' sentiments
  ggplot(aes(sentiments, fill = sentiments)) +
  geom_bar() +
  labs(title = "Sentiment Analysis of Comments")
```


```{r}
library(dplyr)
library(tidytext)
library(ggplot2)
library(caret) # For confusionMatrix

# Add an identifier to each comment
pfcomments <- pfcomments %>%
  mutate(comment_id = row_number())

# Apply sentiment analysis using the AFINN lexicon
sentiment_scores <- pfcomments %>%
  mutate(text = comments) %>%
  unnest_tokens(word, text) %>%
  left_join(get_sentiments("afinn"), by = "word") %>%
  group_by(comment_id, comments) %>%
  summarise(sentiment_score = sum(value, na.rm = TRUE)) %>%
  ungroup()

# Classify sentiments
sentiment_scores <- sentiment_scores %>%
  mutate(sentiment = case_when(
    sentiment_score > 0 ~ "Positive",
    sentiment_score < 0 ~ "Negative",
    TRUE ~ "Neutral"
  ))


# Merge sentiment scores back with original comments
pfcomments <- pfcomments %>%
  left_join(sentiment_scores %>% select(comment_id, sentiment_score, sentiment), by = "comment_id")

# Plot sentiment distribution
pfcomments %>%
  filter(sentiments != "Question") %>%
  ggplot(aes(sentiments, fill = sentiments)) +
  geom_bar() +
  labs(title = "Sentiment Analysis of Comments")

# Ensure same levels for confusion matrix
pfcomments <- pfcomments %>%
  mutate(sentiments = factor(sentiments, levels = c("Positive", "Negative", "Neutral")),
         sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral")))

# Generate confusion matrix
confusionMatrix(pfcomments$sentiment, pfcomments$sentiments)

```


## Wordcloud
A nice visualization for the company to use
```{r}
#install.packages("wordcloud")
library(wordcloud)

#install.packages("RColorBrewer")
library(RColorBrewer)

#install.packages("wordcloud2")
library(wordcloud2)

#install.packages("tm")
library(tm)
#Create a vector containing only the text
text <- pfcomments$comments
# Create a corpus  
docs <- Corpus(VectorSource(text))

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
```

```{r}
#fc5f1b1b-2aeb-4e09-93fc-06fdac0d8030
# Making DF for word clouds

# Pre word cloud
corpus = Corpus(VectorSource(pfcomments$comments))

corpus <- corpus %>% 
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords, stopwords("SMART"))

tdm = TermDocumentMatrix(corpus) %>% 
  as.matrix()

words = sort(rowSums(tdm), decreasing = TRUE)

pre_WCdf = data.frame(words = names(words), freq = words)


# Color Palettes
pre_WCcolors = c("#8bc13f", "#396430", "#6e6e6e")
pre_WCbkgd = "#FFFFFF"
post_WCcolors = c("#FFFFFF", "#510C76", "#87714D")
post_WCbkgd = "#00A8E2"

#rm unneeded vars
rm(corpus, tdm, words)

WC_Pre <- wordcloud2(pre_WCdf,
           color = rep_len(pre_WCcolors, nrow(pre_WCdf)),
           backgroundColor = pre_WCbkgd,
           fontFamily = "AppleMyungjo",
           size = .62,
           rotateRatio = 0)

WC_Pre
```
